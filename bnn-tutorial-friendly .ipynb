{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Neural Networks: variational inference with epistemic uncertainity, i.e., the Neural Networks who can say \"I don't know\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pi Esposito, AI Software Development Intern @ Intel Corporation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we are going to work with some non \"Kaggle default\" packages, you may want to uncomment those pip install for gathering the packages used on this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install -U pip\n",
    "#%pip install tensorflow==2.0\n",
    "#%pip install tfp-nightly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Objective:** \n",
    "    This kernel aims to **(i)** justify the **need for Neural Network models which are able to output the uncertainity for its predictions**, i.e., let the Data Scientist know if his model is not able to perform a reliable prediction on the data which he tried to predict the label; and **(ii)** **present the Bayesian approach to Neural Networks** as a way to supply this need.\n",
    "    \n",
    "    \n",
    "* **Note:** In order to ease the explanation of the mathematical part of the BNNs (without any theorethical loss), Neural Networks will be interpreted not as a maximum likelihood estimation for the training data (statistical interpretation), but as and optimized function to model the train data in function of its labels.  \n",
    "\n",
    "\n",
    "* **Packages used**:\n",
    "    * TensorFlow 2.0\n",
    "    * TensorFlow Probability (for probabilistic NN layers)\n",
    "    * Matplotlib\n",
    "    * Numpy\n",
    "    * Pandas\n",
    "    * Seaborn\n",
    "    * Sklearn train_test_split function\n",
    "    * time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(10171927)\n",
    "tf.random.set_seed(10171927)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version (expected = 2.0.0): 2.0.0\n",
      "TensorFlow Probability version (expected = 0.9.0-dev20190912): 0.9.0-dev20191010\n"
     ]
    }
   ],
   "source": [
    "print('TensorFlow version (expected = 2.0.0):', tf.__version__)\n",
    "print('TensorFlow Probability version (expected = 0.9.0-dev20190912):', tfp.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **1. Introduction**\n",
    "    * 1.1 The \"need\" for uncertainity gathering on predictions\n",
    "    * 1.2 The Bayesian approach for Neural Networks\n",
    "    * 1.3 ELBO loss function\n",
    "    * 1.4 Notes on TensorFlow 2.0 and TensorFlow Probability \n",
    "* **2. Data preparation**\n",
    "    * 2.1 Load, normalize and reshape data\n",
    "    * 2.2 Visualizing the data\n",
    "* **3. Ilustrating the problem with a deterministic CNN**\n",
    "    * 3.1 Implementing and training the deterministic model\n",
    "    * 3.2 Predicting on real data and on noise: there is always a prediction\n",
    "* **4. Implementing a Bayesian Neural Network with TensorFlow 2.0**\n",
    "    * 4.1 Notes on the implementation\n",
    "    * 4.2 Implementing the Bayesian model with TFP Layers\n",
    "    * 4.3 Implementing the ELBO Loss function\n",
    "    * 4.4 Implementing the train function\n",
    "    * 4.5 Wrap-up and train the BNN\n",
    "* **5. Predicting with Monte Carlo methods and epistemic uncertainity gathering**\n",
    "    * 5.1 Notes on the prediction method\n",
    "    * 5.2 Gathering the prediction distribution\n",
    "    * 5.3 Visualizing valid predictions\n",
    "    * 5.4 Visualizing non-predicted values (noise)\n",
    "* **6. Conclusion ** \n",
    "* **7. References **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "## 1. Introduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 The \"need\" for uncertainity gathering on predictions\n",
    "\n",
    "Neural Networks do work by performing feedforward and outputing logits, which can be used as predictions or interpreted as wished. \n",
    "\n",
    "Despite being very accurate and able to model (with the right hyperparameters) any kind of mathematical function, NNs do operate with any kind of data (if encoded in a tensor according to its input shape). For illustration purposes, it means that, on the case of a image-classifier CNN, once I've encoded an image as a tensor as the same of its input, I'm able to try to predict its label even if my image has nothing to see with my classificator purpose. It also means that, even when I'm trying to predict a very noisy image (which can almos be seen as noise), the network will output a deterministic prediction for it (** see point 3. for illustration**). \n",
    "\n",
    "We understand that it would be nice to know how certain our network is of its prediction. For example, in an autonomous car, it should be mandatory to have threshold on which, even if it does not recognize a pedestrian, depending on the uncertanity of its prediction, the car acts as if there was one. This goes for many examples on sensible decisions on which life is involved. \n",
    "\n",
    "We also understand that, in some financial cases, (e.g., neural-network consulting stock trader bot) it would also be nice to understand if there is a situation where it is not sure on how to behave and thus stop trading in order to avoid money loss.\n",
    "\n",
    "**In short, the point is that it would be very beneficial for Deep Learning guided decision process if the neural network could know if it is not trained to understand the data fed on it and output something which can be interpreted as \"I'm interpreting this as noise\".**[](http://)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 The Bayesian approach for Neural Networks\n",
    "\n",
    "In order to address that issue, it was created a Bayesian Approach to Neural Networks. It consists on building networks if layer which we will call **proabilistic layers**. This lets gather uncertainity on our predictions, as we will see."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Probabilistic layers\n",
    "\n",
    "Probabilistic layers are neural network layers on which the weights are not deterministic. They are actually **random variables (normally distributed) sampled differently on each feedforward step of the network**. When the layer is being built, for each weight, it randomly starts **mean and standard deviation, which are the trainable parameters**. \n",
    "\n",
    "**When the layer is called (i.e., on each feedforward step), it samples a weight from the distribution and runs normally.**\n",
    "\n",
    "**On the backpropagation step, for each trainable random variable, the gradients are the partial derivative from the Loss in relative the mean and standard deviation**.\n",
    "\n",
    "This layer-sampling step means that, if we predict many times on the same input, we will have different weights sampled (according to the distribution). Thus, noisy images as prone to have more \"uniform-like\" distribution, being more sensible to the feature extraction layer changes (as they have no features at all), while real clean images will be less sensible to layer changing and ar prone to keep the same prediction (its label).\n",
    "\n",
    "To illustrate the feedforward step, we see a  bayesian network on the right side. We see that the bayesian one has a different probability distribution for each weight.\n",
    "\n",
    "![](https://raw.githubusercontent.com/zackchase/mxnet-the-straight-dope/master/img/bbb_nn_bayes.png)\n",
    "[Credits for Gluon](https://gluon.mxnet.io/chapter18_variational-methods-and-uncertainty/bayes-by-backprop.html)\n",
    "\n",
    "#### Uncertanity gathering\n",
    "\n",
    "To gather uncertanity on our predictions, we use Monte Carlo methods and predict it N times, saving each sampled prediction. We then get the prediction distribution and, using a threshold, decide that, if the mode of the prediction represent less then the threshold, then the network should not decide at all and we do not consider the prediction. Our \"reliability\" of the predicition the % of the total predictions that the mode represent. \n",
    "\n",
    "Real data tend to have the mode representing almost 100%, while the more noisy it is, the more it uniform distribution it will have.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 ELBO loss function \n",
    "\n",
    "\n",
    "As we are working with Probability distribution, we will use a combination of the KL divergence, which is a mathematical representation of the information loss on representing the labels of the train features-label pair with the feedforward representation with the sparse categorical crossentropy, as is usually done on classification NN modelling.\n",
    "\n",
    "As TensorFlow lets us calculate each of this losses, wonÂ´t have to code it by hand. If you want to know more about the derivatives and the math, I'm putting a link for the original paper on the end of this kernel.\n",
    "\n",
    "By the way, we call this function the ELBO loss functon, which stants for expected lower bound.\n",
    "It is defined as:\n",
    "\n",
    "(The KL Divergence of our predicted distribution relative to the labels plus the crossentropy of our predicted relative to the labels - being Q the predicted probabilities or logits):[](http://)\n",
    "\n",
    "\\begin{equation*}\n",
    "* \\mathcal{L}(\\mathcal{D}, \\mathbf{\\theta}) = \\text{KL}[q(\\mathbf{w}\\ |\\ \\mathbf{\\theta})\\ ||\\ P(\\mathbf{w})] - \\mathbb{E}_{q(\\mathbf{w}\\ |\\ \\mathbf{\\theta})}[\\log P(\\mathcal{D}\\ |\\ \\mathbf{w})]\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Notes on TensorFlow 2.0 and TensorFlow probability\n",
    "\n",
    "First, some useful links:\n",
    "\n",
    "* [TensorFlow 2.0 quickstart tutorial](https://www.tensorflow.org/tutorials/quickstart/)\n",
    "\n",
    "* [TensorFlow Probability main page and tutorials](https://www.tensorflow.org/probability)\n",
    "\n",
    "#### TensorFlow 2.0\n",
    "##### All the features explained here will be shown during the implementations\n",
    "* TensorFlow 2.0 has ended tf.Session(), feed_dict and sess.run. We now just have to call the node of the graph to get its value. TensorFlow 2.0 has also ended the need for placeholders, giving us a much more intuitive way to flow tensors thorugh graphs:\n",
    "\n",
    "* We define it as a funcion with the inputs a parameters. We do the operations and return the nodes we want to. The only thing we have to do is put @tf.function decorator above the definition. We can calculate losses and optimize models with tf.functions in a much easier way.\n",
    "\n",
    "* For gradient appliying, we have gradientTape which gets the derivative of our last operation relative to all the nodes conected to it, and a powerful and easy way to apply those gradients with the optimizer.\n",
    "\n",
    "\n",
    "\n",
    "#### TensorFlow Probability\n",
    "\n",
    "* The probabilistic counterpart os TensorFlow has, on its features, some probabilistic counterparts for layers, so we don't have to bother in implementing probabilistic layers.\n",
    "\n",
    "* The nightly version of TFP works well with all the TensorFlow 2.0 features, and so it will be very easy to write custom losses and manually optimizing our Bayesian Network on the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Load, normalize and reshape the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are using MNIST dataset!\n",
      "X_train.shape = (60000, 28, 28, 1)\n",
      "Y_train.shape = (60000, 10)\n",
      "Y_val.shape = (10000, 28, 28, 1)\n",
      "Y_val.shape = (10000, 10)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "first, we load the dataset. We are trying to do it first with MNIST\n",
    "I've found this snippet somewhere in github\n",
    "\"\"\"\n",
    "\n",
    "#as we have 10 classes, I'm setting class number to 10\n",
    "class_nmr = 10\n",
    "\n",
    "print('We are using MNIST dataset!')\n",
    "(X_train, Y_train), (X_val, Y_val) = tf.keras.datasets.mnist.load_data()\n",
    "g = sns.countplot(Y_train)\n",
    "X_train = np.expand_dims(X_train, -1)\n",
    "n_train = X_train.shape[0]\n",
    "X_val = np.expand_dims(X_val, -1)\n",
    "Y_train = tf.keras.utils.to_categorical(Y_train, class_nmr)\n",
    "Y_val = tf.keras.utils.to_categorical(Y_val, class_nmr)\n",
    "\n",
    "# Normalize data\n",
    "X_train = X_train.astype('float32') / 255\n",
    "X_val = X_val.astype('float32') / 255\n",
    "\n",
    "print(\"X_train.shape =\", X_train.shape)\n",
    "print(\"Y_train.shape =\", Y_train.shape)\n",
    "print(\"Y_val.shape =\", X_val.shape)\n",
    "print(\"Y_val.shape =\", Y_val.shape)\n",
    "\n",
    "#plt.imshow(X_train[1026, :, :, 0], cmap='gist_gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, its almos uniformely distributed, so we're not doing any augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We normalize and reshape our data so we have a train set with shape (N, 28, 28, 1), being N the number of images. We also set our Y_train to categorical and divide our data into a train and test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Visualize the data\n",
    "\n",
    "If we want to visualize an image from our dataset, we gather its index and use matplotlib this way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True label of the test sample 1026: 8\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#We are using 1026, as it is my birthday\n",
    "idx = 1026\n",
    "plt.imshow(X_train[idx, :, :, 0], cmap='gist_gray')\n",
    "print(\"True label of the test sample {}: {}\".format(idx, np.argmax(Y_train[idx], axis=-1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Ilustrating the problem with a deterministic CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Implementing and training the deterministic model\n",
    "\n",
    "We will, below, implement a deterministic model for predicting on the MNIST dataset using Keras interface, as there will be no custom function or module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn(input_shape):\n",
    "    \n",
    "    ##model building\n",
    "    model = tf.keras.models.Sequential()\n",
    "    #convolutional layer with rectified linear unit activation\n",
    "    model.add(tf.keras.layers.Conv2D(32, kernel_size=(3, 3),\n",
    "                     activation='relu',\n",
    "                     input_shape=input_shape))\n",
    "    #32 convolution filters used each of size 3x3\n",
    "    #again\n",
    "    model.add(tf.keras.layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    #64 convolution filters used each of size 3x3\n",
    "    #choose the best features via pooling\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    #randomly turn neurons on and off to improve convergence\n",
    "    model.add(tf.keras.layers.Dropout(0.25))\n",
    "    #flatten since too many dimensions, we only want a classification output\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    #fully connected to get all relevant data\n",
    "    model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "    #one more dropout for convergence' sake :) \n",
    "    model.add(tf.keras.layers.Dropout(0.5))\n",
    "    #output a softmax to squash the matrix into output probabilities\n",
    "    model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we build the network, instance an optizmier and compile it\n",
    "cnn = build_cnn(X_train.shape[1:])\n",
    "optimizer = tf.keras.optimizers.Adam(lr=0.01)\n",
    "cnn.compile(loss=tf.keras.losses.categorical_crossentropy,\n",
    "              metrics=['accuracy'], optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/10\n",
      "54000/54000 [==============================] - 41s 752us/sample - loss: 0.3944 - accuracy: 0.8782 - val_loss: 0.0902 - val_accuracy: 0.9730\n",
      "Epoch 2/10\n",
      "54000/54000 [==============================] - 39s 727us/sample - loss: 0.2541 - accuracy: 0.9239 - val_loss: 0.0816 - val_accuracy: 0.9778\n",
      "Epoch 3/10\n",
      "54000/54000 [==============================] - 38s 708us/sample - loss: 0.2345 - accuracy: 0.9308 - val_loss: 0.0882 - val_accuracy: 0.9757\n",
      "Epoch 4/10\n",
      "54000/54000 [==============================] - 37s 680us/sample - loss: 0.2346 - accuracy: 0.9314 - val_loss: 0.0749 - val_accuracy: 0.9813\n",
      "Epoch 5/10\n",
      "54000/54000 [==============================] - 37s 692us/sample - loss: 0.2228 - accuracy: 0.9351 - val_loss: 0.0786 - val_accuracy: 0.9807\n",
      "Epoch 6/10\n",
      "54000/54000 [==============================] - 38s 713us/sample - loss: 0.2248 - accuracy: 0.9353 - val_loss: 0.0784 - val_accuracy: 0.9753\n",
      "Epoch 7/10\n",
      "54000/54000 [==============================] - 37s 690us/sample - loss: 0.2224 - accuracy: 0.9373 - val_loss: 0.0724 - val_accuracy: 0.9803\n",
      "Epoch 8/10\n",
      "54000/54000 [==============================] - 38s 700us/sample - loss: 0.2221 - accuracy: 0.9368 - val_loss: 0.0866 - val_accuracy: 0.9750\n",
      "Epoch 9/10\n",
      "30496/54000 [===============>..............] - ETA: 15s - loss: 0.2239 - accuracy: 0.9357"
     ]
    }
   ],
   "source": [
    "history = cnn.fit(X_train, Y_train, epochs=10, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = cnn.evaluate(X_val, Y_val, verbose=2)\n",
    "print(evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, we have very good metrics and it did converge very fastly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Predicting on real data and on noise: there is always a prediction\n",
    "\n",
    "As we are going to see, our neural network will predict outputs, as we see:\n",
    "\n",
    "For real data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_for_idx = cnn(X_train[idx:idx+1, :, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 1026\n",
    "plt.imshow(X_train[idx, :, :, 0], cmap='gist_gray')\n",
    "print(\"Predicted label of the test sample {}: {}\".format(idx, np.argmax(pred_for_idx[0]), axis=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For noise generated data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = np.random.random((28,28,1))\n",
    "pred_for_noise = cnn(np.array([noise]).astype(np.float32))\n",
    "plt.imshow(noise[:, :, 0], cmap='gist_gray')\n",
    "print(\"Predicted label of the test sample {}: {}\".format(idx, np.argmax(pred_for_noise[0]), axis=-1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we have the illustration of the problem of noise-like data on deterministic Neural Networks: it gains a label and is processed as that, but it does not represent anything. To address this issue, we're implementing the probabilistic network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Implementing a Bayesian Neural Network with TensorFlow 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Notes on the implementation\n",
    "\n",
    "In order to do our implementation, we are will implement the following functions:\n",
    "The model will be a keras one, but won't be compiled, as we are wusing TF 2.0 features\n",
    "* build_bcnn_model: builds a Bayesian Convnet for our MNIST classification;\n",
    "* elbo_loss: tf.function that calculates elbo_loss for our network\n",
    "* train_step: tf.function that trains our bcnn model\n",
    "* accuracy: gets the accuracy of the predictions for plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Implementing the Bayesian model with TFP Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probabilistic layers\n",
    "\n",
    "For us to implement the probabilistic layers we just have to call, from tfp, the layer. It can be added to a Keras model as any tf.nn layer. \n",
    "\n",
    "Our layers to be added are the DenseFlipout, which is the probabilistic counterpart for Dense and Conv2DFlipout, which is the probabilistic counterpart to Conv2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bayesian_bcnn_model(input_shape):\n",
    "    \n",
    "    \"\"\"\n",
    "    Here we use tf.keras.Model to use our graph as a Neural Network:\n",
    "    We select our input node as the net input, and the last node as our output (predict node).\n",
    "    Note that our model won't be compiled, as we are usign TF2.0 and will optimize it with\n",
    "    a custom @tf.function for loss and a @tf.function for train_step\n",
    "    Our input parameter is just the input shape, a tuple, for the input layer\n",
    "    \"\"\"\n",
    "    \n",
    "    model_in = tf.keras.layers.Input(shape=input_shape)\n",
    "    conv_1 = tfp.python.layers.Convolution2DFlipout(32, kernel_size=(3, 3), padding=\"same\", strides=2)\n",
    "    x = conv_1(model_in)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "    conv_2 = tfp.python.layers.Convolution2DFlipout(64, kernel_size=(3, 3), padding=\"same\", strides=2)\n",
    "    x = conv_2(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    dense_1 = tfp.python.layers.DenseFlipout(512, activation='relu')\n",
    "    x = dense_1(x)\n",
    "    dense_2 = tfp.python.layers.DenseFlipout(10, activation=None)\n",
    "    model_out = dense_2(x)  # logits\n",
    "    model = tf.keras.Model(model_in, model_out)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Implementing the ELBO Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "this is our loss function: a sum of KL Divergence and Softmax crossentropy\n",
    "We use the @tf.function annotation becuase of TF2.0, and need no placeholders\n",
    "we get each loss and return its mean\n",
    "\"\"\"\n",
    "\n",
    "@tf.function\n",
    "def elbo_loss(labels, logits):\n",
    "    loss_en = tf.nn.softmax_cross_entropy_with_logits(labels, logits)\n",
    "    loss_kl = tf.keras.losses.KLD(labels, logits)\n",
    "    loss = tf.reduce_mean(tf.add(loss_en, loss_kl))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Implementing the train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "this is our train step with tf2.0, very ellegant:\n",
    "We do our flow of the tensors over the model recording its gradientes\n",
    "Then, our gradient tape to give us a list of the gradients of each parameter in relation of the loss\n",
    "we dan ask our previously instanced optimizer to apply those gradients to the variable\n",
    "It is cool to see that it works even with TensorFlow probability- probabilistic layers parameters\n",
    "\"\"\"\n",
    "@tf.function\n",
    "def train_step(images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = bcnn(X_train)\n",
    "        loss = elbo_loss(labels, logits)\n",
    "    gradients = tape.gradient(loss, bcnn.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, bcnn.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "def accuracy(preds, labels):\n",
    "    return np.mean(np.argmax(preds, axis=1) == np.argmax(labels, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Wrap-up and train the BNN\n",
    "\n",
    "We will se that our BNN takes mroe time to start converging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bcnn = build_bayesian_bcnn_model(X_train.shape[1:])\n",
    "optimizer = tf.keras.optimizers.Adam(lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "in our train step we can see that it lasts more tha na normal CNN to converge\n",
    "on the other side, we can have the confidence interval for our predictions, which are \n",
    "wonderful in terms of taking sensitive predictions\n",
    "\"\"\"\n",
    "times = []\n",
    "accs = []\n",
    "val_accs = []\n",
    "losses = []\n",
    "val_losses = []\n",
    "for i in range(20):\n",
    "    tic = time.time()\n",
    "    loss = train_step(X_train, Y_train)\n",
    "    preds = bcnn(X_train)\n",
    "    acc = accuracy(preds, Y_train)\n",
    "    accs.append(acc)\n",
    "    losses.append(loss)\n",
    "    \n",
    "    val_preds = bcnn(X_val)\n",
    "    val_loss = elbo_loss(Y_val, val_preds)\n",
    "    val_acc = accuracy(Y_val, val_preds)\n",
    "    \n",
    "    val_accs.append(val_acc)\n",
    "    val_losses.append(val_loss)\n",
    "    tac = time.time()\n",
    "    train_time = tac-tic\n",
    "    times.append(train_time)\n",
    "    \n",
    "    print(\"Epoch: {}: loss = {:7.3f} , accuracy = {:7.3f}, val_loss = {:7.3f}, val_acc={:7.3f} time: {:7.3f}\".format(i, loss, acc, val_loss, val_acc, train_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy and loss evolution:\n",
    "\n",
    "Some metrics to illustrate its evolution. We can see from the logs above that our BNN is generalizing very well on the validation set. On the other side, it lasts much more to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting accuracy\n",
    "plt.plot(np.array(accs), label=\"acc\")\n",
    "plt.plot(np.array(val_accs), label=\"val_acc\")\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(np.array(losses), label=\"loss\")\n",
    "plt.plot(np.array(val_losses), label=\"val_loss\")\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Predicting with Monte Carlo methods and epistemic uncertainity gathering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Notes on the prediction method\n",
    "\n",
    "We will now use our trained Bayesian Neural Network to predict on labeled real values and noisy ones. Our approach will consist in sampling 30 predictions for each value, then see the prediction distribution. The neural network will be set to \"I don't know\" if it cant predict, with confidence, any value. To predict a value, using the montecarlo method, the median of the labels predicted probability must surpass 0.35.\n",
    "\n",
    "We want to show that, the more the sampled networks predict on the mode prediction, the more reliable the prediction will be. On the other side, \"confuse\" and uncertain predictions will have the predictions distributed on more labels, approximating to a uniform distribution.\n",
    "\n",
    "In order to to that, we are implementing functions to gather and plot those distributions.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot prediction histogram function:\n",
    "\n",
    "The function below plots the histograms for each label probabilities prediction and will be used to visualize our data. This function was mostly from Zhullingchen GitHub (with minor changes).\n",
    "\n",
    "Here is the link of the repo, which I strongly recomend you give a look: [TFP tutorial from Zhulingchen](https://github.com/zhulingchen/tfp-tutorial/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pred_hist(y_pred, n_class, n_mc_run, n_bins=30, med_prob_thres=0.2, n_subplot_rows=2, figsize=(25, 10)):\n",
    "    bins = np.logspace(-n_bins, 0, n_bins+1)\n",
    "    fig, ax = plt.subplots(n_subplot_rows, n_class // n_subplot_rows + 1, figsize=figsize)\n",
    "    for i in range(n_subplot_rows):\n",
    "        for j in range(n_class // n_subplot_rows + 1):\n",
    "            idx = i * (n_class // n_subplot_rows + 1) + j\n",
    "            if idx < n_class:\n",
    "                ax[i, j].hist(y_pred[idx], bins)\n",
    "                ax[i, j].set_xscale('log')\n",
    "                ax[i, j].set_ylim([0, n_mc_run])\n",
    "                ax[i, j].title.set_text(\"{} (median probability: {:.2f}) ({})\".format(str(idx),\n",
    "                                                                               np.median(y_pred[idx]),\n",
    "                                                                               str(np.median(y_pred[idx]) >= med_prob_thres)))\n",
    "            else:\n",
    "                ax[i, j].axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Gathering the prediction distribution\n",
    "\n",
    "On the snippet below, we just apply the Monte Carlo technique to sample and predict labels 30 times for each image on the validation set. We then store those predictions on lists to plot the distribution and gather insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_mc_run = 50\n",
    "med_prob_thres = 0.35\n",
    "\n",
    "y_pred_logits_list = [bcnn(X_val) for _ in range(n_mc_run)]  # a list of predicted logits\n",
    "y_pred_prob_all = np.concatenate([tf.nn.softmax(y, axis=-1)[:, :, np.newaxis] for y in y_pred_logits_list], axis=-1)\n",
    "y_pred = [[int(np.median(y) >= med_prob_thres) for y in y_pred_prob] for y_pred_prob in y_pred_prob_all]\n",
    "y_pred = np.array(y_pred)\n",
    "\n",
    "idx_valid = [any(y) for y in y_pred]\n",
    "print('Number of recognizable samples:', sum(idx_valid))\n",
    "\n",
    "idx_invalid = [not any(y) for y in y_pred]\n",
    "print('Unrecognizable samples:', np.where(idx_invalid)[0])\n",
    "\n",
    "print('Test accuracy on MNIST (recognizable samples):',\n",
    "      sum(np.equal(np.argmax(Y_val[idx_valid], axis=-1), np.argmax(y_pred[idx_valid], axis=-1))) / len(Y_val[idx_valid]))\n",
    "\n",
    "print('Test accuracy on MNIST (unrecognizable samples):',\n",
    "      sum(np.equal(np.argmax(Y_val[idx_invalid], axis=-1), np.argmax(y_pred[idx_invalid], axis=-1))) / len(Y_val[idx_invalid]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Visualizing valid predictions\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we just visualize some well predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X_val[0, :, :, 0], cmap='gist_gray')\n",
    "print(\"True label of the test sample {}: {}\".format(0, np.argmax(Y_val[0], axis=-1)))\n",
    "\n",
    "plot_pred_hist(y_pred_prob_all[0], class_nmr, n_mc_run, med_prob_thres=med_prob_thres)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Visualizing non-predicted values\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On this snippet we do visualize the noisy images which the network is not able to predict on properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in np.where(idx_invalid)[0]:\n",
    "    plt.imshow(X_val[idx, :, :, 0], cmap='gist_gray')\n",
    "    print(\"True label of the test sample {}: {}\".format(idx, np.argmax(Y_val[idx], axis=-1)))\n",
    "\n",
    "    plot_pred_hist(y_pred_prob_all[idx], class_nmr, n_mc_run, med_prob_thres=med_prob_thres)\n",
    "\n",
    "    if any(y_pred[idx]):\n",
    "        print(\"Predicted label of the test sample {}: {}\".format(idx, np.argmax(y_pred[idx], axis=-1)))\n",
    "    else:\n",
    "        print(\"I don't know!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusion "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a conclusion, we found that \n",
    "1.  Bayesian Neural Networks may be more reliable than the deterministic ones;\n",
    "2.  For sensible decision taking, it may be the more \"ethical approach\", due to the possibility of avoiding taking risky decision which the model is not trained to predict;\n",
    "3.  Bayesian Networks are more prone to avoid overfitting, due to the weights uncertainity;\n",
    "4.  On the other side, Bayesian Networks do last more to train and to start having some good accuracy on which we can rely on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. References and material if you want to know more\n",
    "* [Gluon BNN tutorial](https://gluon.mxnet.io/chapter18_variational-methods-and-uncertainty/bayes-by-backprop.html)\n",
    "\n",
    "* [Weight Uncertainty in Neural Networks](https://arxiv.org/pdf/1505.05424.pdf)\n",
    "\n",
    "* [TensorFlow 2.0 quickstart tutorial](https://www.tensorflow.org/tutorials/quickstart/)\n",
    "\n",
    "* [TensorFlow Probability main page and tutorials](https://www.tensorflow.org/probability)\n",
    "\n",
    "* [An excelent TFP tutorial from Zhulingchen](https://github.com/zhulingchen/tfp-tutorial/)\n",
    "\n",
    "* [My github repo with the implementetion (in case you want to run it on your local machine)](https://github.com/piEsposito/easy-bnn-with-tf2.0)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
