{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Neural Networks: variational inference with epistemic uncertainity, i.e., the Neural Networks who can say \"I don't know\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pi Esposito, AI Software Development Intern @ Intel Corporation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we are going to work with some non \"Kaggle default\" packages, you may want to uncomment those pip install for gathering the packages used on this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install -U pip\n",
    "#%pip install tensorflow==2.0\n",
    "#%pip install tfp-nightly\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Objective:** \n",
    "    This kernel aims to **(i)** justify the **need for Neural Network models which are able to output the uncertainity for its predictions**, i.e., let the Data Scientist know if his model is not able to perform a reliable prediction on the data which he tried to predict the label; and **(ii)** **present the Bayesian approach to Neural Networks** as a way to supply this need.\n",
    "    \n",
    "    \n",
    "* **Note:** In order to ease the explanation of the mathematical part of the BNNs (without any theorethical loss), Neural Networks will be interpreted not as a maximum likelihood estimation for the training data (statistical interpretation), but as and optimized function to model the train data in function of its labels.  \n",
    "\n",
    "\n",
    "* **Packages used**:\n",
    "    * TensorFlow 2.0\n",
    "    * TensorFlow Probability (for probabilistic NN layers)\n",
    "    * Matplotlib\n",
    "    * Numpy\n",
    "    * Pandas\n",
    "    * Seaborn\n",
    "    * Sklearn train_test_split function\n",
    "    * time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version (expected = 2.0.0): 2.0.0-beta1\n",
      "TensorFlow Probability version (expected = 0.9.0-dev20190912): 0.9.0-dev20190913\n"
     ]
    }
   ],
   "source": [
    "print('TensorFlow version (expected = 2.0.0):', tf.__version__)\n",
    "print('TensorFlow Probability version (expected = 0.9.0-dev20190912):', tfp.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **1. Introduction**\n",
    "    * 1.1 The \"need\" for uncertainity gathering on predictions\n",
    "    * 1.2 The Bayesian approach for Neural Networks\n",
    "    * 1.3 ELBO loss function\n",
    "    * 1.4 Notes on TensorFlow 2.0 and TensorFlow Probability \n",
    "* **2. Data preparation**\n",
    "    * 2.1 Load, normalize and reshape data\n",
    "    * 2.2 Visualizing the data\n",
    "* **3. Ilustrating the problem with a deterministic CNN**\n",
    "    * 3.1 Implementing and training the deterministic model\n",
    "    * 3.2 Predicting on real data and on noise: there is always a prediction\n",
    "* **4. Implementing a Bayesian Neural Network with TensorFlow 2.0**\n",
    "    * 4.1 Notes on the implementation\n",
    "    * 4.2 Implementing the Bayesian model with TFP Layers\n",
    "    * 4.3 Implementing the ELBO Loss function\n",
    "    * 4.4 Implementing the train function\n",
    "    * 4.5 Wrap-up and train the BNN\n",
    "* **5. Predicting with Monte Carlo methods and epistemic uncertainity gathering**\n",
    "    * 5.1 Notes on the prediction method\n",
    "    * 5.2 Implementing the prediction distribution function\n",
    "    * 5.3 Visualizing valid predictions\n",
    "    * 5.4 Visualizing non-predicted values\n",
    "    * 5.5 Try to predict on noise\n",
    "* **6. Conclusion ** \n",
    "* **7. References **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "## 1. Introduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 The \"need\" for uncertainity gathering on predictions\n",
    "\n",
    "Neural Networks do work by performing feedforward and outputing logits, which can be used as predictions or interpreted as wished. \n",
    "\n",
    "Despite being very accurate and able to model (with the right hyperparameters) any kind of mathematical function, NNs do operate with any kind of data (if encoded in a tensor according to its input shape). For illustration purposes, it means that, on the case of a image-classifier CNN, once I've encoded an image as a tensor as the same of its input, I'm able to try to predict its label even if my image has nothing to see with my classificator purpose. It also means that, even when I'm trying to predict a very noisy image (which can almos be seen as noise), the network will output a deterministic prediction for it (** see point 3. for illustration**). \n",
    "\n",
    "We understand that it would be nice to know how certain our network is of its prediction. For example, in an autonomous car, it should be mandatory to have threshold on which, even if it does not recognize a pedestrian, depending on the uncertanity of its prediction, the car acts as if there was one. This goes for many examples on sensible decisions on which life is involved. \n",
    "\n",
    "We also understand that, in some financial cases, (e.g., neural-network consulting stock trader bot) it would also be nice to understand if there is a situation where it is not sure on how to behave and thus stop trading in order to avoid money loss.\n",
    "\n",
    "**In short, the point is that it would be very beneficial for Deep Learning guided decision process if the neural network could know if it is not trained to understand the data fed on it and output something which can be interpreted as \"I'm interpreting this as noise\".**[](http://)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 The Bayesian approach for Neural Networks\n",
    "\n",
    "In order to address that issue, it was created a Bayesian Approach to Neural Networks. It consists on building networks if layer which we will call **proabilistic layers**. This lets gather uncertainity on our predictions, as we will see."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Probabilistic layers\n",
    "\n",
    "Probabilistic layers are neural network layers on which the weights are not deterministic. They are actually **random variables (normally distributed) sampled differently on each feedforward step of the network**. When the layer is being built, for each weight, it randomly starts **mean and standard deviation, which are the trainable parameters**. \n",
    "\n",
    "**When the layer is called (i.e., on each feedforward step), it samples a weight from the distribution and runs normally.**\n",
    "\n",
    "**On the backpropagation step, for each trainable random variable, the gradients are the partial derivative from the Loss in relative the mean and standard deviation**.\n",
    "\n",
    "This layer-sampling step means that, if we predict many times on the same input, we will have different weights sampled (according to the distribution). Thus, noisy images as prone to have more \"uniform-like\" distribution, being more sensible to the feature extraction layer changes (as they have no features at all), while real clean images will be less sensible to layer changing and ar prone to keep the same prediction (its label).\n",
    "\n",
    "To illustrate the feedforward step, we see a  bayesian network on the right side. We see that the bayesian one has a different probability distribution for each weight.\n",
    "\n",
    "![](https://raw.githubusercontent.com/zackchase/mxnet-the-straight-dope/master/img/bbb_nn_bayes.png)\n",
    "[Credits for Gluon](https://gluon.mxnet.io/chapter18_variational-methods-and-uncertainty/bayes-by-backprop.html)\n",
    "\n",
    "#### Uncertanity gathering\n",
    "\n",
    "To gather uncertanity on our predictions, we use Monte Carlo methods and predict it N times, saving each sampled prediction. We then get the prediction distribution and, using a threshold, decide that, if the mode of the prediction represent less then the threshold, then the network should not decide at all and we do not consider the prediction. Our \"reliability\" of the predicition the % of the total predictions that the mode represent. \n",
    "\n",
    "Real data tend to have the mode representing almost 100%, while the more noisy it is, the more it uniform distribution it will have.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 ELBO loss function \n",
    "\n",
    "\n",
    "As we are working with Probability distribution, we will use a combination of the KL divergence, which is a mathematical representation of the information loss on representing the labels of the train features-label pair with the feedforward representation with the sparse categorical crossentropy, as is usually done on classification NN modelling.\n",
    "\n",
    "As TensorFlow lets us calculate each of this losses, wonÂ´t have to code it by hand. If you want to know more about the derivatives and the math, I'm putting a link for the original paper on the end of this kernel.\n",
    "\n",
    "By the way, we call this function the ELBO loss functon, which stants for expected lower bound.\n",
    "It is defined as:\n",
    "\n",
    "(The KL Divergence of our predicted distribution relative to the labels plus the crossentropy of our predicted relative to the labels - being Q the predicted probabilities or logits):[](http://)\n",
    "\n",
    "\\begin{equation*}\n",
    "* \\mathcal{L}(\\mathcal{D}, \\mathbf{\\theta}) = \\text{KL}[q(\\mathbf{w}\\ |\\ \\mathbf{\\theta})\\ ||\\ P(\\mathbf{w})] - \\mathbb{E}_{q(\\mathbf{w}\\ |\\ \\mathbf{\\theta})}[\\log P(\\mathcal{D}\\ |\\ \\mathbf{w})]\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Notes on TensorFlow 2.0 and TensorFlow probability\n",
    "\n",
    "First, some useful links:\n",
    "\n",
    "* [TensorFlow 2.0 quickstart tutorial](https://www.tensorflow.org/tutorials/quickstart/)\n",
    "\n",
    "* [TensorFlow Probability main page and tutorials](https://www.tensorflow.org/probability)\n",
    "\n",
    "#### TensorFlow 2.0\n",
    "##### All the features explained here will be shown during the implementations\n",
    "* TensorFlow 2.0 has ended tf.Session(), feed_dict and sess.run. We now just have to call the node of the graph to get its value. TensorFlow 2.0 has also ended the need for placeholders, giving us a much more intuitive way to flow tensors thorugh graphs:\n",
    "\n",
    "* We define it as a funcion with the inputs a parameters. We do the operations and return the nodes we want to. The only thing we have to do is put @tf.function decorator above the definition. We can calculate losses and optimize models with tf.functions in a much easier way.\n",
    "\n",
    "* For gradient appliying, we have gradientTape which gets the derivative of our last operation relative to all the nodes conected to it, and a powerful and easy way to apply those gradients with the optimizer.\n",
    "\n",
    "\n",
    "\n",
    "#### TensorFlow Probability\n",
    "\n",
    "* The probabilistic counterpart os TensorFlow has, on its features, some probabilistic counterparts for layers, so we don't have to bother in implementing probabilistic layers.\n",
    "\n",
    "* The nightly version of TFP works well with all the TensorFlow 2.0 features, and so it will be very easy to write custom losses and manually optimizing our Bayesian Network on the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Load, normalize and reshape the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are using MNIST dataset!\n",
      "X_train.shape = (60000, 28, 28, 1)\n",
      "Y_train.shape = (60000, 10)\n",
      "X_val.shape = (10000, 28, 28, 1)\n",
      "Y_val.shape = (10000, 10)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAD8CAYAAABgmUMCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fafaae64320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "first, we load the dataset. We are trying to do it first with MNIST\n",
    "I've found this snippet somewhere in github\n",
    "\"\"\"\n",
    "\n",
    "#as we have 10 classes, I'm setting class number to 10\n",
    "class_nmr = 10\n",
    "\n",
    "print('We are using MNIST dataset!')\n",
    "(X_train, Y_train), (X_val, Y_val) = tf.keras.datasets.mnist.load_data()\n",
    "g = sns.countplot(Y_train)\n",
    "X_train = np.expand_dims(X_train, -1)\n",
    "n_train = X_train.shape[0]\n",
    "X_val = np.expand_dims(X_val, -1)\n",
    "Y_train = tf.keras.utils.to_categorical(Y_train, class_nmr)\n",
    "Y_val = tf.keras.utils.to_categorical(Y_val, class_nmr)\n",
    "\n",
    "# Normalize data\n",
    "X_train = X_train.astype('float32') / 255\n",
    "X_val = X_val.astype('float32') / 255\n",
    "\n",
    "print(\"X_train.shape =\", X_train.shape)\n",
    "print(\"Y_train.shape =\", Y_train.shape)\n",
    "print(\"X_val.shape =\", X_val.shape)\n",
    "print(\"Y_val.shape =\", Y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, its almos uniformely distributed, so we're not doing any augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We normalize and reshape our data so we have a train set with shape (N, 28, 28, 1), being N the number of images. We also set our Y_train to categorical and divide our data into a train and test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Visualize the data\n",
    "\n",
    "If we want to visualize an image from our dataset, we gather its index and use matplotlib this way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True label of the test sample 1026: 8\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhB8IivADQf0bIHV3m9z7zF4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fafaaf2deb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#We are using 1026, as it is my birthday\n",
    "idx = 1026\n",
    "plt.imshow(X_train[idx, :, :, 0], cmap='gist_gray')\n",
    "print(\"True label of the test sample {}: {}\".format(idx, np.argmax(Y_train[idx], axis=-1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Ilustrating the problem with a deterministic CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Implementing and training the deterministic model\n",
    "\n",
    "We will, below, implement a deterministic model for predicting on the MNIST dataset using Keras interface, as there will be no custom function or module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn(input_shape):\n",
    "    \n",
    "    ##model building\n",
    "    model = tf.keras.models.Sequential()\n",
    "    #convolutional layer with rectified linear unit activation\n",
    "    model.add(tf.keras.layers.Conv2D(32, kernel_size=(3, 3),\n",
    "                     activation='relu',\n",
    "                     input_shape=input_shape))\n",
    "    #32 convolution filters used each of size 3x3\n",
    "    #again\n",
    "    model.add(tf.keras.layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    #64 convolution filters used each of size 3x3\n",
    "    #choose the best features via pooling\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    #randomly turn neurons on and off to improve convergence\n",
    "    model.add(tf.keras.layers.Dropout(0.25))\n",
    "    #flatten since too many dimensions, we only want a classification output\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    #fully connected to get all relevant data\n",
    "    model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "    #one more dropout for convergence' sake :) \n",
    "    model.add(tf.keras.layers.Dropout(0.5))\n",
    "    #output a softmax to squash the matrix into output probabilities\n",
    "    model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we build the network, instance an optizmier and compile it\n",
    "cnn = build_cnn(X_train.shape[1:])\n",
    "optimizer = tf.keras.optimizers.Adam(lr=0.01)\n",
    "cnn.compile(loss=tf.keras.losses.categorical_crossentropy,\n",
    "              metrics=['accuracy'], optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1008 13:23:07.264420 140394748251904 deprecation.py:323] From /home/u30073/.local/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/10\n",
      "54000/54000 [==============================] - 29s 539us/sample - loss: 0.4914 - accuracy: 0.8471 - val_loss: 0.1060 - val_accuracy: 0.9683\n",
      "Epoch 2/10\n",
      "53952/54000 [============================>.] - ETA: 0s - loss: 0.3016 - accuracy: 0.9100"
     ]
    }
   ],
   "source": [
    "history = cnn.fit(X_train, Y_train, epochs=10, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = cnn.evaluate(X_val, Y_val, verbose=2)\n",
    "print(evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, we have very good metrics and it did converge very fastly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Predicting on real data and on noise: there is always a prediction\n",
    "\n",
    "As we are going to see, our neural network will predict outputs, as we see:\n",
    "\n",
    "For real data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_for_idx = cnn(X_train[idx:idx+1, :, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 1026\n",
    "plt.imshow(X_train[idx, :, :, 0], cmap='gist_gray')\n",
    "print(\"Predicted label of the test sample {}: {}\".format(idx, np.argmax(pred_for_idx[0]), axis=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For noise generated data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = np.random.random((28,28,1))\n",
    "pred_for_noise = cnn(np.array([noise]))\n",
    "plt.imshow(noise[:, :, 0], cmap='gist_gray')\n",
    "print(\"Predicted label of the test sample {}: {}\".format(idx, np.argmax(pred_for_noise[0]), axis=-1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we have the illustration of the problem of noise-like data on deterministic Neural Networks: it gains a label and is processed as that, but it does not represent anything. To address this issue, we're implementing the probabilistic network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Implementing a Bayesian Neural Network with TensorFlow 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Notes on the implementation\n",
    "\n",
    "In order to do our implementation, we are will implement the following functions:\n",
    "The model will be a keras one, but won't be compiled, as we are wusing TF 2.0 features\n",
    "* build_bcnn_model: builds a Bayesian Convnet for our MNIST classification;\n",
    "* elbo_loss: tf.function that calculates elbo_loss for our network\n",
    "* train_step: tf.function that trains our bcnn model\n",
    "* accuracy: gets the accuracy of the predictions for plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Implementing the Bayesian model with TFP Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bayesian_bcnn_model(input_shape):\n",
    "    \n",
    "    \"\"\"\n",
    "    Here we use tf.keras.Model to use our graph as a Neural Network:\n",
    "    We select our input node as the net input, and the last node as our output (predict node).\n",
    "    Note that our model won't be compiled, as we are usign TF2.0 and will optimize it with\n",
    "    a custom @tf.function for loss and a @tf.function for train_step\n",
    "    Our input parameter is just the input shape, a tuple, for the input layer\n",
    "    \"\"\"\n",
    "    \n",
    "    model_in = tf.keras.layers.Input(shape=input_shape)\n",
    "    conv_1 = tfp.python.layers.Convolution2DFlipout(32, kernel_size=(3, 3), padding=\"same\", strides=2)\n",
    "    x = conv_1(model_in)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "    conv_2 = tfp.python.layers.Convolution2DFlipout(64, kernel_size=(3, 3), padding=\"same\", strides=2)\n",
    "    x = conv_2(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    dense_1 = tfp.python.layers.DenseFlipout(512, activation='relu')\n",
    "    x = dense_1(x)\n",
    "    dense_2 = tfp.python.layers.DenseFlipout(10, activation=None)\n",
    "    model_out = dense_2(x)  # logits\n",
    "    model = tf.keras.Model(model_in, model_out)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Implementing the ELBO Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "this is our loss function: a sum of KL Divergence and Softmax crossentropy\n",
    "We use the @tf.function annotation becuase of TF2.0, and need no placeholders\n",
    "we get each loss and return its mean\n",
    "\"\"\"\n",
    "\n",
    "@tf.function\n",
    "def elbo_loss(labels, logits):\n",
    "    loss_en = tf.nn.softmax_cross_entropy_with_logits(labels, logits)\n",
    "    loss_kl = tf.keras.losses.KLD(labels, logits)\n",
    "    loss = tf.reduce_mean(tf.add(loss_en, loss_kl))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Implementing the train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "this is our train step with tf2.0, very ellegant:\n",
    "We do our flow of the tensors over the model recording its gradientes\n",
    "Then, our gradient tape to give us a list of the gradients of each parameter in relation of the loss\n",
    "we dan ask our previously instanced optimizer to apply those gradients to the variable\n",
    "It is cool to see that it works even with TensorFlow probability- probabilistic layers parameters\n",
    "\"\"\"\n",
    "@tf.function\n",
    "def train_step(images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = bcnn(X_train)\n",
    "        loss = elbo_loss(labels, logits)\n",
    "    gradients = tape.gradient(loss, bcnn.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, bcnn.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "def accuracy(preds, labels):\n",
    "    return np.mean(np.argmax(preds, axis=1) == np.argmax(labels, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Wrap-up and train the BNN\n",
    "\n",
    "We will se that our BNN takes mroe time to start converging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bcnn = build_bayesian_bcnn_model(X_train.shape[1:])\n",
    "optimizer = tf.keras.optimizers.Adam(lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "in our train step we can see that it lasts more tha na normal CNN to converge\n",
    "on the other side, we can have the confidence interval for our predictions, which are \n",
    "wonderful in terms of taking sensitive predictions\n",
    "\"\"\"\n",
    "times = []\n",
    "accs = []\n",
    "val_accs = []\n",
    "losses = []\n",
    "val_losses = []\n",
    "for i in range(30):\n",
    "    tic = time.time()\n",
    "    loss = train_step(X_train, Y_train)\n",
    "    preds = bcnn(X_train)\n",
    "    acc = accuracy(preds, Y_train)\n",
    "    accs.append(accs)\n",
    "    losses.append(loss)\n",
    "    \n",
    "    val_preds = bcnn(X_val)\n",
    "    val_loss = elbo_loss(Y_val, val_preds)\n",
    "    val_acc = accuracy(Y_val, val_preds)\n",
    "    \n",
    "    val_accs.append(val_acc)\n",
    "    val_losses.append(val_loss)\n",
    "    tac = time.time()\n",
    "    train_time = tac-tic\n",
    "    times.append(train_time)\n",
    "    \n",
    "    print(\"Epoch: {}: loss = {:7.3f} , accuracy = {:7.3f}, val_loss = {:7.3f}, val_acc={:7.3f} time: {:7.3f}\".format(i, loss, acc, val_loss, val_acc, train_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some metrics to illustrate its evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting accuracy\n",
    "plt.plot(np.array(accs), label=\"acc\")\n",
    "plt.plot(np.array(val_accs), label=\"val_acc\")\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(np.array(losses), label=\"loss\")\n",
    "plt.plot(np.array(val_losses), label=\"val_loss\")\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Predicting with Monte Carlo methods and epistemic uncertainity gathering**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Notes on the prediction method\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Implementing the prediction distribution function\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Visualizing valid predictions\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Visualizing non-predicted values\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 Try to predict on noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusion "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. References\n",
    "* [Gluon BNN tutorial](https://gluon.mxnet.io/chapter18_variational-methods-and-uncertainty/bayes-by-backprop.html)\n",
    "\n",
    "* [Weight Uncertainty in Neural Networks](https://arxiv.org/pdf/1505.05424.pdf)\n",
    "\n",
    "* [TensorFlow 2.0 quickstart tutorial](https://www.tensorflow.org/tutorials/quickstart/)\n",
    "\n",
    "* [TensorFlow Probability main page and tutorials](https://www.tensorflow.org/probability)\n",
    "\n",
    "* [An excelent TFP tutorial from Zhulingchen]( https://github.com/zhulingchen/tfp-tutorial/ )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfp_nightly",
   "language": "python",
   "name": "tfp_nightly"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
