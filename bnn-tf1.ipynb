{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Neural Networks in Keras and TensorFlow 1.14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for env setup:\n",
    "#! . activate base\n",
    "#!pip install tensorflow-mkl --user\n",
    "#!pip uninstall tfp_nightly -y\n",
    "#!pip install tensorflow-probability\n",
    "#!python -m ipykernel install --name tf1test --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/u30073/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/u30073/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/u30073/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/u30073/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/u30073/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/u30073/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/glob/intel-python/versions/2018u2/intelpython3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/home/u30073/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/u30073/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/u30073/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/u30073/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/u30073/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/u30073/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "#TODO: PUT RANDOM SEED FOR RESULT SECURING IN DEMOS\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version (expected = 1.14.0): 1.14.0\n",
      "TensorFlow Probability version (expected = 0.7.0): 0.7.0\n"
     ]
    }
   ],
   "source": [
    "print('TensorFlow version (expected = 1.14.0):', tf.__version__)\n",
    "print('TensorFlow Probability version (expected = 0.7.0):', tfp.__version__)\n",
    "#trying to make it work with this version, as the nightly one does not import here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X, theta = 1.0, axis = None):\n",
    "    \"\"\"\n",
    "    Compute the softmax of each element along an axis of X.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X: ND-Array. Probably should be floats.\n",
    "    theta (optional): float parameter, used as a multiplier\n",
    "        prior to exponentiation. Default = 1.0\n",
    "    axis (optional): axis to compute values along. Default is the\n",
    "        first non-singleton axis.\n",
    "\n",
    "    Returns an array the same size as X. The result will sum to 1\n",
    "    along the specified axis.\n",
    "    \"\"\"\n",
    "\n",
    "    # make X at least 2d\n",
    "    y = np.atleast_2d(X)\n",
    "\n",
    "    # find axis\n",
    "    if axis is None:\n",
    "        axis = next(j[0] for j in enumerate(y.shape) if j[1] > 1)\n",
    "\n",
    "    # multiply y against the theta parameter,\n",
    "    y = y * float(theta)\n",
    "\n",
    "    # subtract the max for numerical stability\n",
    "    y = y - np.expand_dims(np.max(y, axis = axis), axis)\n",
    "\n",
    "    # exponentiate y\n",
    "    y = np.exp(y)\n",
    "\n",
    "    # take the sum along the specified axis\n",
    "    ax_sum = np.expand_dims(np.sum(y, axis = axis), axis)\n",
    "\n",
    "    # finally: divide elementwise\n",
    "    p = y / ax_sum\n",
    "\n",
    "    # flatten if X was 1D\n",
    "    if len(X.shape) == 1: p = p.flatten()\n",
    "\n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We are all set up, lets go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are using MNIST dataset!\n",
      "X_train.shape = (60000, 28, 28, 1)\n",
      "y_train.shape = (60000, 10)\n",
      "X_test.shape = (10000, 28, 28, 1)\n",
      "y_test.shape = (10000, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fd2194a10b8>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhB8IivADQf0bIHV3m9z7zF4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd21b2a82b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "first, we load the dataset. We are trying to do it first with MNIST\n",
    "I've found this snippet somewhere in github\n",
    "\"\"\"\n",
    "\n",
    "#as we have 10 classes, I'm setting class number to 10\n",
    "class_nmr = 10\n",
    "\n",
    "print('We are using MNIST dataset!')\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "X_train = np.expand_dims(X_train, -1)\n",
    "n_train = X_train.shape[0]\n",
    "X_test = np.expand_dims(X_test, -1)\n",
    "y_train = tf.keras.utils.to_categorical(y_train, class_nmr)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, class_nmr)\n",
    "\n",
    "# Normalize data\n",
    "X_train = X_train.astype('float32') / 255\n",
    "X_test = X_test.astype('float32') / 255\n",
    "\n",
    "print(\"X_train.shape =\", X_train.shape)\n",
    "print(\"y_train.shape =\", y_train.shape)\n",
    "print(\"X_test.shape =\", X_test.shape)\n",
    "print(\"y_test.shape =\", y_test.shape)\n",
    "\n",
    "plt.imshow(X_train[1026, :, :, 0], cmap='gist_gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BCNN:\n",
    "    def __init__(self):\n",
    "        self._build_model()\n",
    "        \n",
    "    def _build_model(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        we build the model and get the predict, loss and optimization nodes as atributes of the class object\n",
    "        we also put placeholders for the features and labels\n",
    "        \"\"\"\n",
    "        \n",
    "        self.model_in = tf.placeholder(shape=[None,28,28,1], dtype=\"float32\")\n",
    "        self.labels = tf.placeholder(shape=[None,10], dtype=\"float32\")\n",
    "        \n",
    "        \"\"\"\n",
    "        here we model the neural network, creating separate objects for the Probability layers\n",
    "        \"\"\"\n",
    "        self.conv_1 = tfp.python.layers.Convolution2DFlipout(32, kernel_size=(3, 3), padding=\"same\", strides=2)\n",
    "        self.x = self.conv_1(self.model_in)\n",
    "        self.x = tf.keras.layers.BatchNormalization()(self.x)\n",
    "        self.x = tf.keras.layers.Activation('relu')(self.x)\n",
    "        self.conv_2 = tfp.python.layers.Convolution2DFlipout(64, kernel_size=(3, 3), padding=\"same\", strides=2)\n",
    "        self.x = self.conv_2(self.x)\n",
    "        self.x = tf.keras.layers.BatchNormalization()(self.x)\n",
    "        self.x = tf.keras.layers.Activation('relu')(self.x)\n",
    "        self.x = tf.keras.layers.Flatten()(self.x)\n",
    "        self.dense_1 = tfp.python.layers.DenseFlipout(512, activation='relu')\n",
    "        self.x = self.dense_1(self.x)\n",
    "        self.dense_2 = tfp.python.layers.DenseFlipout(10, activation=None)\n",
    "        self.preds = self.dense_2(self.x)  # logits\n",
    "        \n",
    "        \"\"\"\n",
    "        the elbo loss function sumbs de kl divergence loss and the softmax crossentropy\n",
    "        our 'key'-node here is self.loss\n",
    "        \"\"\"\n",
    "        self.loss_en = tf.nn.softmax_cross_entropy_with_logits(labels=self.labels, logits=self.preds)\n",
    "        self.loss_kl = tf.keras.losses.KLD(self.labels, self.preds) \n",
    "        self.loss = tf.reduce_mean(tf.add(self.loss_en, self.loss_kl))\n",
    "        \n",
    "        \"\"\"\n",
    "        our train operation is also a graph node\n",
    "        \"\"\"\n",
    "        self.optimizer = tf.train.AdamOptimizer(0.01)\n",
    "        self.train_op = self.optimizer.minimize(self.loss)\n",
    "        \n",
    "    def predict(self, sess, X):\n",
    "        \"\"\"\n",
    "        prediciton with outdated sess.run and feed dict\n",
    "        \"\"\"\n",
    "        return sess.run(self.preds, { self.model_in: X })\n",
    "    \n",
    "    def update(self, sess, x, y):\n",
    "        \"\"\"\n",
    "        update with outdated sess.run and feed dict, also using the labels placeholder\n",
    "        \"\"\"\n",
    "        feed_dict = { self.model_in: x, self.labels: y }\n",
    "        _, loss = sess.run(\n",
    "            [self.train_op, self.loss],\n",
    "            feed_dict)\n",
    "        return loss\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(preds, labels):\n",
    "    return np.mean(np.argmax(preds, axis=1) == np.argmax(y_train, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0913 09:42:42.420956 140542604672768 deprecation.py:323] From <ipython-input-5-5c9c0b61c4e3>:36: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "W0913 09:42:42.490380 140542604672768 deprecation.py:323] From /home/u30073/.local/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "bcnn = BCNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The time for variable initalizing is :   0.192\n",
      "Epoch: 0: loss =  11.571 , accuracy =   0.155, time:  14.974\n",
      "Epoch: 1: loss =   7.987 , accuracy =   0.143, time:  10.589\n",
      "Epoch: 2: loss =  11.535 , accuracy =   0.175, time:  10.479\n",
      "Epoch: 3: loss =   7.399 , accuracy =   0.252, time:  10.153\n",
      "Epoch: 4: loss =   4.660 , accuracy =   0.262, time:  11.198\n",
      "Epoch: 5: loss =   3.006 , accuracy =   0.323, time:  11.676\n",
      "Epoch: 6: loss =   2.124 , accuracy =   0.442, time:  10.899\n",
      "Epoch: 7: loss =   1.813 , accuracy =   0.563, time:  11.088\n",
      "Epoch: 8: loss =   1.524 , accuracy =   0.627, time:   9.968\n",
      "Epoch: 9: loss =   1.194 , accuracy =   0.715, time:  10.071\n",
      "Epoch: 10: loss =   0.920 , accuracy =   0.708, time:  10.310\n",
      "Epoch: 11: loss =   0.847 , accuracy =   0.768, time:  10.722\n",
      "Epoch: 12: loss =   0.734 , accuracy =   0.786, time:  10.571\n",
      "Epoch: 13: loss =   0.718 , accuracy =   0.814, time:  10.693\n",
      "Epoch: 14: loss =   0.640 , accuracy =   0.830, time:  10.933\n",
      "Epoch: 15: loss =   0.577 , accuracy =   0.840, time:  10.472\n",
      "Epoch: 16: loss =   0.539 , accuracy =   0.852, time:  10.154\n",
      "Epoch: 17: loss =   0.509 , accuracy =   0.860, time:  10.422\n",
      "Epoch: 18: loss =   0.473 , accuracy =   0.864, time:  10.459\n",
      "Epoch: 19: loss =   0.462 , accuracy =   0.868, time:  10.134\n",
      "Epoch: 20: loss =   0.432 , accuracy =   0.881, time:  10.324\n",
      "Epoch: 21: loss =   0.411 , accuracy =   0.887, time:  10.395\n",
      "Epoch: 22: loss =   0.380 , accuracy =   0.894, time:  10.086\n",
      "Epoch: 23: loss =   0.362 , accuracy =   0.895, time:  10.317\n",
      "Epoch: 24: loss =   0.344 , accuracy =   0.903, time:  10.123\n",
      "Epoch: 25: loss =   0.334 , accuracy =   0.907, time:  10.254\n",
      "Epoch: 26: loss =   0.308 , accuracy =   0.912, time:  10.053\n",
      "Epoch: 27: loss =   0.299 , accuracy =   0.912, time:  10.022\n",
      "Epoch: 28: loss =   0.290 , accuracy =   0.919, time:  10.058\n",
      "Epoch: 29: loss =   0.275 , accuracy =   0.923, time:  10.225\n"
     ]
    }
   ],
   "source": [
    "times = []\n",
    "with tf.Session() as sess:   \n",
    "    tic = time.time()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    tac = time.time()\n",
    "    init_time = tac-tic\n",
    "    print(\"The time for variable initalizing is : {:7.3f}\".format(init_time))\n",
    "    for i in range(30):\n",
    "        tic = time.time()\n",
    "        loss = bcnn.update(x=X_train, sess=sess, y=y_train)\n",
    "        #print(loss)\n",
    "        preds = bcnn.predict(sess, X_train)\n",
    "        acc = accuracy(preds, y_train)\n",
    "        tac = time.time()\n",
    "        train_time = tac-tic\n",
    "        times.append(train_time)\n",
    "        print(\"Epoch: {}: loss = {:7.3f} , accuracy = {:7.3f}, time: {:7.3f}\".format(i, loss, acc, train_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In intel optimized Tensorflow 1.14, our train time mean was :  10.594, with std :   0.903\n"
     ]
    }
   ],
   "source": [
    "### about the performance\n",
    "mean = np.mean(times)\n",
    "std = np.std(times)\n",
    "print(\"In intel optimized Tensorflow 1.14, our train time mean was : {:7.3f}, with std : {:7.3f}\".format(mean, std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of recognizable samples: 9995\n",
      "Unrecognizable samples: [2211 3887 6982 7302 7767]\n",
      "Test accuracy on MNIST (recognizable samples): 0.08934467233616808\n",
      "Test accuracy on MNIST (unrecognizable samples): 0.0\n"
     ]
    }
   ],
   "source": [
    "n_mc_run = 50\n",
    "med_prob_thres = 0.20\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    y_pred_logits_list = [bcnn.predict(X=X_test, sess=sess) for _ in range(n_mc_run)]  # a list of predicted logits\n",
    "                                       \n",
    "y_pred_prob_all = np.concatenate([softmax(y, axis=-1)[:, :, np.newaxis] for y in y_pred_logits_list], axis=-1)\n",
    "y_pred = [[int(np.median(y) >= med_prob_thres) for y in y_pred_prob] for y_pred_prob in y_pred_prob_all]\n",
    "y_pred = np.array(y_pred)\n",
    "\n",
    "idx_valid = [any(y) for y in y_pred]\n",
    "print('Number of recognizable samples:', sum(idx_valid))\n",
    "\n",
    "idx_invalid = [not any(y) for y in y_pred]\n",
    "print('Unrecognizable samples:', np.where(idx_invalid)[0])\n",
    "\n",
    "print('Test accuracy on MNIST (recognizable samples):',\n",
    "      sum(np.equal(np.argmax(y_test[idx_valid], axis=-1), np.argmax(y_pred[idx_valid], axis=-1))) / len(y_test[idx_valid]))\n",
    "\n",
    "print('Test accuracy on MNIST (unrecognizable samples):',\n",
    "      sum(np.equal(np.argmax(y_test[idx_invalid], axis=-1), np.argmax(y_pred[idx_invalid], axis=-1))) / len(y_test[idx_invalid]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fd1f012ce10>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nuGrmojJO5gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd1f016d8d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_test[3887, :, :, 0], cmap='gist_gray')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf1test",
   "language": "python",
   "name": "tf1test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
