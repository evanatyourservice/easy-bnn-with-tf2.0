{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Neural Networks in Keras and TensorFlow 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! . activate base\n",
    "#!pip install tensorflow==2.0.0b1 --user\n",
    "#!pip install tfp_nightly --user\n",
    "#!python -m ipykernel install --name tf_prob --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/u30073/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/u30073/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/u30073/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/u30073/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/u30073/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/u30073/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/glob/intel-python/python3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/home/u30073/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/u30073/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/u30073/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/u30073/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/u30073/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/u30073/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "#TODO: PUT RANDOM SEED FOR RESULT SECURING IN DEMOS\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import cv2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version (expected = 2.0.0-beta1): 2.0.0-beta1\n",
      "TensorFlow Probability version (expected = 0.9.0-dev20190912): 0.9.0-dev20190913\n"
     ]
    }
   ],
   "source": [
    "print('TensorFlow version (expected = 2.0.0-beta1):', tf.__version__)\n",
    "print('TensorFlow Probability version (expected = 0.9.0-dev20190912):', tfp.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X, theta = 1.0, axis = None):\n",
    "    \"\"\"\n",
    "    Compute the softmax of each element along an axis of X.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X: ND-Array. Probably should be floats.\n",
    "    theta (optional): float parameter, used as a multiplier\n",
    "        prior to exponentiation. Default = 1.0\n",
    "    axis (optional): axis to compute values along. Default is the\n",
    "        first non-singleton axis.\n",
    "\n",
    "    Returns an array the same size as X. The result will sum to 1\n",
    "    along the specified axis.\n",
    "    \"\"\"\n",
    "\n",
    "    # make X at least 2d\n",
    "    y = np.atleast_2d(X)\n",
    "\n",
    "    # find axis\n",
    "    if axis is None:\n",
    "        axis = next(j[0] for j in enumerate(y.shape) if j[1] > 1)\n",
    "\n",
    "    # multiply y against the theta parameter,\n",
    "    y = y * float(theta)\n",
    "\n",
    "    # subtract the max for numerical stability\n",
    "    y = y - np.expand_dims(np.max(y, axis = axis), axis)\n",
    "\n",
    "    # exponentiate y\n",
    "    y = np.exp(y)\n",
    "\n",
    "    # take the sum along the specified axis\n",
    "    ax_sum = np.expand_dims(np.sum(y, axis = axis), axis)\n",
    "\n",
    "    # finally: divide elementwise\n",
    "    p = y / ax_sum\n",
    "\n",
    "    # flatten if X was 1D\n",
    "    if len(X.shape) == 1: p = p.flatten()\n",
    "\n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We are all set up, lets go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are using CIFAR10 dataset!\n",
      "X_train.shape = (50000, 32, 32, 3)\n",
      "y_train.shape = (50000, 10)\n",
      "X_test.shape = (10000, 32, 32, 3)\n",
      "y_test.shape = (10000, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f9f2a289518>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ncn4hEoqcX4iEIucXIqH8P1mpzvtETxLWAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9f317e44a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "first, we load the dataset. We are trying to do it first with CIFAR\n",
    "I've found this snippet somewhere in github\n",
    "\"\"\"\n",
    "\n",
    "#as we have 10 classes, I'm setting class number to 10\n",
    "class_nmr = 10\n",
    "\n",
    "print('We are using CIFAR10 dataset!')\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "#X_train = np.expand_dims(X_train, -1)\n",
    "n_train = X_train.shape[0]\n",
    "#X_test = np.expand_dims(X_test, -1)\n",
    "y_train = tf.keras.utils.to_categorical(y_train, class_nmr)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, class_nmr)\n",
    "\n",
    "# Normalize data\n",
    "X_train = X_train.astype('float32') / 255\n",
    "X_test = X_test.astype('float32') / 255\n",
    "\n",
    "print(\"X_train.shape =\", X_train.shape)\n",
    "print(\"y_train.shape =\", y_train.shape)\n",
    "print(\"X_test.shape =\", X_test.shape)\n",
    "print(\"y_test.shape =\", y_test.shape)\n",
    "\n",
    "plt.imshow(X_train[1026, :, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bayesian_bcnn_model(input_shape):\n",
    "    \n",
    "    \"\"\"\n",
    "    Here we use tf.keras.Model to use our graph as a Neural Network:\n",
    "    We select our input node as the net input, and the last node as our output (predict node).\n",
    "    Note that our model won't be compiled, as we are usign TF2.0 and will optimize it with\n",
    "    a custom @tf.function for loss and a @tf.function for train_step\n",
    "    Our input parameter is just the input shape, a tuple, for the input layer\n",
    "    \"\"\"\n",
    "    \n",
    "    model_in = tf.keras.layers.Input(shape=input_shape)\n",
    "    conv_1 = tfp.python.layers.Convolution2DFlipout(32, kernel_size=(3, 3), padding=\"same\", strides=2)\n",
    "    x = conv_1(model_in)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "    x = tf.keras.layers.MaxPool2D(pool_size=2, padding='SAME', data_format='channels_last')(x)\n",
    "    \n",
    "    conv_2 = tfp.python.layers.Convolution2DFlipout(64, kernel_size=(3, 3), padding=\"same\", strides=2)\n",
    "    x = conv_2(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "    x = tf.keras.layers.MaxPool2D(pool_size=2, padding='SAME', data_format='channels_last')(x)\n",
    "\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    x = tf.keras.layers.Dropout(rate=0.25)(x)\n",
    "    dense_1 = tfp.python.layers.DenseFlipout(512, activation='relu')\n",
    "    x = dense_1(x)\n",
    "    x = tf.keras.layers.Dropout(rate=0.25)(x)\n",
    "    dense_2 = tfp.python.layers.DenseFlipout(10, activation=None)\n",
    "    model_out = dense_2(x)  # logits\n",
    "    model = tf.keras.Model(model_in, model_out)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "here we are just instancing our model and setting up an Optimizer\n",
    "\"\"\"\n",
    "bcnn = build_bayesian_bcnn_model(X_train.shape[1:])\n",
    "optimizer = tf.keras.optimizers.Adam(lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 32, 32, 3)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d_flipout_2 (Conv2DFlip (None, 16, 16, 32)        1760      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 16, 16, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_flipout_3 (Conv2DFlip (None, 4, 4, 64)          36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 4, 4, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 2, 2, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_flipout_2 (DenseFlipou (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_flipout_3 (DenseFlipou (None, 10)                10250     \n",
      "=================================================================\n",
      "Total params: 311,978\n",
      "Trainable params: 311,786\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "bcnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "this is our loss function: a sum of KL Divergence and Softmax crossentropy\n",
    "We use the @tf.function annotation becuase of TF2.0, and need no placeholders\n",
    "we get each loss and return its mean\n",
    "\"\"\"\n",
    "\n",
    "@tf.function\n",
    "def elbo_loss(labels, logits):\n",
    "    loss_en = tf.nn.softmax_cross_entropy_with_logits(labels, logits)\n",
    "    loss_kl = tf.keras.losses.KLD(labels, logits)\n",
    "    loss = tf.reduce_mean(tf.add(loss_en, loss_kl))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "this is our train step with tf2.0, very ellegant:\n",
    "We do our flow of the tensors over the model recording its gradientes\n",
    "Then, our gradient tape to give us a list of the gradients of each parameter in relation of the loss\n",
    "we dan ask our previously instanced optimizer to apply those gradients to the variable\n",
    "It is cool to see that it works even with TensorFlow probability- probabilistic layers parameters\n",
    "\"\"\"\n",
    "@tf.function\n",
    "def train_step(images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = bcnn(X_train)\n",
    "        loss = elbo_loss(labels, logits)\n",
    "    gradients = tape.gradient(loss, bcnn.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, bcnn.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "def accuracy(preds, labels):\n",
    "    return np.mean(np.argmax(preds, axis=1) == np.argmax(labels, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0: loss =   9.110 , accuracy =   0.100, test_acc =   0.100 time:  34.716\n",
      "Epoch: 1: loss =   9.441 , accuracy =   0.104, test_acc =   0.102 time:   7.619\n",
      "Epoch: 2: loss =   4.565 , accuracy =   0.103, test_acc =   0.103 time:   7.635\n",
      "Epoch: 3: loss =   2.898 , accuracy =   0.109, test_acc =   0.109 time:   7.564\n",
      "Epoch: 4: loss =   2.888 , accuracy =   0.112, test_acc =   0.109 time:   8.266\n",
      "Epoch: 5: loss =   2.665 , accuracy =   0.109, test_acc =   0.109 time:   7.668\n",
      "Epoch: 6: loss =   2.675 , accuracy =   0.103, test_acc =   0.113 time:   7.634\n",
      "Epoch: 7: loss =   2.542 , accuracy =   0.106, test_acc =   0.109 time:   7.750\n",
      "Epoch: 8: loss =   2.494 , accuracy =   0.104, test_acc =   0.108 time:   7.461\n",
      "Epoch: 9: loss =   2.440 , accuracy =   0.104, test_acc =   0.102 time:   7.458\n",
      "Epoch: 10: loss =   2.395 , accuracy =   0.103, test_acc =   0.108 time:   7.581\n",
      "Epoch: 11: loss =   2.363 , accuracy =   0.104, test_acc =   0.105 time:   7.604\n",
      "Epoch: 12: loss =   2.341 , accuracy =   0.104, test_acc =   0.101 time:   7.617\n",
      "Epoch: 13: loss =   2.320 , accuracy =   0.104, test_acc =   0.107 time:   7.473\n",
      "Epoch: 14: loss =   2.310 , accuracy =   0.103, test_acc =   0.107 time:   7.652\n",
      "Epoch: 15: loss =   2.305 , accuracy =   0.106, test_acc =   0.106 time:   7.497\n",
      "Epoch: 16: loss =   2.306 , accuracy =   0.105, test_acc =   0.101 time:   7.535\n",
      "Epoch: 17: loss =   2.306 , accuracy =   0.108, test_acc =   0.107 time:   7.729\n",
      "Epoch: 18: loss =   2.307 , accuracy =   0.111, test_acc =   0.110 time:   7.551\n",
      "Epoch: 19: loss =   2.306 , accuracy =   0.112, test_acc =   0.115 time:   7.562\n",
      "Epoch: 20: loss =   2.306 , accuracy =   0.112, test_acc =   0.115 time:   7.496\n",
      "Epoch: 21: loss =   2.305 , accuracy =   0.111, test_acc =   0.110 time:   7.521\n",
      "Epoch: 22: loss =   2.304 , accuracy =   0.111, test_acc =   0.113 time:   7.559\n",
      "Epoch: 23: loss =   2.302 , accuracy =   0.113, test_acc =   0.116 time:   7.510\n",
      "Epoch: 24: loss =   2.302 , accuracy =   0.116, test_acc =   0.115 time:   7.585\n",
      "Epoch: 25: loss =   2.300 , accuracy =   0.114, test_acc =   0.116 time:   7.464\n",
      "Epoch: 26: loss =   2.299 , accuracy =   0.114, test_acc =   0.114 time:   7.527\n",
      "Epoch: 27: loss =   2.299 , accuracy =   0.116, test_acc =   0.116 time:   7.494\n",
      "Epoch: 28: loss =   2.297 , accuracy =   0.119, test_acc =   0.119 time:   7.481\n",
      "Epoch: 29: loss =   2.295 , accuracy =   0.121, test_acc =   0.116 time:   7.702\n",
      "Epoch: 30: loss =   2.293 , accuracy =   0.130, test_acc =   0.126 time:   7.541\n",
      "Epoch: 31: loss =   2.291 , accuracy =   0.129, test_acc =   0.131 time:   7.509\n",
      "Epoch: 32: loss =   2.288 , accuracy =   0.143, test_acc =   0.144 time:   7.609\n",
      "Epoch: 33: loss =   2.282 , accuracy =   0.153, test_acc =   0.141 time:   7.500\n",
      "Epoch: 34: loss =   2.278 , accuracy =   0.148, test_acc =   0.154 time:   7.607\n",
      "Epoch: 35: loss =   2.269 , accuracy =   0.162, test_acc =   0.166 time:   7.522\n",
      "Epoch: 36: loss =   2.264 , accuracy =   0.155, test_acc =   0.172 time:   7.550\n",
      "Epoch: 37: loss =   2.251 , accuracy =   0.169, test_acc =   0.173 time:   7.493\n",
      "Epoch: 38: loss =   2.243 , accuracy =   0.170, test_acc =   0.174 time:   7.630\n",
      "Epoch: 39: loss =   2.231 , accuracy =   0.166, test_acc =   0.178 time:   7.461\n",
      "Epoch: 40: loss =   2.220 , accuracy =   0.187, test_acc =   0.182 time:   7.557\n",
      "Epoch: 41: loss =   2.203 , accuracy =   0.188, test_acc =   0.189 time:   7.555\n",
      "Epoch: 42: loss =   2.198 , accuracy =   0.190, test_acc =   0.184 time:   7.550\n",
      "Epoch: 43: loss =   2.185 , accuracy =   0.201, test_acc =   0.203 time:   7.536\n",
      "Epoch: 44: loss =   2.168 , accuracy =   0.202, test_acc =   0.197 time:   7.524\n",
      "Epoch: 45: loss =   2.152 , accuracy =   0.209, test_acc =   0.209 time:   7.581\n",
      "Epoch: 46: loss =   2.153 , accuracy =   0.212, test_acc =   0.209 time:   7.545\n",
      "Epoch: 47: loss =   2.148 , accuracy =   0.221, test_acc =   0.220 time:   7.495\n",
      "Epoch: 48: loss =   2.132 , accuracy =   0.222, test_acc =   0.219 time:   7.531\n",
      "Epoch: 49: loss =   2.120 , accuracy =   0.206, test_acc =   0.225 time:   7.463\n",
      "Epoch: 50: loss =   2.115 , accuracy =   0.226, test_acc =   0.230 time:   7.524\n",
      "Epoch: 51: loss =   2.103 , accuracy =   0.228, test_acc =   0.230 time:   7.605\n",
      "Epoch: 52: loss =   2.106 , accuracy =   0.227, test_acc =   0.234 time:   7.730\n",
      "Epoch: 53: loss =   2.079 , accuracy =   0.245, test_acc =   0.227 time:   7.486\n",
      "Epoch: 54: loss =   2.072 , accuracy =   0.238, test_acc =   0.249 time:   7.626\n",
      "Epoch: 55: loss =   2.080 , accuracy =   0.245, test_acc =   0.232 time:   7.673\n",
      "Epoch: 56: loss =   2.055 , accuracy =   0.242, test_acc =   0.250 time:   7.583\n",
      "Epoch: 57: loss =   2.049 , accuracy =   0.252, test_acc =   0.251 time:   7.583\n",
      "Epoch: 58: loss =   2.031 , accuracy =   0.224, test_acc =   0.265 time:   7.565\n",
      "Epoch: 59: loss =   2.036 , accuracy =   0.242, test_acc =   0.258 time:   7.579\n",
      "Epoch: 60: loss =   2.017 , accuracy =   0.269, test_acc =   0.253 time:   7.488\n",
      "Epoch: 61: loss =   2.032 , accuracy =   0.263, test_acc =   0.235 time:   7.499\n",
      "Epoch: 62: loss =   2.076 , accuracy =   0.260, test_acc =   0.255 time:   7.613\n",
      "Epoch: 63: loss =   2.032 , accuracy =   0.271, test_acc =   0.276 time:   7.556\n",
      "Epoch: 64: loss =   1.991 , accuracy =   0.247, test_acc =   0.265 time:   7.514\n",
      "Epoch: 65: loss =   2.003 , accuracy =   0.278, test_acc =   0.271 time:   7.525\n",
      "Epoch: 66: loss =   2.046 , accuracy =   0.264, test_acc =   0.281 time:   7.495\n",
      "Epoch: 67: loss =   1.973 , accuracy =   0.276, test_acc =   0.287 time:   7.524\n",
      "Epoch: 68: loss =   1.985 , accuracy =   0.283, test_acc =   0.284 time:   7.597\n",
      "Epoch: 69: loss =   2.034 , accuracy =   0.275, test_acc =   0.270 time:   7.727\n",
      "Epoch: 70: loss =   1.958 , accuracy =   0.285, test_acc =   0.291 time:   7.601\n",
      "Epoch: 71: loss =   2.031 , accuracy =   0.275, test_acc =   0.290 time:   7.463\n",
      "Epoch: 72: loss =   1.968 , accuracy =   0.283, test_acc =   0.282 time:   7.658\n",
      "Epoch: 73: loss =   1.951 , accuracy =   0.296, test_acc =   0.293 time:   7.537\n",
      "Epoch: 74: loss =   1.947 , accuracy =   0.287, test_acc =   0.296 time:   7.579\n",
      "Epoch: 75: loss =   1.944 , accuracy =   0.305, test_acc =   0.304 time:   7.498\n",
      "Epoch: 76: loss =   1.947 , accuracy =   0.301, test_acc =   0.272 time:   7.664\n",
      "Epoch: 77: loss =   1.900 , accuracy =   0.298, test_acc =   0.296 time:   7.553\n",
      "Epoch: 78: loss =   1.891 , accuracy =   0.315, test_acc =   0.312 time:   7.535\n",
      "Epoch: 79: loss =   1.897 , accuracy =   0.290, test_acc =   0.307 time:   7.666\n",
      "Epoch: 80: loss =   1.940 , accuracy =   0.315, test_acc =   0.312 time:   7.626\n",
      "Epoch: 81: loss =   1.930 , accuracy =   0.318, test_acc =   0.319 time:   7.588\n",
      "Epoch: 82: loss =   1.885 , accuracy =   0.326, test_acc =   0.311 time:   7.633\n",
      "Epoch: 83: loss =   1.883 , accuracy =   0.321, test_acc =   0.321 time:   7.672\n",
      "Epoch: 84: loss =   1.975 , accuracy =   0.297, test_acc =   0.314 time:   7.558\n",
      "Epoch: 85: loss =   1.900 , accuracy =   0.327, test_acc =   0.311 time:   7.511\n",
      "Epoch: 86: loss =   1.870 , accuracy =   0.323, test_acc =   0.331 time:   7.570\n",
      "Epoch: 87: loss =   1.863 , accuracy =   0.313, test_acc =   0.324 time:   7.618\n",
      "Epoch: 88: loss =   1.861 , accuracy =   0.323, test_acc =   0.322 time:   7.593\n",
      "Epoch: 89: loss =   1.846 , accuracy =   0.330, test_acc =   0.323 time:   7.689\n",
      "Epoch: 90: loss =   1.850 , accuracy =   0.329, test_acc =   0.326 time:   7.517\n",
      "Epoch: 91: loss =   1.826 , accuracy =   0.330, test_acc =   0.328 time:   7.456\n",
      "Epoch: 92: loss =   1.848 , accuracy =   0.339, test_acc =   0.332 time:   7.495\n",
      "Epoch: 93: loss =   1.820 , accuracy =   0.332, test_acc =   0.333 time:   7.548\n",
      "Epoch: 94: loss =   1.853 , accuracy =   0.329, test_acc =   0.323 time:   7.640\n",
      "Epoch: 95: loss =   1.816 , accuracy =   0.340, test_acc =   0.353 time:   7.654\n",
      "Epoch: 96: loss =   1.825 , accuracy =   0.346, test_acc =   0.337 time:   7.612\n",
      "Epoch: 97: loss =   1.805 , accuracy =   0.338, test_acc =   0.337 time:   7.573\n",
      "Epoch: 98: loss =   1.789 , accuracy =   0.337, test_acc =   0.333 time:   7.619\n",
      "Epoch: 99: loss =   1.823 , accuracy =   0.346, test_acc =   0.338 time:   7.554\n",
      "Epoch: 100: loss =   1.801 , accuracy =   0.353, test_acc =   0.332 time:   7.595\n",
      "Epoch: 101: loss =   1.796 , accuracy =   0.363, test_acc =   0.337 time:   7.568\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 102: loss =   1.773 , accuracy =   0.342, test_acc =   0.337 time:   7.569\n",
      "Epoch: 103: loss =   1.802 , accuracy =   0.349, test_acc =   0.347 time:   7.573\n",
      "Epoch: 104: loss =   1.789 , accuracy =   0.363, test_acc =   0.360 time:   7.608\n",
      "Epoch: 105: loss =   1.786 , accuracy =   0.357, test_acc =   0.342 time:   7.553\n",
      "Epoch: 106: loss =   1.761 , accuracy =   0.355, test_acc =   0.352 time:   7.623\n",
      "Epoch: 107: loss =   1.829 , accuracy =   0.367, test_acc =   0.356 time:   7.581\n",
      "Epoch: 108: loss =   1.753 , accuracy =   0.352, test_acc =   0.347 time:   7.552\n",
      "Epoch: 109: loss =   1.734 , accuracy =   0.360, test_acc =   0.356 time:   7.596\n",
      "Epoch: 110: loss =   1.751 , accuracy =   0.367, test_acc =   0.364 time:   7.572\n",
      "Epoch: 111: loss =   1.744 , accuracy =   0.373, test_acc =   0.363 time:   7.580\n",
      "Epoch: 112: loss =   1.741 , accuracy =   0.367, test_acc =   0.361 time:   7.580\n",
      "Epoch: 113: loss =   1.757 , accuracy =   0.378, test_acc =   0.361 time:   7.518\n",
      "Epoch: 114: loss =   1.771 , accuracy =   0.372, test_acc =   0.364 time:   7.667\n",
      "Epoch: 115: loss =   1.733 , accuracy =   0.365, test_acc =   0.359 time:   7.544\n",
      "Epoch: 116: loss =   1.751 , accuracy =   0.374, test_acc =   0.360 time:   7.627\n",
      "Epoch: 117: loss =   1.749 , accuracy =   0.379, test_acc =   0.374 time:   7.627\n",
      "Epoch: 118: loss =   1.730 , accuracy =   0.381, test_acc =   0.371 time:   7.571\n",
      "Epoch: 119: loss =   1.722 , accuracy =   0.379, test_acc =   0.359 time:   7.548\n",
      "Epoch: 120: loss =   1.712 , accuracy =   0.366, test_acc =   0.373 time:   7.579\n",
      "Epoch: 121: loss =   1.693 , accuracy =   0.378, test_acc =   0.377 time:   7.537\n",
      "Epoch: 122: loss =   1.694 , accuracy =   0.388, test_acc =   0.377 time:   7.469\n",
      "Epoch: 123: loss =   1.699 , accuracy =   0.381, test_acc =   0.377 time:   7.541\n",
      "Epoch: 124: loss =   1.737 , accuracy =   0.383, test_acc =   0.372 time:   7.562\n",
      "Epoch: 125: loss =   1.670 , accuracy =   0.384, test_acc =   0.376 time:   7.598\n",
      "Epoch: 126: loss =   1.701 , accuracy =   0.389, test_acc =   0.381 time:   7.575\n",
      "Epoch: 127: loss =   1.707 , accuracy =   0.396, test_acc =   0.389 time:   7.595\n",
      "Epoch: 128: loss =   1.701 , accuracy =   0.384, test_acc =   0.392 time:   7.551\n",
      "Epoch: 129: loss =   1.699 , accuracy =   0.381, test_acc =   0.382 time:   7.635\n",
      "Epoch: 130: loss =   1.682 , accuracy =   0.379, test_acc =   0.388 time:   7.642\n",
      "Epoch: 131: loss =   1.687 , accuracy =   0.390, test_acc =   0.389 time:   7.704\n",
      "Epoch: 132: loss =   1.682 , accuracy =   0.383, test_acc =   0.398 time:   7.591\n",
      "Epoch: 133: loss =   1.693 , accuracy =   0.392, test_acc =   0.393 time:   7.587\n",
      "Epoch: 134: loss =   1.686 , accuracy =   0.399, test_acc =   0.380 time:   7.652\n",
      "Epoch: 135: loss =   1.666 , accuracy =   0.387, test_acc =   0.380 time:   7.599\n",
      "Epoch: 136: loss =   1.657 , accuracy =   0.391, test_acc =   0.368 time:   7.580\n",
      "Epoch: 137: loss =   1.660 , accuracy =   0.400, test_acc =   0.388 time:   7.544\n",
      "Epoch: 138: loss =   1.637 , accuracy =   0.412, test_acc =   0.401 time:   7.600\n",
      "Epoch: 139: loss =   1.678 , accuracy =   0.403, test_acc =   0.402 time:   7.664\n",
      "Epoch: 140: loss =   1.636 , accuracy =   0.409, test_acc =   0.402 time:   7.546\n",
      "Epoch: 141: loss =   1.648 , accuracy =   0.412, test_acc =   0.409 time:   7.630\n",
      "Epoch: 142: loss =   1.664 , accuracy =   0.405, test_acc =   0.380 time:   7.596\n",
      "Epoch: 143: loss =   1.630 , accuracy =   0.409, test_acc =   0.406 time:   7.716\n",
      "Epoch: 144: loss =   1.605 , accuracy =   0.408, test_acc =   0.408 time:   7.647\n",
      "Epoch: 145: loss =   1.632 , accuracy =   0.408, test_acc =   0.407 time:   7.578\n",
      "Epoch: 146: loss =   1.601 , accuracy =   0.413, test_acc =   0.394 time:   7.600\n",
      "Epoch: 147: loss =   1.590 , accuracy =   0.416, test_acc =   0.397 time:   7.506\n",
      "Epoch: 148: loss =   1.608 , accuracy =   0.415, test_acc =   0.411 time:   7.634\n",
      "Epoch: 149: loss =   1.612 , accuracy =   0.416, test_acc =   0.412 time:   7.574\n",
      "Epoch: 150: loss =   1.586 , accuracy =   0.420, test_acc =   0.414 time:   7.595\n",
      "Epoch: 151: loss =   1.616 , accuracy =   0.414, test_acc =   0.407 time:   7.683\n",
      "Epoch: 152: loss =   1.592 , accuracy =   0.417, test_acc =   0.414 time:   7.597\n",
      "Epoch: 153: loss =   1.581 , accuracy =   0.420, test_acc =   0.416 time:   7.579\n",
      "Epoch: 154: loss =   1.570 , accuracy =   0.431, test_acc =   0.413 time:   7.543\n",
      "Epoch: 155: loss =   1.619 , accuracy =   0.427, test_acc =   0.426 time:   7.555\n",
      "Epoch: 156: loss =   1.576 , accuracy =   0.428, test_acc =   0.419 time:   7.660\n",
      "Epoch: 157: loss =   1.569 , accuracy =   0.419, test_acc =   0.400 time:   7.540\n",
      "Epoch: 158: loss =   1.597 , accuracy =   0.433, test_acc =   0.421 time:   7.620\n",
      "Epoch: 159: loss =   1.610 , accuracy =   0.437, test_acc =   0.415 time:   7.600\n",
      "Epoch: 160: loss =   1.586 , accuracy =   0.431, test_acc =   0.409 time:   7.586\n",
      "Epoch: 161: loss =   1.598 , accuracy =   0.438, test_acc =   0.431 time:   7.604\n",
      "Epoch: 162: loss =   1.558 , accuracy =   0.435, test_acc =   0.425 time:   7.581\n",
      "Epoch: 163: loss =   1.564 , accuracy =   0.439, test_acc =   0.426 time:   7.884\n",
      "Epoch: 164: loss =   1.574 , accuracy =   0.438, test_acc =   0.423 time:   7.672\n",
      "Epoch: 165: loss =   1.554 , accuracy =   0.443, test_acc =   0.428 time:   7.651\n",
      "Epoch: 166: loss =   1.596 , accuracy =   0.433, test_acc =   0.432 time:   7.630\n",
      "Epoch: 167: loss =   1.541 , accuracy =   0.444, test_acc =   0.422 time:   7.734\n",
      "Epoch: 168: loss =   1.567 , accuracy =   0.433, test_acc =   0.427 time:   7.544\n",
      "Epoch: 169: loss =   1.562 , accuracy =   0.437, test_acc =   0.435 time:   7.626\n",
      "Epoch: 170: loss =   1.542 , accuracy =   0.447, test_acc =   0.441 time:   7.593\n",
      "Epoch: 171: loss =   1.537 , accuracy =   0.435, test_acc =   0.431 time:   7.565\n",
      "Epoch: 172: loss =   1.546 , accuracy =   0.448, test_acc =   0.430 time:   7.605\n",
      "Epoch: 173: loss =   1.538 , accuracy =   0.449, test_acc =   0.439 time:   7.680\n",
      "Epoch: 174: loss =   1.546 , accuracy =   0.445, test_acc =   0.445 time:   7.614\n",
      "Epoch: 175: loss =   1.528 , accuracy =   0.453, test_acc =   0.441 time:   7.662\n",
      "Epoch: 176: loss =   1.506 , accuracy =   0.443, test_acc =   0.436 time:   7.551\n",
      "Epoch: 177: loss =   1.514 , accuracy =   0.447, test_acc =   0.430 time:   7.639\n",
      "Epoch: 178: loss =   1.522 , accuracy =   0.455, test_acc =   0.438 time:   7.631\n",
      "Epoch: 179: loss =   1.518 , accuracy =   0.457, test_acc =   0.444 time:   7.619\n",
      "Epoch: 180: loss =   1.529 , accuracy =   0.459, test_acc =   0.449 time:   7.565\n",
      "Epoch: 181: loss =   1.514 , accuracy =   0.454, test_acc =   0.446 time:   7.553\n",
      "Epoch: 182: loss =   1.514 , accuracy =   0.462, test_acc =   0.442 time:   7.681\n",
      "Epoch: 183: loss =   1.504 , accuracy =   0.462, test_acc =   0.451 time:   7.600\n",
      "Epoch: 184: loss =   1.498 , accuracy =   0.458, test_acc =   0.447 time:   7.575\n",
      "Epoch: 185: loss =   1.520 , accuracy =   0.461, test_acc =   0.448 time:   7.573\n",
      "Epoch: 186: loss =   1.490 , accuracy =   0.462, test_acc =   0.450 time:   7.665\n",
      "Epoch: 187: loss =   1.498 , accuracy =   0.462, test_acc =   0.443 time:   7.636\n",
      "Epoch: 188: loss =   1.494 , accuracy =   0.450, test_acc =   0.453 time:   7.635\n",
      "Epoch: 189: loss =   1.479 , accuracy =   0.459, test_acc =   0.454 time:   7.632\n",
      "Epoch: 190: loss =   1.472 , accuracy =   0.461, test_acc =   0.446 time:   7.556\n",
      "Epoch: 191: loss =   1.467 , accuracy =   0.468, test_acc =   0.453 time:   7.639\n",
      "Epoch: 192: loss =   1.466 , accuracy =   0.464, test_acc =   0.446 time:   7.599\n",
      "Epoch: 193: loss =   1.482 , accuracy =   0.467, test_acc =   0.453 time:   7.584\n",
      "Epoch: 194: loss =   1.476 , accuracy =   0.455, test_acc =   0.451 time:   7.620\n",
      "Epoch: 195: loss =   1.459 , accuracy =   0.463, test_acc =   0.455 time:   7.587\n",
      "Epoch: 196: loss =   1.480 , accuracy =   0.440, test_acc =   0.449 time:   7.585\n",
      "Epoch: 197: loss =   1.466 , accuracy =   0.470, test_acc =   0.459 time:   7.574\n",
      "Epoch: 198: loss =   1.472 , accuracy =   0.453, test_acc =   0.440 time:   7.613\n",
      "Epoch: 199: loss =   1.521 , accuracy =   0.466, test_acc =   0.457 time:   7.578\n",
      "Epoch: 200: loss =   1.505 , accuracy =   0.463, test_acc =   0.450 time:   7.542\n",
      "Epoch: 201: loss =   1.531 , accuracy =   0.454, test_acc =   0.446 time:   7.654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 202: loss =   1.510 , accuracy =   0.460, test_acc =   0.446 time:   7.596\n",
      "Epoch: 203: loss =   1.505 , accuracy =   0.465, test_acc =   0.444 time:   7.721\n",
      "Epoch: 204: loss =   1.481 , accuracy =   0.472, test_acc =   0.451 time:   7.589\n",
      "Epoch: 205: loss =   1.491 , accuracy =   0.466, test_acc =   0.456 time:   7.672\n",
      "Epoch: 206: loss =   1.460 , accuracy =   0.474, test_acc =   0.444 time:   7.695\n",
      "Epoch: 207: loss =   1.467 , accuracy =   0.466, test_acc =   0.457 time:   7.679\n",
      "Epoch: 208: loss =   1.485 , accuracy =   0.464, test_acc =   0.451 time:   7.578\n",
      "Epoch: 209: loss =   1.467 , accuracy =   0.453, test_acc =   0.463 time:   7.668\n",
      "Epoch: 210: loss =   1.472 , accuracy =   0.475, test_acc =   0.459 time:   7.649\n",
      "Epoch: 211: loss =   1.507 , accuracy =   0.479, test_acc =   0.462 time:   7.639\n",
      "Epoch: 212: loss =   1.473 , accuracy =   0.475, test_acc =   0.469 time:   7.639\n",
      "Epoch: 213: loss =   1.437 , accuracy =   0.473, test_acc =   0.464 time:   7.563\n",
      "Epoch: 214: loss =   1.435 , accuracy =   0.474, test_acc =   0.458 time:   7.684\n",
      "Epoch: 215: loss =   1.463 , accuracy =   0.479, test_acc =   0.463 time:   7.666\n",
      "Epoch: 216: loss =   1.436 , accuracy =   0.476, test_acc =   0.466 time:   7.559\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "in our train step we can see that it lasts more tha na normal CNN to converge\n",
    "on the other side, we can have the confidence interval for our predictions, which are \n",
    "wonderful in terms of taking sensitive predictions\n",
    "\"\"\"\n",
    "times = []\n",
    "for i in range(700):\n",
    "    tic = time.time()\n",
    "    loss = train_step(X_train, y_train)\n",
    "    preds = bcnn(X_train)\n",
    "    acc = accuracy(preds, y_train)\n",
    "    preds_test = bcnn(X_test)\n",
    "    test_acc = accuracy(preds_test, y_test)\n",
    "    tac = time.time()\n",
    "    train_time = tac-tic\n",
    "    times.append(train_time)\n",
    "    print(\"Epoch: {}: loss = {:7.3f} , accuracy = {:7.3f}, test_acc = {:7.3f} time: {:7.3f}\".format(i, loss, acc, test_acc, train_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bcnn.save_weights(\"bcnn_cifar10.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### About the performance:\n",
    "\n",
    "mean = np.mean(times)\n",
    "std = np.std(times)\n",
    "print(\"In TensorFlow 2.0.0b1 our train time mean was : {:7.3f}, with std : {:7.3f}\".format(mean, std))\n",
    "\n",
    "no_outlier = times[1:]\n",
    "no_mean = np.mean(no_outlier)\n",
    "no_std = np.std(no_outlier)\n",
    "print(\"\\nHowever, by removing the outlier 1st time, our train time mean was : {:7.3f}, with std : {:7.3f}\".format(no_mean, no_std))\n",
    "#print(\"\\nWe conclude TensorFlow 2 has a longer time to start its variables, but then does it faster than TF1.14 Intel Optimzied (see other notebook)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we will illustrate our predictions and confidence intervals\n",
    "\n",
    "Those illustrative functions were taken from https://github.com/zhulingchen/tfp-tutorial/ repo, which had the tutorial (in Keras) that did let me learn how to \n",
    "\n",
    "### Here we have some statistics on recognizable and unrecognizable images from MNIST:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_mc_run = 100\n",
    "med_prob_thres = 0.20\n",
    "\n",
    "y_pred_logits_list = [bcnn(X_test) for _ in range(n_mc_run)]  # a list of predicted logits\n",
    "y_pred_prob_all = np.concatenate([softmax(y, axis=-1)[:, :, np.newaxis] for y in y_pred_logits_list], axis=-1)\n",
    "y_pred = [[int(np.median(y) >= med_prob_thres) for y in y_pred_prob] for y_pred_prob in y_pred_prob_all]\n",
    "y_pred = np.array(y_pred)\n",
    "\n",
    "idx_valid = [any(y) for y in y_pred]\n",
    "print('Number of recognizable samples:', sum(idx_valid))\n",
    "\n",
    "idx_invalid = [not any(y) for y in y_pred]\n",
    "print('Unrecognizable samples:', np.where(idx_invalid)[0])\n",
    "\n",
    "print('Test accuracy on MNIST (recognizable samples):',\n",
    "      sum(np.equal(np.argmax(y_test[idx_valid], axis=-1), np.argmax(y_pred[idx_valid], axis=-1))) / len(y_test[idx_valid]))\n",
    "\n",
    "print('Test accuracy on MNIST (unrecognizable samples):',\n",
    "      sum(np.equal(np.argmax(y_test[idx_invalid], axis=-1), np.argmax(y_pred[idx_invalid], axis=-1))) / len(y_test[idx_invalid]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With this other snippet, we can plot the predict distribution of our network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pred_hist(y_pred, n_class, n_mc_run, n_bins=30, med_prob_thres=0.2, n_subplot_rows=2, figsize=(25, 10)):\n",
    "    bins = np.logspace(-n_bins, 0, n_bins+1)\n",
    "    fig, ax = plt.subplots(n_subplot_rows, n_class // n_subplot_rows + 1, figsize=figsize)\n",
    "    for i in range(n_subplot_rows):\n",
    "        for j in range(n_class // n_subplot_rows + 1):\n",
    "            idx = i * (n_class // n_subplot_rows + 1) + j\n",
    "            if idx < n_class:\n",
    "                ax[i, j].hist(y_pred[idx], bins)\n",
    "                ax[i, j].set_xscale('log')\n",
    "                ax[i, j].set_ylim([0, n_mc_run])\n",
    "                ax[i, j].title.set_text(\"{} (median prob: {:.2f}) ({})\".format(str(idx),\n",
    "                                                                               np.median(y_pred[idx]),\n",
    "                                                                               str(np.median(y_pred[idx]) >= med_prob_thres)))\n",
    "            else:\n",
    "                ax[i, j].axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This example is not recognizable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in np.where(idx_invalid)[0]:\n",
    "    plt.imshow(X_test[idx, :, :, :], cmap='gist_gray')\n",
    "    print(\"True label of the test sample {}: {}\".format(idx, np.argmax(y_test[idx], axis=-1)))\n",
    "\n",
    "    plot_pred_hist(y_pred_prob_all[idx], class_nmr, n_mc_run, med_prob_thres=med_prob_thres)\n",
    "\n",
    "    if any(y_pred[idx]):\n",
    "        print(\"Predicted label of the test sample {}: {}\".format(idx, np.argmax(y_pred[idx], axis=-1)))\n",
    "    else:\n",
    "        print(\"I don't know!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A recognizable one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X_test[0, :, :, :], cmap='gist_gray')\n",
    "print(\"True label of the test sample {}: {}\".format(0, np.argmax(y_test[0], axis=-1)))\n",
    "\n",
    "plot_pred_hist(y_pred_prob_all[0], class_nmr, n_mc_run, med_prob_thres=med_prob_thres)\n",
    "\n",
    "if any(y_pred[0]):\n",
    "    print(\"Predicted label of the test sample {}: {}\".format(idx, np.argmax(y_pred[0], axis=-1)))\n",
    "else:\n",
    "    print(\"I don't know!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_prob",
   "language": "python",
   "name": "tf_prob"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
