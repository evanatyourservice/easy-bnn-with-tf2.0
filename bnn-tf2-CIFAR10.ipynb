{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Neural Networks in Keras and TensorFlow 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! . activate base\n",
    "#!pip install tensorflow==2.0.0b1 --user\n",
    "#!pip install tfp_nightly --user\n",
    "#!python -m ipykernel install --name tf_prob --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/u30073/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/u30073/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/u30073/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/u30073/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/u30073/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/u30073/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/glob/intel-python/python3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/home/u30073/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/u30073/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/u30073/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/u30073/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/u30073/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/u30073/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "#TODO: PUT RANDOM SEED FOR RESULT SECURING IN DEMOS\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import cv2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version (expected = 2.0.0-beta1): 2.0.0-beta1\n",
      "TensorFlow Probability version (expected = 0.9.0-dev20190912): 0.9.0-dev20190913\n"
     ]
    }
   ],
   "source": [
    "print('TensorFlow version (expected = 2.0.0-beta1):', tf.__version__)\n",
    "print('TensorFlow Probability version (expected = 0.9.0-dev20190912):', tfp.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X, theta = 1.0, axis = None):\n",
    "    \"\"\"\n",
    "    Compute the softmax of each element along an axis of X.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X: ND-Array. Probably should be floats.\n",
    "    theta (optional): float parameter, used as a multiplier\n",
    "        prior to exponentiation. Default = 1.0\n",
    "    axis (optional): axis to compute values along. Default is the\n",
    "        first non-singleton axis.\n",
    "\n",
    "    Returns an array the same size as X. The result will sum to 1\n",
    "    along the specified axis.\n",
    "    \"\"\"\n",
    "\n",
    "    # make X at least 2d\n",
    "    y = np.atleast_2d(X)\n",
    "\n",
    "    # find axis\n",
    "    if axis is None:\n",
    "        axis = next(j[0] for j in enumerate(y.shape) if j[1] > 1)\n",
    "\n",
    "    # multiply y against the theta parameter,\n",
    "    y = y * float(theta)\n",
    "\n",
    "    # subtract the max for numerical stability\n",
    "    y = y - np.expand_dims(np.max(y, axis = axis), axis)\n",
    "\n",
    "    # exponentiate y\n",
    "    y = np.exp(y)\n",
    "\n",
    "    # take the sum along the specified axis\n",
    "    ax_sum = np.expand_dims(np.sum(y, axis = axis), axis)\n",
    "\n",
    "    # finally: divide elementwise\n",
    "    p = y / ax_sum\n",
    "\n",
    "    # flatten if X was 1D\n",
    "    if len(X.shape) == 1: p = p.flatten()\n",
    "\n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We are all set up, lets go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are using CIFAR10 dataset!\n",
      "X_train.shape = (50000, 32, 32, 3)\n",
      "y_train.shape = (50000, 10)\n",
      "X_test.shape = (10000, 32, 32, 3)\n",
      "y_test.shape = (10000, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f9f2a289518>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ncn4hEoqcX4iEIucXIqH8P1mpzvtETxLWAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9f317e44a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "first, we load the dataset. We are trying to do it first with CIFAR\n",
    "I've found this snippet somewhere in github\n",
    "\"\"\"\n",
    "\n",
    "#as we have 10 classes, I'm setting class number to 10\n",
    "class_nmr = 10\n",
    "\n",
    "print('We are using CIFAR10 dataset!')\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "#X_train = np.expand_dims(X_train, -1)\n",
    "n_train = X_train.shape[0]\n",
    "#X_test = np.expand_dims(X_test, -1)\n",
    "y_train = tf.keras.utils.to_categorical(y_train, class_nmr)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, class_nmr)\n",
    "\n",
    "# Normalize data\n",
    "X_train = X_train.astype('float32') / 255\n",
    "X_test = X_test.astype('float32') / 255\n",
    "\n",
    "print(\"X_train.shape =\", X_train.shape)\n",
    "print(\"y_train.shape =\", y_train.shape)\n",
    "print(\"X_test.shape =\", X_test.shape)\n",
    "print(\"y_test.shape =\", y_test.shape)\n",
    "\n",
    "plt.imshow(X_train[1026, :, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bayesian_bcnn_model(input_shape):\n",
    "    \n",
    "    \"\"\"\n",
    "    Here we use tf.keras.Model to use our graph as a Neural Network:\n",
    "    We select our input node as the net input, and the last node as our output (predict node).\n",
    "    Note that our model won't be compiled, as we are usign TF2.0 and will optimize it with\n",
    "    a custom @tf.function for loss and a @tf.function for train_step\n",
    "    Our input parameter is just the input shape, a tuple, for the input layer\n",
    "    \"\"\"\n",
    "    \n",
    "    model_in = tf.keras.layers.Input(shape=input_shape)\n",
    "    conv_1 = tfp.python.layers.Convolution2DFlipout(32, kernel_size=(3, 3), padding=\"same\", strides=2)\n",
    "    x = conv_1(model_in)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "    x = tf.keras.layers.MaxPool2D(pool_size=2, padding='SAME', data_format='channels_last')(x)\n",
    "    \n",
    "    conv_2 = tfp.python.layers.Convolution2DFlipout(64, kernel_size=(3, 3), padding=\"same\", strides=2)\n",
    "    x = conv_2(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "    x = tf.keras.layers.MaxPool2D(pool_size=2, padding='SAME', data_format='channels_last')(x)\n",
    "\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    x = tf.keras.layers.Dropout(rate=0.25)(x)\n",
    "    dense_1 = tfp.python.layers.DenseFlipout(512, activation='relu')\n",
    "    x = dense_1(x)\n",
    "    x = tf.keras.layers.Dropout(rate=0.25)(x)\n",
    "    dense_2 = tfp.python.layers.DenseFlipout(10, activation=None)\n",
    "    model_out = dense_2(x)  # logits\n",
    "    model = tf.keras.Model(model_in, model_out)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "here we are just instancing our model and setting up an Optimizer\n",
    "\"\"\"\n",
    "bcnn = build_bayesian_bcnn_model(X_train.shape[1:])\n",
    "optimizer = tf.keras.optimizers.Adam(lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 32, 32, 3)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d_flipout_2 (Conv2DFlip (None, 16, 16, 32)        1760      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 16, 16, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_flipout_3 (Conv2DFlip (None, 4, 4, 64)          36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 4, 4, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 2, 2, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_flipout_2 (DenseFlipou (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_flipout_3 (DenseFlipou (None, 10)                10250     \n",
      "=================================================================\n",
      "Total params: 311,978\n",
      "Trainable params: 311,786\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "bcnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "this is our loss function: a sum of KL Divergence and Softmax crossentropy\n",
    "We use the @tf.function annotation becuase of TF2.0, and need no placeholders\n",
    "we get each loss and return its mean\n",
    "\"\"\"\n",
    "\n",
    "@tf.function\n",
    "def elbo_loss(labels, logits):\n",
    "    loss_en = tf.nn.softmax_cross_entropy_with_logits(labels, logits)\n",
    "    loss_kl = tf.keras.losses.KLD(labels, logits)\n",
    "    loss = tf.reduce_mean(tf.add(loss_en, loss_kl))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "this is our train step with tf2.0, very ellegant:\n",
    "We do our flow of the tensors over the model recording its gradientes\n",
    "Then, our gradient tape to give us a list of the gradients of each parameter in relation of the loss\n",
    "we dan ask our previously instanced optimizer to apply those gradients to the variable\n",
    "It is cool to see that it works even with TensorFlow probability- probabilistic layers parameters\n",
    "\"\"\"\n",
    "@tf.function\n",
    "def train_step(images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = bcnn(X_train)\n",
    "        loss = elbo_loss(labels, logits)\n",
    "    gradients = tape.gradient(loss, bcnn.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, bcnn.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "def accuracy(preds, labels):\n",
    "    return np.mean(np.argmax(preds, axis=1) == np.argmax(labels, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0918 06:36:21.175921 140324075835136 deprecation.py:323] From /home/u30073/.local/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "in our train step we can see that it lasts more tha na normal CNN to converge\n",
    "on the other side, we can have the confidence interval for our predictions, which are \n",
    "wonderful in terms of taking sensitive predictions\n",
    "\"\"\"\n",
    "times = []\n",
    "for i in range(568, 700):\n",
    "    tic = time.time()\n",
    "    loss = train_step(X_train, y_train)\n",
    "    preds = bcnn(X_train)\n",
    "    acc = accuracy(preds, y_train)\n",
    "    preds_test = bcnn(X_test)\n",
    "    test_acc = accuracy(preds_test, y_test)\n",
    "    tac = time.time()\n",
    "    train_time = tac-tic\n",
    "    times.append(train_time)\n",
    "    print(\"Epoch: {}: loss = {:7.3f} , accuracy = {:7.3f}, test_acc = {:7.3f} time: {:7.3f}\".format(i, loss, acc, test_acc, train_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bcnn.save_weights(\"bcnn_cifar10.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### About the performance:\n",
    "\n",
    "mean = np.mean(times)\n",
    "std = np.std(times)\n",
    "print(\"In TensorFlow 2.0.0b1 our train time mean was : {:7.3f}, with std : {:7.3f}\".format(mean, std))\n",
    "\n",
    "no_outlier = times[1:]\n",
    "no_mean = np.mean(no_outlier)\n",
    "no_std = np.std(no_outlier)\n",
    "print(\"\\nHowever, by removing the outlier 1st time, our train time mean was : {:7.3f}, with std : {:7.3f}\".format(no_mean, no_std))\n",
    "#print(\"\\nWe conclude TensorFlow 2 has a longer time to start its variables, but then does it faster than TF1.14 Intel Optimzied (see other notebook)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we will illustrate our predictions and confidence intervals\n",
    "\n",
    "Those illustrative functions were taken from https://github.com/zhulingchen/tfp-tutorial/ repo, which had the tutorial (in Keras) that did let me learn how to \n",
    "\n",
    "### Here we have some statistics on recognizable and unrecognizable images from MNIST:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_mc_run = 100\n",
    "med_prob_thres = 0.20\n",
    "\n",
    "y_pred_logits_list = [bcnn(X_test) for _ in range(n_mc_run)]  # a list of predicted logits\n",
    "y_pred_prob_all = np.concatenate([softmax(y, axis=-1)[:, :, np.newaxis] for y in y_pred_logits_list], axis=-1)\n",
    "y_pred = [[int(np.median(y) >= med_prob_thres) for y in y_pred_prob] for y_pred_prob in y_pred_prob_all]\n",
    "y_pred = np.array(y_pred)\n",
    "\n",
    "idx_valid = [any(y) for y in y_pred]\n",
    "print('Number of recognizable samples:', sum(idx_valid))\n",
    "\n",
    "idx_invalid = [not any(y) for y in y_pred]\n",
    "print('Unrecognizable samples:', np.where(idx_invalid)[0])\n",
    "\n",
    "print('Test accuracy on MNIST (recognizable samples):',\n",
    "      sum(np.equal(np.argmax(y_test[idx_valid], axis=-1), np.argmax(y_pred[idx_valid], axis=-1))) / len(y_test[idx_valid]))\n",
    "\n",
    "print('Test accuracy on MNIST (unrecognizable samples):',\n",
    "      sum(np.equal(np.argmax(y_test[idx_invalid], axis=-1), np.argmax(y_pred[idx_invalid], axis=-1))) / len(y_test[idx_invalid]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With this other snippet, we can plot the predict distribution of our network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pred_hist(y_pred, n_class, n_mc_run, n_bins=30, med_prob_thres=0.2, n_subplot_rows=2, figsize=(25, 10)):\n",
    "    bins = np.logspace(-n_bins, 0, n_bins+1)\n",
    "    fig, ax = plt.subplots(n_subplot_rows, n_class // n_subplot_rows + 1, figsize=figsize)\n",
    "    for i in range(n_subplot_rows):\n",
    "        for j in range(n_class // n_subplot_rows + 1):\n",
    "            idx = i * (n_class // n_subplot_rows + 1) + j\n",
    "            if idx < n_class:\n",
    "                ax[i, j].hist(y_pred[idx], bins)\n",
    "                ax[i, j].set_xscale('log')\n",
    "                ax[i, j].set_ylim([0, n_mc_run])\n",
    "                ax[i, j].title.set_text(\"{} (median prob: {:.2f}) ({})\".format(str(idx),\n",
    "                                                                               np.median(y_pred[idx]),\n",
    "                                                                               str(np.median(y_pred[idx]) >= med_prob_thres)))\n",
    "            else:\n",
    "                ax[i, j].axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This example is not recognizable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in np.where(idx_invalid)[0]:\n",
    "    plt.imshow(X_test[idx, :, :, :], cmap='gist_gray')\n",
    "    print(\"True label of the test sample {}: {}\".format(idx, np.argmax(y_test[idx], axis=-1)))\n",
    "\n",
    "    plot_pred_hist(y_pred_prob_all[idx], class_nmr, n_mc_run, med_prob_thres=med_prob_thres)\n",
    "\n",
    "    if any(y_pred[idx]):\n",
    "        print(\"Predicted label of the test sample {}: {}\".format(idx, np.argmax(y_pred[idx], axis=-1)))\n",
    "    else:\n",
    "        print(\"I don't know!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A recognizable one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X_test[0, :, :, :], cmap='gist_gray')\n",
    "print(\"True label of the test sample {}: {}\".format(0, np.argmax(y_test[0], axis=-1)))\n",
    "\n",
    "plot_pred_hist(y_pred_prob_all[0], class_nmr, n_mc_run, med_prob_thres=med_prob_thres)\n",
    "\n",
    "if any(y_pred[0]):\n",
    "    print(\"Predicted label of the test sample {}: {}\".format(idx, np.argmax(y_pred[0], axis=-1)))\n",
    "else:\n",
    "    print(\"I don't know!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_prob",
   "language": "python",
   "name": "tf_prob"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
