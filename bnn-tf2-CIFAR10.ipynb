{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Neural Networks in Keras and TensorFlow 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! . activate base\n",
    "#!pip install tensorflow==2.0.0b1 --user\n",
    "#!pip install tfp_nightly --user\n",
    "#!python -m ipykernel install --name tf_prob --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: PUT RANDOM SEED FOR RESULT SECURING IN DEMOS\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import cv2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version (expected = 2.0.0-beta1): 2.0.0-beta1\n",
      "TensorFlow Probability version (expected = 0.9.0-dev20190912): 0.9.0-dev20190913\n"
     ]
    }
   ],
   "source": [
    "print('TensorFlow version (expected = 2.0.0-beta1):', tf.__version__)\n",
    "print('TensorFlow Probability version (expected = 0.9.0-dev20190912):', tfp.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X, theta = 1.0, axis = None):\n",
    "    \"\"\"\n",
    "    Compute the softmax of each element along an axis of X.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X: ND-Array. Probably should be floats.\n",
    "    theta (optional): float parameter, used as a multiplier\n",
    "        prior to exponentiation. Default = 1.0\n",
    "    axis (optional): axis to compute values along. Default is the\n",
    "        first non-singleton axis.\n",
    "\n",
    "    Returns an array the same size as X. The result will sum to 1\n",
    "    along the specified axis.\n",
    "    \"\"\"\n",
    "\n",
    "    # make X at least 2d\n",
    "    y = np.atleast_2d(X)\n",
    "\n",
    "    # find axis\n",
    "    if axis is None:\n",
    "        axis = next(j[0] for j in enumerate(y.shape) if j[1] > 1)\n",
    "\n",
    "    # multiply y against the theta parameter,\n",
    "    y = y * float(theta)\n",
    "\n",
    "    # subtract the max for numerical stability\n",
    "    y = y - np.expand_dims(np.max(y, axis = axis), axis)\n",
    "\n",
    "    # exponentiate y\n",
    "    y = np.exp(y)\n",
    "\n",
    "    # take the sum along the specified axis\n",
    "    ax_sum = np.expand_dims(np.sum(y, axis = axis), axis)\n",
    "\n",
    "    # finally: divide elementwise\n",
    "    p = y / ax_sum\n",
    "\n",
    "    # flatten if X was 1D\n",
    "    if len(X.shape) == 1: p = p.flatten()\n",
    "\n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We are all set up, lets go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are using CIFAR10 dataset!\n",
      "X_train.shape = (50000, 32, 32, 3)\n",
      "y_train.shape = (50000, 10)\n",
      "X_test.shape = (10000, 32, 32, 3)\n",
      "y_test.shape = (10000, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f9d8e476f98>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ncn4hEoqcX4iEIucXIqH8P1mpzvtETxLWAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9e0822a4e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "first, we load the dataset. We are trying to do it first with CIFAR\n",
    "I've found this snippet somewhere in github\n",
    "\"\"\"\n",
    "\n",
    "#as we have 10 classes, I'm setting class number to 10\n",
    "class_nmr = 10\n",
    "\n",
    "print('We are using CIFAR10 dataset!')\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "#X_train = np.expand_dims(X_train, -1)\n",
    "n_train = X_train.shape[0]\n",
    "#X_test = np.expand_dims(X_test, -1)\n",
    "y_train = tf.keras.utils.to_categorical(y_train, class_nmr)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, class_nmr)\n",
    "\n",
    "# Normalize data\n",
    "X_train = X_train.astype('float32') / 255\n",
    "X_test = X_test.astype('float32') / 255\n",
    "\n",
    "print(\"X_train.shape =\", X_train.shape)\n",
    "print(\"y_train.shape =\", y_train.shape)\n",
    "print(\"X_test.shape =\", X_test.shape)\n",
    "print(\"y_test.shape =\", y_test.shape)\n",
    "\n",
    "plt.imshow(X_train[1026, :, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bayesian_bcnn_model(input_shape):\n",
    "    \n",
    "    \"\"\"\n",
    "    Here we use tf.keras.Model to use our graph as a Neural Network:\n",
    "    We select our input node as the net input, and the last node as our output (predict node).\n",
    "    Note that our model won't be compiled, as we are usign TF2.0 and will optimize it with\n",
    "    a custom @tf.function for loss and a @tf.function for train_step\n",
    "    Our input parameter is just the input shape, a tuple, for the input layer\n",
    "    \"\"\"\n",
    "    \n",
    "    model_in = tf.keras.layers.Input(shape=input_shape)\n",
    "    conv_1 = tfp.python.layers.Convolution2DFlipout(32, kernel_size=(3, 3), padding=\"same\", strides=2)\n",
    "    x = conv_1(model_in)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "    x = tf.keras.layers.MaxPool2D(pool_size=3, padding='SAME', data_format='channels_last')(x)\n",
    "    \n",
    "    conv_2 = tfp.python.layers.Convolution2DFlipout(64, kernel_size=(3, 3), padding=\"same\", strides=2)\n",
    "    x = conv_2(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "    x = tf.keras.layers.MaxPool2D(pool_size=3, padding='SAME', data_format='channels_last')(x)\n",
    "\n",
    "    \n",
    "    conv_3 = tfp.python.layers.Convolution2DFlipout(64, kernel_size=(3, 3), padding=\"same\", strides=2)\n",
    "    x = conv_3(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "    x = tf.keras.layers.MaxPool2D(pool_size=3, padding='SAME', data_format='channels_last')(x)\n",
    "    \n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    x = tf.keras.layers.Dropout(rate=0.25)(x)\n",
    "    dense_1 = tfp.python.layers.DenseFlipout(256, activation='relu')\n",
    "    x = dense_1(x)\n",
    "    \n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    x = tf.keras.layers.Dropout(rate=0.25)(x)\n",
    "    dense_2 = tfp.python.layers.DenseFlipout(256, activation='relu')\n",
    "    x = dense_2(x)\n",
    "    \n",
    "    x = tf.keras.layers.Dropout(rate=0.25)(x)\n",
    "    dense_3 = tfp.python.layers.DenseFlipout(10, activation=None)\n",
    "    model_out = dense_3(x)  # logits\n",
    "    model = tf.keras.Model(model_in, model_out)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "here we are just instancing our model and setting up an Optimizer\n",
    "\"\"\"\n",
    "bcnn = build_bayesian_bcnn_model(X_train.shape[1:])\n",
    "optimizer = tf.keras.optimizers.Adam(lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 32, 32, 3)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d_flipout_7 (Conv2DFlip (None, 16, 16, 32)        1760      \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 16, 16, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 6, 6, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_flipout_8 (Conv2DFlip (None, 3, 3, 64)          36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 3, 3, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 3, 3, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 1, 1, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_flipout_9 (Conv2DFlip (None, 1, 1, 64)          73792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 1, 1, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 1, 1, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 1, 1, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_flipout_7 (DenseFlipou (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_flipout_8 (DenseFlipou (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_flipout_9 (DenseFlipou (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 282,602\n",
      "Trainable params: 282,282\n",
      "Non-trainable params: 320\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "bcnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "this is our loss function: a sum of KL Divergence and Softmax crossentropy\n",
    "We use the @tf.function annotation becuase of TF2.0, and need no placeholders\n",
    "we get each loss and return its mean\n",
    "\"\"\"\n",
    "\n",
    "@tf.function\n",
    "def elbo_loss(labels, logits):\n",
    "    loss_en = tf.nn.softmax_cross_entropy_with_logits(labels, logits)\n",
    "    loss_kl = tf.keras.losses.KLD(labels, logits)\n",
    "    loss = tf.reduce_mean(tf.add(loss_en, loss_kl))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "this is our train step with tf2.0, very ellegant:\n",
    "We do our flow of the tensors over the model recording its gradientes\n",
    "Then, our gradient tape to give us a list of the gradients of each parameter in relation of the loss\n",
    "we dan ask our previously instanced optimizer to apply those gradients to the variable\n",
    "It is cool to see that it works even with TensorFlow probability- probabilistic layers parameters\n",
    "\"\"\"\n",
    "@tf.function\n",
    "def train_step(images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = bcnn(X_train)\n",
    "        loss = elbo_loss(labels, logits)\n",
    "    gradients = tape.gradient(loss, bcnn.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, bcnn.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "def accuracy(preds, labels):\n",
    "    return np.mean(np.argmax(preds, axis=1) == np.argmax(labels, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0: loss =   9.104 , accuracy =   0.100, test_acc =   0.101 time:  44.111\n",
      "Epoch: 1: loss =  13.590 , accuracy =   0.102, test_acc =   0.101 time:   8.370\n",
      "Epoch: 2: loss =  10.281 , accuracy =   0.103, test_acc =   0.097 time:   8.344\n",
      "Epoch: 3: loss =   7.812 , accuracy =   0.099, test_acc =   0.094 time:   8.363\n",
      "Epoch: 4: loss =   5.133 , accuracy =   0.100, test_acc =   0.097 time:   8.421\n",
      "Epoch: 5: loss =   3.610 , accuracy =   0.102, test_acc =   0.099 time:   8.312\n",
      "Epoch: 6: loss =   3.008 , accuracy =   0.099, test_acc =   0.102 time:   8.321\n",
      "Epoch: 7: loss =   2.826 , accuracy =   0.101, test_acc =   0.096 time:   8.395\n",
      "Epoch: 8: loss =   2.778 , accuracy =   0.099, test_acc =   0.101 time:   8.476\n",
      "Epoch: 9: loss =   2.694 , accuracy =   0.099, test_acc =   0.099 time:   8.352\n",
      "Epoch: 10: loss =   2.623 , accuracy =   0.099, test_acc =   0.097 time:   8.423\n",
      "Epoch: 11: loss =   2.570 , accuracy =   0.102, test_acc =   0.103 time:   8.344\n",
      "Epoch: 12: loss =   2.509 , accuracy =   0.104, test_acc =   0.108 time:   8.418\n",
      "Epoch: 13: loss =   2.451 , accuracy =   0.103, test_acc =   0.099 time:   8.472\n",
      "Epoch: 14: loss =   2.424 , accuracy =   0.104, test_acc =   0.102 time:   8.388\n",
      "Epoch: 15: loss =   2.374 , accuracy =   0.103, test_acc =   0.103 time:   8.342\n",
      "Epoch: 16: loss =   2.358 , accuracy =   0.105, test_acc =   0.098 time:   8.380\n",
      "Epoch: 17: loss =   2.348 , accuracy =   0.104, test_acc =   0.109 time:   8.395\n",
      "Epoch: 18: loss =   2.349 , accuracy =   0.102, test_acc =   0.104 time:   8.369\n",
      "Epoch: 19: loss =   2.336 , accuracy =   0.103, test_acc =   0.102 time:   8.299\n",
      "Epoch: 20: loss =   2.338 , accuracy =   0.104, test_acc =   0.105 time:   8.341\n",
      "Epoch: 21: loss =   2.334 , accuracy =   0.107, test_acc =   0.105 time:   8.392\n",
      "Epoch: 22: loss =   2.322 , accuracy =   0.106, test_acc =   0.105 time:   8.282\n",
      "Epoch: 23: loss =   2.323 , accuracy =   0.109, test_acc =   0.107 time:   8.419\n",
      "Epoch: 24: loss =   2.321 , accuracy =   0.107, test_acc =   0.106 time:   8.368\n",
      "Epoch: 25: loss =   2.313 , accuracy =   0.109, test_acc =   0.111 time:   8.304\n",
      "Epoch: 26: loss =   2.315 , accuracy =   0.107, test_acc =   0.109 time:   8.322\n",
      "Epoch: 27: loss =   2.311 , accuracy =   0.109, test_acc =   0.104 time:   8.221\n",
      "Epoch: 28: loss =   2.307 , accuracy =   0.109, test_acc =   0.109 time:   8.330\n",
      "Epoch: 29: loss =   2.311 , accuracy =   0.111, test_acc =   0.107 time:   8.365\n",
      "Epoch: 30: loss =   2.307 , accuracy =   0.110, test_acc =   0.113 time:   8.375\n",
      "Epoch: 31: loss =   2.307 , accuracy =   0.109, test_acc =   0.103 time:   8.335\n",
      "Epoch: 32: loss =   2.305 , accuracy =   0.108, test_acc =   0.115 time:   8.331\n",
      "Epoch: 33: loss =   2.303 , accuracy =   0.109, test_acc =   0.111 time:   8.297\n",
      "Epoch: 34: loss =   2.307 , accuracy =   0.111, test_acc =   0.104 time:   8.362\n",
      "Epoch: 35: loss =   2.301 , accuracy =   0.111, test_acc =   0.111 time:   8.461\n",
      "Epoch: 36: loss =   2.299 , accuracy =   0.113, test_acc =   0.115 time:   8.438\n",
      "Epoch: 37: loss =   2.300 , accuracy =   0.115, test_acc =   0.111 time:   8.404\n",
      "Epoch: 38: loss =   2.295 , accuracy =   0.117, test_acc =   0.112 time:   8.390\n",
      "Epoch: 39: loss =   2.295 , accuracy =   0.115, test_acc =   0.116 time:   8.340\n",
      "Epoch: 40: loss =   2.295 , accuracy =   0.118, test_acc =   0.121 time:   8.359\n",
      "Epoch: 41: loss =   2.292 , accuracy =   0.114, test_acc =   0.128 time:   8.401\n",
      "Epoch: 42: loss =   2.290 , accuracy =   0.122, test_acc =   0.118 time:   8.392\n",
      "Epoch: 43: loss =   2.288 , accuracy =   0.121, test_acc =   0.114 time:   8.346\n",
      "Epoch: 44: loss =   2.290 , accuracy =   0.121, test_acc =   0.123 time:   8.286\n",
      "Epoch: 45: loss =   2.282 , accuracy =   0.125, test_acc =   0.131 time:   8.337\n",
      "Epoch: 46: loss =   2.276 , accuracy =   0.127, test_acc =   0.126 time:   8.325\n",
      "Epoch: 47: loss =   2.272 , accuracy =   0.124, test_acc =   0.129 time:   8.524\n",
      "Epoch: 48: loss =   2.269 , accuracy =   0.130, test_acc =   0.133 time:   8.360\n",
      "Epoch: 49: loss =   2.268 , accuracy =   0.134, test_acc =   0.136 time:   8.470\n",
      "Epoch: 50: loss =   2.265 , accuracy =   0.138, test_acc =   0.141 time:   8.396\n",
      "Epoch: 51: loss =   2.265 , accuracy =   0.137, test_acc =   0.142 time:   8.342\n",
      "Epoch: 52: loss =   2.247 , accuracy =   0.142, test_acc =   0.145 time:   8.312\n",
      "Epoch: 53: loss =   2.243 , accuracy =   0.143, test_acc =   0.139 time:   8.354\n",
      "Epoch: 54: loss =   2.244 , accuracy =   0.141, test_acc =   0.142 time:   8.352\n",
      "Epoch: 55: loss =   2.270 , accuracy =   0.147, test_acc =   0.141 time:   8.443\n",
      "Epoch: 56: loss =   2.215 , accuracy =   0.137, test_acc =   0.150 time:   8.403\n",
      "Epoch: 57: loss =   2.220 , accuracy =   0.146, test_acc =   0.155 time:   8.372\n",
      "Epoch: 58: loss =   2.209 , accuracy =   0.152, test_acc =   0.146 time:   8.345\n",
      "Epoch: 59: loss =   2.213 , accuracy =   0.148, test_acc =   0.150 time:   8.404\n",
      "Epoch: 60: loss =   2.217 , accuracy =   0.155, test_acc =   0.156 time:   8.349\n",
      "Epoch: 61: loss =   2.229 , accuracy =   0.155, test_acc =   0.157 time:   8.419\n",
      "Epoch: 62: loss =   2.202 , accuracy =   0.156, test_acc =   0.156 time:   8.269\n",
      "Epoch: 63: loss =   2.190 , accuracy =   0.157, test_acc =   0.156 time:   8.388\n",
      "Epoch: 64: loss =   2.219 , accuracy =   0.156, test_acc =   0.153 time:   8.415\n",
      "Epoch: 65: loss =   2.190 , accuracy =   0.166, test_acc =   0.163 time:   8.518\n",
      "Epoch: 66: loss =   2.176 , accuracy =   0.164, test_acc =   0.167 time:   8.503\n",
      "Epoch: 67: loss =   2.184 , accuracy =   0.164, test_acc =   0.160 time:   8.491\n",
      "Epoch: 68: loss =   2.188 , accuracy =   0.166, test_acc =   0.168 time:   8.422\n",
      "Epoch: 69: loss =   2.174 , accuracy =   0.163, test_acc =   0.164 time:   8.379\n",
      "Epoch: 70: loss =   2.176 , accuracy =   0.171, test_acc =   0.169 time:   8.418\n",
      "Epoch: 71: loss =   2.164 , accuracy =   0.170, test_acc =   0.174 time:   8.419\n",
      "Epoch: 72: loss =   2.166 , accuracy =   0.179, test_acc =   0.171 time:   8.392\n",
      "Epoch: 73: loss =   2.198 , accuracy =   0.179, test_acc =   0.176 time:   8.450\n",
      "Epoch: 74: loss =   2.146 , accuracy =   0.175, test_acc =   0.176 time:   8.384\n",
      "Epoch: 75: loss =   2.137 , accuracy =   0.183, test_acc =   0.184 time:   8.345\n",
      "Epoch: 76: loss =   2.136 , accuracy =   0.178, test_acc =   0.184 time:   8.420\n",
      "Epoch: 77: loss =   2.141 , accuracy =   0.179, test_acc =   0.192 time:   8.384\n",
      "Epoch: 78: loss =   2.181 , accuracy =   0.187, test_acc =   0.178 time:   8.326\n",
      "Epoch: 79: loss =   2.127 , accuracy =   0.185, test_acc =   0.188 time:   8.433\n",
      "Epoch: 80: loss =   2.129 , accuracy =   0.196, test_acc =   0.202 time:   8.363\n",
      "Epoch: 81: loss =   2.108 , accuracy =   0.198, test_acc =   0.196 time:   8.358\n",
      "Epoch: 82: loss =   2.109 , accuracy =   0.201, test_acc =   0.208 time:   8.316\n",
      "Epoch: 83: loss =   2.109 , accuracy =   0.197, test_acc =   0.201 time:   8.352\n",
      "Epoch: 84: loss =   2.120 , accuracy =   0.196, test_acc =   0.207 time:   8.440\n",
      "Epoch: 85: loss =   2.088 , accuracy =   0.207, test_acc =   0.221 time:   8.425\n",
      "Epoch: 86: loss =   2.100 , accuracy =   0.213, test_acc =   0.220 time:   8.672\n",
      "Epoch: 87: loss =   2.080 , accuracy =   0.214, test_acc =   0.214 time:   8.538\n",
      "Epoch: 88: loss =   2.115 , accuracy =   0.217, test_acc =   0.216 time:   8.311\n",
      "Epoch: 89: loss =   2.064 , accuracy =   0.217, test_acc =   0.225 time:   8.280\n",
      "Epoch: 90: loss =   2.085 , accuracy =   0.212, test_acc =   0.220 time:   8.349\n",
      "Epoch: 91: loss =   2.071 , accuracy =   0.219, test_acc =   0.226 time:   8.350\n",
      "Epoch: 92: loss =   2.073 , accuracy =   0.219, test_acc =   0.238 time:   8.311\n",
      "Epoch: 93: loss =   2.080 , accuracy =   0.211, test_acc =   0.222 time:   8.261\n",
      "Epoch: 94: loss =   2.074 , accuracy =   0.227, test_acc =   0.233 time:   8.365\n",
      "Epoch: 95: loss =   2.067 , accuracy =   0.224, test_acc =   0.227 time:   8.368\n",
      "Epoch: 96: loss =   2.057 , accuracy =   0.233, test_acc =   0.233 time:   8.324\n",
      "Epoch: 97: loss =   2.056 , accuracy =   0.236, test_acc =   0.238 time:   8.336\n",
      "Epoch: 98: loss =   2.052 , accuracy =   0.235, test_acc =   0.236 time:   8.285\n",
      "Epoch: 99: loss =   2.055 , accuracy =   0.230, test_acc =   0.234 time:   8.335\n",
      "Epoch: 100: loss =   2.037 , accuracy =   0.232, test_acc =   0.237 time:   8.338\n",
      "Epoch: 101: loss =   2.047 , accuracy =   0.234, test_acc =   0.245 time:   8.268\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 102: loss =   2.038 , accuracy =   0.229, test_acc =   0.241 time:   8.321\n",
      "Epoch: 103: loss =   2.029 , accuracy =   0.240, test_acc =   0.233 time:   8.321\n",
      "Epoch: 104: loss =   2.057 , accuracy =   0.232, test_acc =   0.243 time:   8.408\n",
      "Epoch: 105: loss =   2.019 , accuracy =   0.241, test_acc =   0.243 time:   8.431\n",
      "Epoch: 106: loss =   2.020 , accuracy =   0.232, test_acc =   0.245 time:   8.309\n",
      "Epoch: 107: loss =   2.022 , accuracy =   0.246, test_acc =   0.236 time:   8.337\n",
      "Epoch: 108: loss =   2.025 , accuracy =   0.245, test_acc =   0.246 time:   8.350\n",
      "Epoch: 109: loss =   1.999 , accuracy =   0.234, test_acc =   0.247 time:   8.318\n",
      "Epoch: 110: loss =   2.002 , accuracy =   0.246, test_acc =   0.241 time:   8.411\n",
      "Epoch: 111: loss =   2.003 , accuracy =   0.250, test_acc =   0.238 time:   8.377\n",
      "Epoch: 112: loss =   2.010 , accuracy =   0.240, test_acc =   0.253 time:   8.410\n",
      "Epoch: 113: loss =   2.012 , accuracy =   0.247, test_acc =   0.244 time:   8.367\n",
      "Epoch: 114: loss =   1.993 , accuracy =   0.249, test_acc =   0.238 time:   8.480\n",
      "Epoch: 115: loss =   2.000 , accuracy =   0.243, test_acc =   0.245 time:   8.331\n",
      "Epoch: 116: loss =   2.002 , accuracy =   0.254, test_acc =   0.256 time:   8.498\n",
      "Epoch: 117: loss =   1.981 , accuracy =   0.254, test_acc =   0.252 time:   8.460\n",
      "Epoch: 118: loss =   1.989 , accuracy =   0.245, test_acc =   0.263 time:   8.393\n",
      "Epoch: 119: loss =   1.995 , accuracy =   0.260, test_acc =   0.258 time:   8.399\n",
      "Epoch: 120: loss =   1.982 , accuracy =   0.242, test_acc =   0.255 time:   8.291\n",
      "Epoch: 121: loss =   1.983 , accuracy =   0.255, test_acc =   0.255 time:   8.421\n",
      "Epoch: 122: loss =   2.001 , accuracy =   0.246, test_acc =   0.256 time:   8.397\n",
      "Epoch: 123: loss =   1.981 , accuracy =   0.251, test_acc =   0.257 time:   8.330\n",
      "Epoch: 124: loss =   1.977 , accuracy =   0.257, test_acc =   0.265 time:   8.386\n",
      "Epoch: 125: loss =   1.972 , accuracy =   0.256, test_acc =   0.269 time:   8.377\n",
      "Epoch: 126: loss =   1.979 , accuracy =   0.259, test_acc =   0.256 time:   8.393\n",
      "Epoch: 127: loss =   1.944 , accuracy =   0.255, test_acc =   0.254 time:   8.371\n",
      "Epoch: 128: loss =   1.982 , accuracy =   0.263, test_acc =   0.259 time:   8.448\n",
      "Epoch: 129: loss =   1.964 , accuracy =   0.262, test_acc =   0.264 time:   8.365\n",
      "Epoch: 130: loss =   1.959 , accuracy =   0.262, test_acc =   0.264 time:   8.372\n",
      "Epoch: 131: loss =   1.948 , accuracy =   0.269, test_acc =   0.263 time:   8.378\n",
      "Epoch: 132: loss =   1.955 , accuracy =   0.263, test_acc =   0.265 time:   8.388\n",
      "Epoch: 133: loss =   1.942 , accuracy =   0.275, test_acc =   0.276 time:   8.293\n",
      "Epoch: 134: loss =   1.930 , accuracy =   0.259, test_acc =   0.276 time:   8.447\n",
      "Epoch: 135: loss =   1.928 , accuracy =   0.273, test_acc =   0.275 time:   8.469\n",
      "Epoch: 136: loss =   1.942 , accuracy =   0.282, test_acc =   0.284 time:   8.553\n",
      "Epoch: 137: loss =   1.919 , accuracy =   0.280, test_acc =   0.272 time:   8.445\n",
      "Epoch: 138: loss =   1.907 , accuracy =   0.281, test_acc =   0.281 time:   8.369\n",
      "Epoch: 139: loss =   1.890 , accuracy =   0.287, test_acc =   0.284 time:   8.339\n",
      "Epoch: 140: loss =   1.905 , accuracy =   0.293, test_acc =   0.283 time:   8.417\n",
      "Epoch: 141: loss =   1.894 , accuracy =   0.285, test_acc =   0.304 time:   8.355\n",
      "Epoch: 142: loss =   1.896 , accuracy =   0.296, test_acc =   0.293 time:   8.371\n",
      "Epoch: 143: loss =   1.929 , accuracy =   0.298, test_acc =   0.300 time:   8.452\n",
      "Epoch: 144: loss =   1.876 , accuracy =   0.304, test_acc =   0.315 time:   8.473\n",
      "Epoch: 145: loss =   1.883 , accuracy =   0.301, test_acc =   0.306 time:   8.400\n",
      "Epoch: 146: loss =   1.922 , accuracy =   0.305, test_acc =   0.312 time:   8.398\n",
      "Epoch: 147: loss =   1.872 , accuracy =   0.319, test_acc =   0.310 time:   8.496\n",
      "Epoch: 148: loss =   1.900 , accuracy =   0.311, test_acc =   0.317 time:   8.349\n",
      "Epoch: 149: loss =   1.869 , accuracy =   0.309, test_acc =   0.322 time:   8.408\n",
      "Epoch: 150: loss =   1.877 , accuracy =   0.315, test_acc =   0.314 time:   8.393\n",
      "Epoch: 151: loss =   1.860 , accuracy =   0.316, test_acc =   0.320 time:   8.368\n",
      "Epoch: 152: loss =   1.854 , accuracy =   0.318, test_acc =   0.327 time:   8.456\n",
      "Epoch: 153: loss =   1.848 , accuracy =   0.325, test_acc =   0.325 time:   8.563\n",
      "Epoch: 154: loss =   1.847 , accuracy =   0.319, test_acc =   0.326 time:   8.470\n",
      "Epoch: 155: loss =   1.866 , accuracy =   0.321, test_acc =   0.327 time:   8.425\n",
      "Epoch: 156: loss =   1.843 , accuracy =   0.326, test_acc =   0.319 time:   8.355\n",
      "Epoch: 157: loss =   1.827 , accuracy =   0.326, test_acc =   0.328 time:   8.378\n",
      "Epoch: 158: loss =   1.835 , accuracy =   0.330, test_acc =   0.330 time:   8.485\n",
      "Epoch: 159: loss =   1.823 , accuracy =   0.329, test_acc =   0.343 time:   8.466\n",
      "Epoch: 160: loss =   1.810 , accuracy =   0.332, test_acc =   0.335 time:   8.465\n",
      "Epoch: 161: loss =   1.823 , accuracy =   0.336, test_acc =   0.341 time:   8.396\n",
      "Epoch: 162: loss =   1.814 , accuracy =   0.338, test_acc =   0.333 time:   8.527\n",
      "Epoch: 163: loss =   1.796 , accuracy =   0.326, test_acc =   0.340 time:   8.389\n",
      "Epoch: 164: loss =   1.796 , accuracy =   0.339, test_acc =   0.331 time:   8.419\n",
      "Epoch: 165: loss =   1.802 , accuracy =   0.343, test_acc =   0.349 time:   8.312\n",
      "Epoch: 166: loss =   1.777 , accuracy =   0.340, test_acc =   0.354 time:   8.396\n",
      "Epoch: 167: loss =   1.781 , accuracy =   0.341, test_acc =   0.343 time:   8.510\n",
      "Epoch: 168: loss =   1.792 , accuracy =   0.349, test_acc =   0.360 time:   8.426\n",
      "Epoch: 169: loss =   1.782 , accuracy =   0.346, test_acc =   0.349 time:   8.377\n",
      "Epoch: 170: loss =   1.775 , accuracy =   0.348, test_acc =   0.344 time:   8.449\n",
      "Epoch: 171: loss =   1.761 , accuracy =   0.353, test_acc =   0.356 time:   8.413\n",
      "Epoch: 172: loss =   1.784 , accuracy =   0.352, test_acc =   0.360 time:   8.461\n",
      "Epoch: 173: loss =   1.786 , accuracy =   0.348, test_acc =   0.357 time:   8.398\n",
      "Epoch: 174: loss =   1.771 , accuracy =   0.354, test_acc =   0.360 time:   8.390\n",
      "Epoch: 175: loss =   1.763 , accuracy =   0.354, test_acc =   0.351 time:   8.375\n",
      "Epoch: 176: loss =   1.776 , accuracy =   0.361, test_acc =   0.365 time:   8.391\n",
      "Epoch: 177: loss =   1.757 , accuracy =   0.357, test_acc =   0.363 time:   8.336\n",
      "Epoch: 178: loss =   1.749 , accuracy =   0.359, test_acc =   0.368 time:   8.348\n",
      "Epoch: 179: loss =   1.744 , accuracy =   0.362, test_acc =   0.368 time:   8.379\n",
      "Epoch: 180: loss =   1.735 , accuracy =   0.370, test_acc =   0.359 time:   8.382\n",
      "Epoch: 181: loss =   1.739 , accuracy =   0.363, test_acc =   0.374 time:   8.398\n",
      "Epoch: 182: loss =   1.748 , accuracy =   0.368, test_acc =   0.366 time:   8.427\n",
      "Epoch: 183: loss =   1.723 , accuracy =   0.363, test_acc =   0.375 time:   8.495\n",
      "Epoch: 184: loss =   1.723 , accuracy =   0.373, test_acc =   0.363 time:   8.341\n",
      "Epoch: 185: loss =   1.730 , accuracy =   0.367, test_acc =   0.382 time:   8.391\n",
      "Epoch: 186: loss =   1.733 , accuracy =   0.369, test_acc =   0.378 time:   8.421\n",
      "Epoch: 187: loss =   1.738 , accuracy =   0.374, test_acc =   0.383 time:   8.399\n",
      "Epoch: 188: loss =   1.713 , accuracy =   0.374, test_acc =   0.377 time:   8.387\n",
      "Epoch: 189: loss =   1.710 , accuracy =   0.377, test_acc =   0.382 time:   8.494\n",
      "Epoch: 190: loss =   1.700 , accuracy =   0.365, test_acc =   0.384 time:   8.529\n",
      "Epoch: 191: loss =   1.726 , accuracy =   0.376, test_acc =   0.386 time:   8.453\n",
      "Epoch: 192: loss =   1.711 , accuracy =   0.377, test_acc =   0.391 time:   8.355\n",
      "Epoch: 193: loss =   1.705 , accuracy =   0.375, test_acc =   0.394 time:   8.395\n",
      "Epoch: 194: loss =   1.720 , accuracy =   0.383, test_acc =   0.380 time:   8.475\n",
      "Epoch: 195: loss =   1.688 , accuracy =   0.384, test_acc =   0.383 time:   8.495\n",
      "Epoch: 196: loss =   1.702 , accuracy =   0.380, test_acc =   0.390 time:   8.456\n",
      "Epoch: 197: loss =   1.691 , accuracy =   0.384, test_acc =   0.390 time:   8.418\n",
      "Epoch: 198: loss =   1.677 , accuracy =   0.388, test_acc =   0.384 time:   8.452\n",
      "Epoch: 199: loss =   1.684 , accuracy =   0.388, test_acc =   0.391 time:   8.424\n",
      "Epoch: 200: loss =   1.682 , accuracy =   0.388, test_acc =   0.390 time:   8.454\n",
      "Epoch: 201: loss =   1.665 , accuracy =   0.383, test_acc =   0.392 time:   8.341\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 202: loss =   1.659 , accuracy =   0.392, test_acc =   0.395 time:   8.398\n",
      "Epoch: 203: loss =   1.682 , accuracy =   0.397, test_acc =   0.395 time:   8.383\n",
      "Epoch: 204: loss =   1.675 , accuracy =   0.388, test_acc =   0.401 time:   8.452\n",
      "Epoch: 205: loss =   1.663 , accuracy =   0.397, test_acc =   0.400 time:   8.462\n",
      "Epoch: 206: loss =   1.664 , accuracy =   0.393, test_acc =   0.399 time:   8.422\n",
      "Epoch: 207: loss =   1.650 , accuracy =   0.398, test_acc =   0.403 time:   8.361\n",
      "Epoch: 208: loss =   1.638 , accuracy =   0.405, test_acc =   0.389 time:   8.396\n",
      "Epoch: 209: loss =   1.644 , accuracy =   0.403, test_acc =   0.402 time:   8.391\n",
      "Epoch: 210: loss =   1.633 , accuracy =   0.401, test_acc =   0.409 time:   8.418\n",
      "Epoch: 211: loss =   1.643 , accuracy =   0.403, test_acc =   0.404 time:   8.417\n",
      "Epoch: 212: loss =   1.635 , accuracy =   0.407, test_acc =   0.415 time:   8.440\n",
      "Epoch: 213: loss =   1.628 , accuracy =   0.408, test_acc =   0.400 time:   8.449\n",
      "Epoch: 214: loss =   1.649 , accuracy =   0.388, test_acc =   0.411 time:   8.477\n",
      "Epoch: 215: loss =   1.623 , accuracy =   0.403, test_acc =   0.412 time:   8.380\n",
      "Epoch: 216: loss =   1.624 , accuracy =   0.412, test_acc =   0.424 time:   8.420\n",
      "Epoch: 217: loss =   1.620 , accuracy =   0.411, test_acc =   0.408 time:   8.443\n",
      "Epoch: 218: loss =   1.619 , accuracy =   0.408, test_acc =   0.417 time:   8.442\n",
      "Epoch: 219: loss =   1.607 , accuracy =   0.406, test_acc =   0.423 time:   8.505\n",
      "Epoch: 220: loss =   1.599 , accuracy =   0.410, test_acc =   0.414 time:   8.479\n",
      "Epoch: 221: loss =   1.604 , accuracy =   0.416, test_acc =   0.417 time:   8.441\n",
      "Epoch: 222: loss =   1.600 , accuracy =   0.416, test_acc =   0.424 time:   8.451\n",
      "Epoch: 223: loss =   1.596 , accuracy =   0.415, test_acc =   0.419 time:   8.439\n",
      "Epoch: 224: loss =   1.627 , accuracy =   0.423, test_acc =   0.417 time:   8.567\n",
      "Epoch: 225: loss =   1.601 , accuracy =   0.420, test_acc =   0.422 time:   8.419\n",
      "Epoch: 226: loss =   1.625 , accuracy =   0.413, test_acc =   0.425 time:   8.457\n",
      "Epoch: 227: loss =   1.598 , accuracy =   0.421, test_acc =   0.420 time:   8.406\n",
      "Epoch: 228: loss =   1.574 , accuracy =   0.414, test_acc =   0.432 time:   8.474\n",
      "Epoch: 229: loss =   1.586 , accuracy =   0.419, test_acc =   0.430 time:   8.446\n",
      "Epoch: 230: loss =   1.589 , accuracy =   0.424, test_acc =   0.422 time:   8.415\n",
      "Epoch: 231: loss =   1.618 , accuracy =   0.418, test_acc =   0.423 time:   8.421\n",
      "Epoch: 232: loss =   1.598 , accuracy =   0.428, test_acc =   0.428 time:   8.553\n",
      "Epoch: 233: loss =   1.569 , accuracy =   0.418, test_acc =   0.419 time:   8.393\n",
      "Epoch: 234: loss =   1.571 , accuracy =   0.427, test_acc =   0.428 time:   8.414\n",
      "Epoch: 235: loss =   1.595 , accuracy =   0.431, test_acc =   0.428 time:   8.422\n",
      "Epoch: 236: loss =   1.583 , accuracy =   0.429, test_acc =   0.420 time:   8.474\n",
      "Epoch: 237: loss =   1.591 , accuracy =   0.436, test_acc =   0.416 time:   8.497\n",
      "Epoch: 238: loss =   1.579 , accuracy =   0.429, test_acc =   0.424 time:   8.431\n",
      "Epoch: 239: loss =   1.571 , accuracy =   0.434, test_acc =   0.429 time:   8.399\n",
      "Epoch: 240: loss =   1.554 , accuracy =   0.430, test_acc =   0.438 time:   8.465\n",
      "Epoch: 241: loss =   1.563 , accuracy =   0.435, test_acc =   0.435 time:   8.504\n",
      "Epoch: 242: loss =   1.541 , accuracy =   0.440, test_acc =   0.427 time:   8.550\n",
      "Epoch: 243: loss =   1.567 , accuracy =   0.431, test_acc =   0.432 time:   8.440\n",
      "Epoch: 244: loss =   1.568 , accuracy =   0.437, test_acc =   0.441 time:   8.458\n",
      "Epoch: 245: loss =   1.545 , accuracy =   0.429, test_acc =   0.437 time:   8.450\n",
      "Epoch: 246: loss =   1.556 , accuracy =   0.448, test_acc =   0.439 time:   8.546\n",
      "Epoch: 247: loss =   1.563 , accuracy =   0.436, test_acc =   0.441 time:   8.490\n",
      "Epoch: 248: loss =   1.577 , accuracy =   0.441, test_acc =   0.433 time:   8.477\n",
      "Epoch: 249: loss =   1.551 , accuracy =   0.439, test_acc =   0.446 time:   8.448\n",
      "Epoch: 250: loss =   1.536 , accuracy =   0.442, test_acc =   0.440 time:   8.532\n",
      "Epoch: 251: loss =   1.556 , accuracy =   0.444, test_acc =   0.447 time:   8.463\n",
      "Epoch: 252: loss =   1.540 , accuracy =   0.442, test_acc =   0.440 time:   8.499\n",
      "Epoch: 253: loss =   1.534 , accuracy =   0.439, test_acc =   0.451 time:   8.598\n",
      "Epoch: 254: loss =   1.524 , accuracy =   0.438, test_acc =   0.434 time:   8.390\n",
      "Epoch: 255: loss =   1.541 , accuracy =   0.442, test_acc =   0.437 time:   8.489\n",
      "Epoch: 256: loss =   1.526 , accuracy =   0.444, test_acc =   0.450 time:   8.398\n",
      "Epoch: 257: loss =   1.529 , accuracy =   0.443, test_acc =   0.445 time:   8.504\n",
      "Epoch: 258: loss =   1.529 , accuracy =   0.448, test_acc =   0.439 time:   8.431\n",
      "Epoch: 259: loss =   1.523 , accuracy =   0.455, test_acc =   0.443 time:   8.428\n",
      "Epoch: 260: loss =   1.514 , accuracy =   0.452, test_acc =   0.446 time:   8.453\n",
      "Epoch: 261: loss =   1.526 , accuracy =   0.456, test_acc =   0.446 time:   8.464\n",
      "Epoch: 262: loss =   1.499 , accuracy =   0.451, test_acc =   0.441 time:   8.487\n",
      "Epoch: 263: loss =   1.527 , accuracy =   0.460, test_acc =   0.450 time:   8.410\n",
      "Epoch: 264: loss =   1.508 , accuracy =   0.459, test_acc =   0.452 time:   8.457\n",
      "Epoch: 265: loss =   1.506 , accuracy =   0.443, test_acc =   0.448 time:   8.440\n",
      "Epoch: 266: loss =   1.511 , accuracy =   0.459, test_acc =   0.453 time:   8.508\n",
      "Epoch: 267: loss =   1.514 , accuracy =   0.458, test_acc =   0.440 time:   8.505\n",
      "Epoch: 268: loss =   1.513 , accuracy =   0.447, test_acc =   0.457 time:   8.502\n",
      "Epoch: 269: loss =   1.501 , accuracy =   0.462, test_acc =   0.463 time:   8.500\n",
      "Epoch: 270: loss =   1.494 , accuracy =   0.466, test_acc =   0.444 time:   8.435\n",
      "Epoch: 271: loss =   1.496 , accuracy =   0.462, test_acc =   0.447 time:   8.501\n",
      "Epoch: 272: loss =   1.499 , accuracy =   0.461, test_acc =   0.450 time:   8.392\n",
      "Epoch: 273: loss =   1.498 , accuracy =   0.462, test_acc =   0.458 time:   8.432\n",
      "Epoch: 274: loss =   1.509 , accuracy =   0.465, test_acc =   0.458 time:   8.497\n",
      "Epoch: 275: loss =   1.475 , accuracy =   0.466, test_acc =   0.448 time:   8.529\n",
      "Epoch: 276: loss =   1.508 , accuracy =   0.462, test_acc =   0.455 time:   8.552\n",
      "Epoch: 277: loss =   1.468 , accuracy =   0.452, test_acc =   0.459 time:   8.497\n",
      "Epoch: 278: loss =   1.482 , accuracy =   0.468, test_acc =   0.451 time:   8.425\n",
      "Epoch: 279: loss =   1.490 , accuracy =   0.467, test_acc =   0.461 time:   8.465\n",
      "Epoch: 280: loss =   1.476 , accuracy =   0.464, test_acc =   0.463 time:   8.446\n",
      "Epoch: 281: loss =   1.465 , accuracy =   0.464, test_acc =   0.458 time:   8.483\n",
      "Epoch: 282: loss =   1.470 , accuracy =   0.469, test_acc =   0.463 time:   8.451\n",
      "Epoch: 283: loss =   1.477 , accuracy =   0.472, test_acc =   0.466 time:   8.481\n",
      "Epoch: 284: loss =   1.479 , accuracy =   0.461, test_acc =   0.466 time:   8.462\n",
      "Epoch: 285: loss =   1.462 , accuracy =   0.467, test_acc =   0.457 time:   8.505\n",
      "Epoch: 286: loss =   1.461 , accuracy =   0.472, test_acc =   0.460 time:   8.434\n",
      "Epoch: 287: loss =   1.464 , accuracy =   0.467, test_acc =   0.468 time:   8.543\n",
      "Epoch: 288: loss =   1.449 , accuracy =   0.473, test_acc =   0.468 time:   8.487\n",
      "Epoch: 289: loss =   1.487 , accuracy =   0.480, test_acc =   0.464 time:   8.435\n",
      "Epoch: 290: loss =   1.461 , accuracy =   0.477, test_acc =   0.463 time:   8.491\n",
      "Epoch: 291: loss =   1.462 , accuracy =   0.473, test_acc =   0.463 time:   8.616\n",
      "Epoch: 292: loss =   1.437 , accuracy =   0.474, test_acc =   0.460 time:   8.477\n",
      "Epoch: 293: loss =   1.450 , accuracy =   0.470, test_acc =   0.465 time:   8.623\n",
      "Epoch: 294: loss =   1.478 , accuracy =   0.478, test_acc =   0.464 time:   8.439\n",
      "Epoch: 295: loss =   1.484 , accuracy =   0.478, test_acc =   0.464 time:   8.538\n",
      "Epoch: 296: loss =   1.448 , accuracy =   0.474, test_acc =   0.460 time:   8.538\n",
      "Epoch: 297: loss =   1.477 , accuracy =   0.478, test_acc =   0.463 time:   8.499\n",
      "Epoch: 298: loss =   1.450 , accuracy =   0.479, test_acc =   0.464 time:   8.502\n",
      "Epoch: 299: loss =   1.436 , accuracy =   0.475, test_acc =   0.469 time:   8.557\n",
      "Epoch: 300: loss =   1.457 , accuracy =   0.479, test_acc =   0.472 time:   8.568\n",
      "Epoch: 301: loss =   1.439 , accuracy =   0.481, test_acc =   0.462 time:   8.565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 302: loss =   1.435 , accuracy =   0.483, test_acc =   0.467 time:   8.592\n",
      "Epoch: 303: loss =   1.431 , accuracy =   0.483, test_acc =   0.463 time:   8.527\n",
      "Epoch: 304: loss =   1.432 , accuracy =   0.483, test_acc =   0.467 time:   8.418\n",
      "Epoch: 305: loss =   1.430 , accuracy =   0.484, test_acc =   0.475 time:   8.421\n",
      "Epoch: 306: loss =   1.422 , accuracy =   0.485, test_acc =   0.475 time:   8.484\n",
      "Epoch: 307: loss =   1.443 , accuracy =   0.482, test_acc =   0.473 time:   8.522\n",
      "Epoch: 308: loss =   1.434 , accuracy =   0.486, test_acc =   0.474 time:   8.604\n",
      "Epoch: 309: loss =   1.429 , accuracy =   0.486, test_acc =   0.465 time:   8.429\n",
      "Epoch: 310: loss =   1.415 , accuracy =   0.487, test_acc =   0.471 time:   8.430\n",
      "Epoch: 311: loss =   1.434 , accuracy =   0.483, test_acc =   0.469 time:   8.438\n",
      "Epoch: 312: loss =   1.435 , accuracy =   0.488, test_acc =   0.476 time:   8.434\n",
      "Epoch: 313: loss =   1.420 , accuracy =   0.486, test_acc =   0.465 time:   8.525\n",
      "Epoch: 314: loss =   1.414 , accuracy =   0.491, test_acc =   0.478 time:   8.568\n",
      "Epoch: 315: loss =   1.419 , accuracy =   0.490, test_acc =   0.472 time:   8.445\n",
      "Epoch: 316: loss =   1.414 , accuracy =   0.492, test_acc =   0.473 time:   8.536\n",
      "Epoch: 317: loss =   1.444 , accuracy =   0.496, test_acc =   0.479 time:   8.472\n",
      "Epoch: 318: loss =   1.420 , accuracy =   0.494, test_acc =   0.475 time:   8.539\n",
      "Epoch: 319: loss =   1.418 , accuracy =   0.485, test_acc =   0.481 time:   8.482\n",
      "Epoch: 320: loss =   1.397 , accuracy =   0.495, test_acc =   0.479 time:   8.595\n",
      "Epoch: 321: loss =   1.418 , accuracy =   0.493, test_acc =   0.472 time:   8.430\n",
      "Epoch: 322: loss =   1.410 , accuracy =   0.496, test_acc =   0.469 time:   8.382\n",
      "Epoch: 323: loss =   1.418 , accuracy =   0.494, test_acc =   0.481 time:   8.466\n",
      "Epoch: 324: loss =   1.409 , accuracy =   0.487, test_acc =   0.477 time:   8.437\n",
      "Epoch: 325: loss =   1.397 , accuracy =   0.492, test_acc =   0.481 time:   8.487\n",
      "Epoch: 326: loss =   1.406 , accuracy =   0.498, test_acc =   0.481 time:   8.461\n",
      "Epoch: 327: loss =   1.388 , accuracy =   0.498, test_acc =   0.473 time:   8.590\n",
      "Epoch: 328: loss =   1.399 , accuracy =   0.500, test_acc =   0.479 time:   8.635\n",
      "Epoch: 329: loss =   1.395 , accuracy =   0.497, test_acc =   0.481 time:   8.588\n",
      "Epoch: 330: loss =   1.388 , accuracy =   0.499, test_acc =   0.480 time:   8.639\n",
      "Epoch: 331: loss =   1.387 , accuracy =   0.500, test_acc =   0.479 time:   8.481\n",
      "Epoch: 332: loss =   1.394 , accuracy =   0.498, test_acc =   0.483 time:   8.470\n",
      "Epoch: 333: loss =   1.381 , accuracy =   0.500, test_acc =   0.484 time:   8.473\n",
      "Epoch: 334: loss =   1.378 , accuracy =   0.501, test_acc =   0.474 time:   8.507\n",
      "Epoch: 335: loss =   1.391 , accuracy =   0.505, test_acc =   0.484 time:   8.498\n",
      "Epoch: 336: loss =   1.400 , accuracy =   0.499, test_acc =   0.482 time:   8.494\n",
      "Epoch: 337: loss =   1.391 , accuracy =   0.497, test_acc =   0.480 time:   8.573\n",
      "Epoch: 338: loss =   1.381 , accuracy =   0.500, test_acc =   0.482 time:   8.595\n",
      "Epoch: 339: loss =   1.405 , accuracy =   0.502, test_acc =   0.476 time:   8.556\n",
      "Epoch: 340: loss =   1.381 , accuracy =   0.503, test_acc =   0.483 time:   8.602\n",
      "Epoch: 341: loss =   1.392 , accuracy =   0.506, test_acc =   0.482 time:   8.622\n",
      "Epoch: 342: loss =   1.376 , accuracy =   0.504, test_acc =   0.487 time:   8.490\n",
      "Epoch: 343: loss =   1.382 , accuracy =   0.504, test_acc =   0.488 time:   8.467\n",
      "Epoch: 344: loss =   1.368 , accuracy =   0.509, test_acc =   0.485 time:   8.448\n",
      "Epoch: 345: loss =   1.363 , accuracy =   0.509, test_acc =   0.490 time:   8.669\n",
      "Epoch: 346: loss =   1.381 , accuracy =   0.507, test_acc =   0.486 time:   8.487\n",
      "Epoch: 347: loss =   1.356 , accuracy =   0.507, test_acc =   0.472 time:   8.621\n",
      "Epoch: 348: loss =   1.384 , accuracy =   0.512, test_acc =   0.485 time:   8.546\n",
      "Epoch: 349: loss =   1.357 , accuracy =   0.506, test_acc =   0.481 time:   8.493\n",
      "Epoch: 350: loss =   1.374 , accuracy =   0.509, test_acc =   0.486 time:   8.431\n",
      "Epoch: 351: loss =   1.361 , accuracy =   0.510, test_acc =   0.487 time:   8.570\n",
      "Epoch: 352: loss =   1.361 , accuracy =   0.515, test_acc =   0.495 time:   8.524\n",
      "Epoch: 353: loss =   1.359 , accuracy =   0.509, test_acc =   0.487 time:   8.572\n",
      "Epoch: 354: loss =   1.351 , accuracy =   0.508, test_acc =   0.495 time:   8.558\n",
      "Epoch: 355: loss =   1.336 , accuracy =   0.514, test_acc =   0.484 time:   8.503\n",
      "Epoch: 356: loss =   1.366 , accuracy =   0.512, test_acc =   0.490 time:   8.636\n",
      "Epoch: 357: loss =   1.366 , accuracy =   0.515, test_acc =   0.487 time:   8.456\n",
      "Epoch: 358: loss =   1.363 , accuracy =   0.512, test_acc =   0.492 time:   8.524\n",
      "Epoch: 359: loss =   1.343 , accuracy =   0.507, test_acc =   0.485 time:   8.614\n",
      "Epoch: 360: loss =   1.340 , accuracy =   0.510, test_acc =   0.484 time:   8.483\n",
      "Epoch: 361: loss =   1.392 , accuracy =   0.506, test_acc =   0.483 time:   8.569\n",
      "Epoch: 362: loss =   1.368 , accuracy =   0.514, test_acc =   0.488 time:   8.507\n",
      "Epoch: 363: loss =   1.341 , accuracy =   0.511, test_acc =   0.488 time:   8.603\n",
      "Epoch: 364: loss =   1.372 , accuracy =   0.511, test_acc =   0.483 time:   8.453\n",
      "Epoch: 365: loss =   1.369 , accuracy =   0.515, test_acc =   0.489 time:   8.500\n",
      "Epoch: 366: loss =   1.363 , accuracy =   0.514, test_acc =   0.490 time:   8.462\n",
      "Epoch: 367: loss =   1.370 , accuracy =   0.514, test_acc =   0.486 time:   8.507\n",
      "Epoch: 368: loss =   1.363 , accuracy =   0.520, test_acc =   0.488 time:   8.547\n",
      "Epoch: 369: loss =   1.333 , accuracy =   0.514, test_acc =   0.489 time:   8.518\n",
      "Epoch: 370: loss =   1.358 , accuracy =   0.512, test_acc =   0.491 time:   8.507\n",
      "Epoch: 371: loss =   1.356 , accuracy =   0.516, test_acc =   0.490 time:   8.426\n",
      "Epoch: 372: loss =   1.333 , accuracy =   0.518, test_acc =   0.499 time:   8.534\n",
      "Epoch: 373: loss =   1.342 , accuracy =   0.519, test_acc =   0.491 time:   8.639\n",
      "Epoch: 374: loss =   1.361 , accuracy =   0.519, test_acc =   0.488 time:   8.513\n",
      "Epoch: 375: loss =   1.340 , accuracy =   0.520, test_acc =   0.500 time:   8.553\n",
      "Epoch: 376: loss =   1.340 , accuracy =   0.522, test_acc =   0.491 time:   8.511\n",
      "Epoch: 377: loss =   1.323 , accuracy =   0.525, test_acc =   0.491 time:   8.502\n",
      "Epoch: 378: loss =   1.322 , accuracy =   0.523, test_acc =   0.490 time:   8.669\n",
      "Epoch: 379: loss =   1.331 , accuracy =   0.521, test_acc =   0.495 time:   8.618\n",
      "Epoch: 380: loss =   1.328 , accuracy =   0.517, test_acc =   0.492 time:   8.554\n",
      "Epoch: 381: loss =   1.327 , accuracy =   0.522, test_acc =   0.490 time:   8.556\n",
      "Epoch: 382: loss =   1.318 , accuracy =   0.528, test_acc =   0.502 time:   8.616\n",
      "Epoch: 383: loss =   1.323 , accuracy =   0.525, test_acc =   0.493 time:   8.522\n",
      "Epoch: 384: loss =   1.312 , accuracy =   0.526, test_acc =   0.497 time:   8.478\n",
      "Epoch: 385: loss =   1.318 , accuracy =   0.518, test_acc =   0.500 time:   8.509\n",
      "Epoch: 386: loss =   1.320 , accuracy =   0.528, test_acc =   0.495 time:   8.584\n",
      "Epoch: 387: loss =   1.318 , accuracy =   0.528, test_acc =   0.488 time:   8.493\n",
      "Epoch: 388: loss =   1.315 , accuracy =   0.518, test_acc =   0.497 time:   8.495\n",
      "Epoch: 389: loss =   1.325 , accuracy =   0.529, test_acc =   0.499 time:   8.521\n",
      "Epoch: 390: loss =   1.317 , accuracy =   0.521, test_acc =   0.496 time:   8.474\n",
      "Epoch: 391: loss =   1.320 , accuracy =   0.525, test_acc =   0.503 time:   8.547\n",
      "Epoch: 392: loss =   1.313 , accuracy =   0.527, test_acc =   0.498 time:   8.499\n",
      "Epoch: 393: loss =   1.312 , accuracy =   0.527, test_acc =   0.493 time:   8.527\n",
      "Epoch: 394: loss =   1.316 , accuracy =   0.529, test_acc =   0.499 time:   8.513\n",
      "Epoch: 395: loss =   1.302 , accuracy =   0.526, test_acc =   0.499 time:   8.622\n",
      "Epoch: 396: loss =   1.306 , accuracy =   0.527, test_acc =   0.498 time:   8.496\n",
      "Epoch: 397: loss =   1.310 , accuracy =   0.529, test_acc =   0.501 time:   8.796\n",
      "Epoch: 398: loss =   1.292 , accuracy =   0.538, test_acc =   0.504 time:   8.572\n",
      "Epoch: 399: loss =   1.308 , accuracy =   0.535, test_acc =   0.496 time:   8.578\n",
      "Epoch: 400: loss =   1.304 , accuracy =   0.533, test_acc =   0.502 time:   8.657\n",
      "Epoch: 401: loss =   1.308 , accuracy =   0.532, test_acc =   0.499 time:   8.584\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 402: loss =   1.294 , accuracy =   0.533, test_acc =   0.502 time:   8.452\n",
      "Epoch: 403: loss =   1.316 , accuracy =   0.534, test_acc =   0.506 time:   8.499\n",
      "Epoch: 404: loss =   1.292 , accuracy =   0.534, test_acc =   0.497 time:   8.486\n",
      "Epoch: 405: loss =   1.303 , accuracy =   0.533, test_acc =   0.506 time:   8.560\n",
      "Epoch: 406: loss =   1.293 , accuracy =   0.534, test_acc =   0.503 time:   8.478\n",
      "Epoch: 407: loss =   1.289 , accuracy =   0.536, test_acc =   0.495 time:   8.594\n",
      "Epoch: 408: loss =   1.293 , accuracy =   0.530, test_acc =   0.505 time:   8.533\n",
      "Epoch: 409: loss =   1.294 , accuracy =   0.530, test_acc =   0.501 time:   8.586\n",
      "Epoch: 410: loss =   1.284 , accuracy =   0.532, test_acc =   0.502 time:   8.597\n",
      "Epoch: 411: loss =   1.299 , accuracy =   0.536, test_acc =   0.503 time:   8.481\n",
      "Epoch: 412: loss =   1.296 , accuracy =   0.536, test_acc =   0.498 time:   8.563\n",
      "Epoch: 413: loss =   1.290 , accuracy =   0.532, test_acc =   0.503 time:   8.475\n",
      "Epoch: 414: loss =   1.297 , accuracy =   0.539, test_acc =   0.505 time:   8.549\n",
      "Epoch: 415: loss =   1.292 , accuracy =   0.531, test_acc =   0.501 time:   8.569\n",
      "Epoch: 416: loss =   1.298 , accuracy =   0.533, test_acc =   0.495 time:   8.498\n",
      "Epoch: 417: loss =   1.293 , accuracy =   0.530, test_acc =   0.504 time:   8.580\n",
      "Epoch: 418: loss =   1.289 , accuracy =   0.533, test_acc =   0.495 time:   8.524\n",
      "Epoch: 419: loss =   1.293 , accuracy =   0.537, test_acc =   0.494 time:   8.658\n",
      "Epoch: 420: loss =   1.287 , accuracy =   0.531, test_acc =   0.496 time:   8.622\n",
      "Epoch: 421: loss =   1.296 , accuracy =   0.543, test_acc =   0.504 time:   8.549\n",
      "Epoch: 422: loss =   1.278 , accuracy =   0.540, test_acc =   0.501 time:   8.661\n",
      "Epoch: 423: loss =   1.291 , accuracy =   0.539, test_acc =   0.498 time:   8.527\n",
      "Epoch: 424: loss =   1.278 , accuracy =   0.535, test_acc =   0.503 time:   8.806\n",
      "Epoch: 425: loss =   1.282 , accuracy =   0.543, test_acc =   0.499 time:   8.506\n",
      "Epoch: 426: loss =   1.299 , accuracy =   0.537, test_acc =   0.502 time:   8.571\n",
      "Epoch: 427: loss =   1.280 , accuracy =   0.541, test_acc =   0.503 time:   8.521\n",
      "Epoch: 428: loss =   1.265 , accuracy =   0.544, test_acc =   0.503 time:   8.545\n",
      "Epoch: 429: loss =   1.271 , accuracy =   0.544, test_acc =   0.506 time:   8.549\n",
      "Epoch: 430: loss =   1.277 , accuracy =   0.537, test_acc =   0.504 time:   8.582\n",
      "Epoch: 431: loss =   1.266 , accuracy =   0.543, test_acc =   0.502 time:   8.521\n",
      "Epoch: 432: loss =   1.272 , accuracy =   0.543, test_acc =   0.509 time:   8.527\n",
      "Epoch: 433: loss =   1.253 , accuracy =   0.544, test_acc =   0.503 time:   8.479\n",
      "Epoch: 434: loss =   1.264 , accuracy =   0.544, test_acc =   0.502 time:   8.500\n",
      "Epoch: 435: loss =   1.261 , accuracy =   0.543, test_acc =   0.500 time:   8.463\n",
      "Epoch: 436: loss =   1.249 , accuracy =   0.543, test_acc =   0.503 time:   8.531\n",
      "Epoch: 437: loss =   1.255 , accuracy =   0.538, test_acc =   0.495 time:   8.600\n",
      "Epoch: 438: loss =   1.281 , accuracy =   0.544, test_acc =   0.514 time:   8.687\n",
      "Epoch: 439: loss =   1.258 , accuracy =   0.549, test_acc =   0.508 time:   8.485\n",
      "Epoch: 440: loss =   1.253 , accuracy =   0.546, test_acc =   0.500 time:   8.574\n",
      "Epoch: 441: loss =   1.257 , accuracy =   0.542, test_acc =   0.514 time:   8.518\n",
      "Epoch: 442: loss =   1.256 , accuracy =   0.546, test_acc =   0.506 time:   8.508\n",
      "Epoch: 443: loss =   1.250 , accuracy =   0.544, test_acc =   0.505 time:   8.543\n",
      "Epoch: 444: loss =   1.268 , accuracy =   0.546, test_acc =   0.508 time:   8.470\n",
      "Epoch: 445: loss =   1.282 , accuracy =   0.544, test_acc =   0.501 time:   8.601\n",
      "Epoch: 446: loss =   1.268 , accuracy =   0.550, test_acc =   0.509 time:   8.550\n",
      "Epoch: 447: loss =   1.247 , accuracy =   0.552, test_acc =   0.504 time:   8.525\n",
      "Epoch: 448: loss =   1.251 , accuracy =   0.549, test_acc =   0.506 time:   8.494\n",
      "Epoch: 449: loss =   1.265 , accuracy =   0.553, test_acc =   0.505 time:   8.471\n",
      "Epoch: 450: loss =   1.245 , accuracy =   0.548, test_acc =   0.508 time:   8.512\n",
      "Epoch: 451: loss =   1.245 , accuracy =   0.553, test_acc =   0.510 time:   8.472\n",
      "Epoch: 452: loss =   1.267 , accuracy =   0.550, test_acc =   0.509 time:   8.553\n",
      "Epoch: 453: loss =   1.238 , accuracy =   0.549, test_acc =   0.504 time:   8.559\n",
      "Epoch: 454: loss =   1.251 , accuracy =   0.557, test_acc =   0.512 time:   8.600\n",
      "Epoch: 455: loss =   1.242 , accuracy =   0.558, test_acc =   0.512 time:   8.571\n",
      "Epoch: 456: loss =   1.249 , accuracy =   0.556, test_acc =   0.504 time:   8.581\n",
      "Epoch: 457: loss =   1.245 , accuracy =   0.558, test_acc =   0.506 time:   8.554\n",
      "Epoch: 458: loss =   1.230 , accuracy =   0.557, test_acc =   0.507 time:   8.536\n",
      "Epoch: 459: loss =   1.230 , accuracy =   0.553, test_acc =   0.508 time:   8.532\n",
      "Epoch: 460: loss =   1.236 , accuracy =   0.556, test_acc =   0.511 time:   8.504\n",
      "Epoch: 461: loss =   1.244 , accuracy =   0.556, test_acc =   0.511 time:   8.536\n",
      "Epoch: 462: loss =   1.226 , accuracy =   0.555, test_acc =   0.504 time:   8.545\n",
      "Epoch: 463: loss =   1.231 , accuracy =   0.561, test_acc =   0.508 time:   8.504\n",
      "Epoch: 464: loss =   1.222 , accuracy =   0.557, test_acc =   0.512 time:   8.488\n",
      "Epoch: 465: loss =   1.223 , accuracy =   0.553, test_acc =   0.508 time:   8.520\n",
      "Epoch: 466: loss =   1.233 , accuracy =   0.560, test_acc =   0.512 time:   8.498\n",
      "Epoch: 467: loss =   1.254 , accuracy =   0.559, test_acc =   0.512 time:   8.497\n",
      "Epoch: 468: loss =   1.229 , accuracy =   0.566, test_acc =   0.511 time:   8.527\n",
      "Epoch: 469: loss =   1.220 , accuracy =   0.557, test_acc =   0.510 time:   8.561\n",
      "Epoch: 470: loss =   1.221 , accuracy =   0.557, test_acc =   0.514 time:   8.464\n",
      "Epoch: 471: loss =   1.224 , accuracy =   0.562, test_acc =   0.511 time:   8.551\n",
      "Epoch: 472: loss =   1.206 , accuracy =   0.560, test_acc =   0.506 time:   8.510\n",
      "Epoch: 473: loss =   1.225 , accuracy =   0.558, test_acc =   0.510 time:   8.544\n",
      "Epoch: 474: loss =   1.228 , accuracy =   0.558, test_acc =   0.505 time:   8.577\n",
      "Epoch: 475: loss =   1.209 , accuracy =   0.566, test_acc =   0.505 time:   8.610\n",
      "Epoch: 476: loss =   1.212 , accuracy =   0.567, test_acc =   0.508 time:   8.508\n",
      "Epoch: 477: loss =   1.208 , accuracy =   0.565, test_acc =   0.509 time:   8.472\n",
      "Epoch: 478: loss =   1.210 , accuracy =   0.565, test_acc =   0.513 time:   8.463\n",
      "Epoch: 479: loss =   1.195 , accuracy =   0.563, test_acc =   0.510 time:   8.413\n",
      "Epoch: 480: loss =   1.213 , accuracy =   0.564, test_acc =   0.512 time:   8.545\n",
      "Epoch: 481: loss =   1.211 , accuracy =   0.566, test_acc =   0.510 time:   8.516\n",
      "Epoch: 482: loss =   1.193 , accuracy =   0.565, test_acc =   0.512 time:   8.523\n",
      "Epoch: 483: loss =   1.211 , accuracy =   0.568, test_acc =   0.514 time:   8.506\n",
      "Epoch: 484: loss =   1.207 , accuracy =   0.564, test_acc =   0.509 time:   8.489\n",
      "Epoch: 485: loss =   1.210 , accuracy =   0.571, test_acc =   0.507 time:   8.552\n",
      "Epoch: 486: loss =   1.211 , accuracy =   0.569, test_acc =   0.505 time:   8.509\n",
      "Epoch: 487: loss =   1.204 , accuracy =   0.570, test_acc =   0.520 time:   8.488\n",
      "Epoch: 488: loss =   1.200 , accuracy =   0.568, test_acc =   0.517 time:   8.534\n",
      "Epoch: 489: loss =   1.196 , accuracy =   0.570, test_acc =   0.512 time:   8.529\n",
      "Epoch: 490: loss =   1.192 , accuracy =   0.575, test_acc =   0.512 time:   8.545\n",
      "Epoch: 491: loss =   1.194 , accuracy =   0.570, test_acc =   0.513 time:   8.511\n",
      "Epoch: 492: loss =   1.200 , accuracy =   0.574, test_acc =   0.509 time:   8.499\n",
      "Epoch: 493: loss =   1.201 , accuracy =   0.567, test_acc =   0.512 time:   8.463\n",
      "Epoch: 494: loss =   1.189 , accuracy =   0.577, test_acc =   0.511 time:   8.521\n",
      "Epoch: 495: loss =   1.202 , accuracy =   0.574, test_acc =   0.515 time:   8.485\n",
      "Epoch: 496: loss =   1.190 , accuracy =   0.570, test_acc =   0.511 time:   8.648\n",
      "Epoch: 497: loss =   1.204 , accuracy =   0.574, test_acc =   0.512 time:   8.511\n",
      "Epoch: 498: loss =   1.203 , accuracy =   0.570, test_acc =   0.516 time:   8.540\n",
      "Epoch: 499: loss =   1.182 , accuracy =   0.575, test_acc =   0.511 time:   8.591\n",
      "Epoch: 500: loss =   1.175 , accuracy =   0.575, test_acc =   0.514 time:   8.615\n",
      "Epoch: 501: loss =   1.176 , accuracy =   0.576, test_acc =   0.517 time:   8.715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 502: loss =   1.176 , accuracy =   0.576, test_acc =   0.513 time:   8.647\n",
      "Epoch: 503: loss =   1.177 , accuracy =   0.578, test_acc =   0.509 time:   8.655\n",
      "Epoch: 504: loss =   1.180 , accuracy =   0.576, test_acc =   0.515 time:   8.607\n",
      "Epoch: 505: loss =   1.179 , accuracy =   0.578, test_acc =   0.517 time:   8.721\n",
      "Epoch: 506: loss =   1.177 , accuracy =   0.578, test_acc =   0.514 time:   8.594\n",
      "Epoch: 507: loss =   1.180 , accuracy =   0.577, test_acc =   0.508 time:   8.728\n",
      "Epoch: 508: loss =   1.177 , accuracy =   0.581, test_acc =   0.519 time:   8.663\n",
      "Epoch: 509: loss =   1.163 , accuracy =   0.577, test_acc =   0.519 time:   8.750\n",
      "Epoch: 510: loss =   1.160 , accuracy =   0.581, test_acc =   0.514 time:   8.652\n",
      "Epoch: 511: loss =   1.175 , accuracy =   0.581, test_acc =   0.510 time:   8.723\n",
      "Epoch: 512: loss =   1.157 , accuracy =   0.582, test_acc =   0.516 time:   8.783\n",
      "Epoch: 513: loss =   1.165 , accuracy =   0.580, test_acc =   0.510 time:   8.651\n",
      "Epoch: 514: loss =   1.175 , accuracy =   0.580, test_acc =   0.512 time:   8.924\n",
      "Epoch: 515: loss =   1.158 , accuracy =   0.580, test_acc =   0.513 time:   8.635\n",
      "Epoch: 516: loss =   1.155 , accuracy =   0.581, test_acc =   0.515 time:   8.650\n",
      "Epoch: 517: loss =   1.162 , accuracy =   0.580, test_acc =   0.511 time:   8.707\n",
      "Epoch: 518: loss =   1.176 , accuracy =   0.584, test_acc =   0.517 time:   8.673\n",
      "Epoch: 519: loss =   1.159 , accuracy =   0.585, test_acc =   0.514 time:   8.634\n",
      "Epoch: 520: loss =   1.173 , accuracy =   0.583, test_acc =   0.516 time:   8.607\n",
      "Epoch: 521: loss =   1.158 , accuracy =   0.588, test_acc =   0.513 time:   8.701\n",
      "Epoch: 522: loss =   1.146 , accuracy =   0.581, test_acc =   0.510 time:   8.616\n",
      "Epoch: 523: loss =   1.152 , accuracy =   0.588, test_acc =   0.514 time:   8.651\n",
      "Epoch: 524: loss =   1.150 , accuracy =   0.586, test_acc =   0.511 time:   8.577\n",
      "Epoch: 525: loss =   1.149 , accuracy =   0.585, test_acc =   0.509 time:   8.649\n",
      "Epoch: 526: loss =   1.161 , accuracy =   0.577, test_acc =   0.508 time:   8.651\n",
      "Epoch: 527: loss =   1.176 , accuracy =   0.580, test_acc =   0.503 time:   8.587\n",
      "Epoch: 528: loss =   1.165 , accuracy =   0.584, test_acc =   0.514 time:   8.630\n",
      "Epoch: 529: loss =   1.156 , accuracy =   0.584, test_acc =   0.507 time:   8.615\n",
      "Epoch: 530: loss =   1.153 , accuracy =   0.586, test_acc =   0.509 time:   8.566\n",
      "Epoch: 531: loss =   1.158 , accuracy =   0.581, test_acc =   0.505 time:   8.533\n",
      "Epoch: 532: loss =   1.150 , accuracy =   0.581, test_acc =   0.511 time:   8.583\n",
      "Epoch: 533: loss =   1.153 , accuracy =   0.587, test_acc =   0.510 time:   8.633\n",
      "Epoch: 534: loss =   1.150 , accuracy =   0.586, test_acc =   0.513 time:   8.715\n",
      "Epoch: 535: loss =   1.151 , accuracy =   0.596, test_acc =   0.516 time:   8.733\n",
      "Epoch: 536: loss =   1.143 , accuracy =   0.592, test_acc =   0.510 time:   8.627\n",
      "Epoch: 537: loss =   1.143 , accuracy =   0.592, test_acc =   0.512 time:   8.698\n",
      "Epoch: 538: loss =   1.139 , accuracy =   0.591, test_acc =   0.509 time:   8.683\n",
      "Epoch: 539: loss =   1.148 , accuracy =   0.594, test_acc =   0.512 time:   8.633\n",
      "Epoch: 540: loss =   1.137 , accuracy =   0.591, test_acc =   0.509 time:   8.684\n",
      "Epoch: 541: loss =   1.150 , accuracy =   0.592, test_acc =   0.515 time:   8.581\n",
      "Epoch: 542: loss =   1.135 , accuracy =   0.595, test_acc =   0.511 time:   8.636\n",
      "Epoch: 543: loss =   1.137 , accuracy =   0.592, test_acc =   0.519 time:   8.633\n",
      "Epoch: 544: loss =   1.123 , accuracy =   0.597, test_acc =   0.514 time:   8.695\n",
      "Epoch: 545: loss =   1.131 , accuracy =   0.591, test_acc =   0.514 time:   8.637\n",
      "Epoch: 546: loss =   1.111 , accuracy =   0.600, test_acc =   0.512 time:   8.577\n",
      "Epoch: 547: loss =   1.110 , accuracy =   0.599, test_acc =   0.514 time:   8.609\n",
      "Epoch: 548: loss =   1.117 , accuracy =   0.602, test_acc =   0.511 time:   8.672\n",
      "Epoch: 549: loss =   1.116 , accuracy =   0.599, test_acc =   0.509 time:   8.663\n",
      "Epoch: 550: loss =   1.115 , accuracy =   0.598, test_acc =   0.510 time:   8.642\n",
      "Epoch: 551: loss =   1.118 , accuracy =   0.600, test_acc =   0.510 time:   8.722\n",
      "Epoch: 552: loss =   1.120 , accuracy =   0.601, test_acc =   0.506 time:   8.675\n",
      "Epoch: 553: loss =   1.112 , accuracy =   0.593, test_acc =   0.512 time:   8.592\n",
      "Epoch: 554: loss =   1.118 , accuracy =   0.598, test_acc =   0.514 time:   8.623\n",
      "Epoch: 555: loss =   1.126 , accuracy =   0.596, test_acc =   0.506 time:   8.748\n",
      "Epoch: 556: loss =   1.114 , accuracy =   0.598, test_acc =   0.505 time:   8.722\n",
      "Epoch: 557: loss =   1.126 , accuracy =   0.604, test_acc =   0.510 time:   8.622\n",
      "Epoch: 558: loss =   1.099 , accuracy =   0.605, test_acc =   0.511 time:   8.665\n",
      "Epoch: 559: loss =   1.100 , accuracy =   0.596, test_acc =   0.506 time:   8.603\n",
      "Epoch: 560: loss =   1.109 , accuracy =   0.595, test_acc =   0.510 time:   8.710\n",
      "Epoch: 561: loss =   1.106 , accuracy =   0.601, test_acc =   0.510 time:   8.702\n",
      "Epoch: 562: loss =   1.112 , accuracy =   0.601, test_acc =   0.510 time:   8.636\n",
      "Epoch: 563: loss =   1.112 , accuracy =   0.603, test_acc =   0.506 time:   8.703\n",
      "Epoch: 564: loss =   1.094 , accuracy =   0.598, test_acc =   0.513 time:   8.640\n",
      "Epoch: 565: loss =   1.106 , accuracy =   0.601, test_acc =   0.512 time:   8.743\n",
      "Epoch: 566: loss =   1.110 , accuracy =   0.595, test_acc =   0.504 time:   8.666\n",
      "Epoch: 567: loss =   1.114 , accuracy =   0.599, test_acc =   0.510 time:   8.644\n",
      "Epoch: 568: loss =   1.122 , accuracy =   0.599, test_acc =   0.505 time:   8.789\n",
      "Epoch: 569: loss =   1.123 , accuracy =   0.606, test_acc =   0.509 time:   8.734\n",
      "Epoch: 570: loss =   1.106 , accuracy =   0.612, test_acc =   0.505 time:   8.654\n",
      "Epoch: 571: loss =   1.095 , accuracy =   0.603, test_acc =   0.508 time:   8.609\n",
      "Epoch: 572: loss =   1.101 , accuracy =   0.603, test_acc =   0.510 time:   8.643\n",
      "Epoch: 573: loss =   1.099 , accuracy =   0.605, test_acc =   0.506 time:   8.717\n",
      "Epoch: 574: loss =   1.114 , accuracy =   0.601, test_acc =   0.511 time:   8.734\n",
      "Epoch: 575: loss =   1.096 , accuracy =   0.607, test_acc =   0.513 time:   8.645\n",
      "Epoch: 576: loss =   1.080 , accuracy =   0.606, test_acc =   0.508 time:   8.652\n",
      "Epoch: 577: loss =   1.099 , accuracy =   0.611, test_acc =   0.513 time:   8.760\n",
      "Epoch: 578: loss =   1.084 , accuracy =   0.609, test_acc =   0.509 time:   8.647\n",
      "Epoch: 579: loss =   1.093 , accuracy =   0.615, test_acc =   0.510 time:   8.613\n",
      "Epoch: 580: loss =   1.084 , accuracy =   0.612, test_acc =   0.507 time:   8.732\n",
      "Epoch: 581: loss =   1.092 , accuracy =   0.612, test_acc =   0.503 time:   8.605\n",
      "Epoch: 582: loss =   1.081 , accuracy =   0.600, test_acc =   0.508 time:   8.667\n",
      "Epoch: 583: loss =   1.088 , accuracy =   0.609, test_acc =   0.509 time:   8.675\n",
      "Epoch: 584: loss =   1.085 , accuracy =   0.615, test_acc =   0.507 time:   8.722\n",
      "Epoch: 585: loss =   1.085 , accuracy =   0.610, test_acc =   0.512 time:   8.566\n",
      "Epoch: 586: loss =   1.084 , accuracy =   0.613, test_acc =   0.506 time:   8.615\n",
      "Epoch: 587: loss =   1.071 , accuracy =   0.616, test_acc =   0.511 time:   8.656\n",
      "Epoch: 588: loss =   1.057 , accuracy =   0.618, test_acc =   0.506 time:   8.712\n",
      "Epoch: 589: loss =   1.071 , accuracy =   0.613, test_acc =   0.506 time:   8.649\n",
      "Epoch: 590: loss =   1.065 , accuracy =   0.625, test_acc =   0.511 time:   8.674\n",
      "Epoch: 591: loss =   1.057 , accuracy =   0.612, test_acc =   0.506 time:   8.626\n",
      "Epoch: 592: loss =   1.069 , accuracy =   0.611, test_acc =   0.499 time:   8.717\n",
      "Epoch: 593: loss =   1.071 , accuracy =   0.620, test_acc =   0.509 time:   8.729\n",
      "Epoch: 594: loss =   1.058 , accuracy =   0.613, test_acc =   0.502 time:   8.692\n",
      "Epoch: 595: loss =   1.057 , accuracy =   0.618, test_acc =   0.507 time:   8.613\n",
      "Epoch: 596: loss =   1.062 , accuracy =   0.620, test_acc =   0.506 time:   8.770\n",
      "Epoch: 597: loss =   1.054 , accuracy =   0.618, test_acc =   0.504 time:   8.730\n",
      "Epoch: 598: loss =   1.064 , accuracy =   0.621, test_acc =   0.503 time:   8.725\n",
      "Epoch: 599: loss =   1.047 , accuracy =   0.615, test_acc =   0.510 time:   8.616\n",
      "Epoch: 600: loss =   1.061 , accuracy =   0.617, test_acc =   0.506 time:   8.632\n",
      "Epoch: 601: loss =   1.049 , accuracy =   0.627, test_acc =   0.510 time:   8.684\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 602: loss =   1.050 , accuracy =   0.622, test_acc =   0.509 time:   8.686\n",
      "Epoch: 603: loss =   1.049 , accuracy =   0.623, test_acc =   0.512 time:   8.770\n",
      "Epoch: 604: loss =   1.045 , accuracy =   0.625, test_acc =   0.511 time:   8.654\n",
      "Epoch: 605: loss =   1.027 , accuracy =   0.630, test_acc =   0.510 time:   8.666\n",
      "Epoch: 606: loss =   1.031 , accuracy =   0.627, test_acc =   0.508 time:   8.729\n",
      "Epoch: 607: loss =   1.042 , accuracy =   0.629, test_acc =   0.510 time:   8.617\n",
      "Epoch: 608: loss =   1.034 , accuracy =   0.629, test_acc =   0.508 time:   8.661\n",
      "Epoch: 609: loss =   1.035 , accuracy =   0.627, test_acc =   0.508 time:   8.690\n",
      "Epoch: 610: loss =   1.027 , accuracy =   0.627, test_acc =   0.501 time:   8.674\n",
      "Epoch: 611: loss =   1.035 , accuracy =   0.625, test_acc =   0.507 time:   8.690\n",
      "Epoch: 612: loss =   1.046 , accuracy =   0.627, test_acc =   0.504 time:   8.603\n",
      "Epoch: 613: loss =   1.031 , accuracy =   0.628, test_acc =   0.502 time:   8.700\n",
      "Epoch: 614: loss =   1.026 , accuracy =   0.628, test_acc =   0.501 time:   8.693\n",
      "Epoch: 615: loss =   1.027 , accuracy =   0.627, test_acc =   0.503 time:   8.682\n",
      "Epoch: 616: loss =   1.045 , accuracy =   0.623, test_acc =   0.508 time:   8.676\n",
      "Epoch: 617: loss =   1.047 , accuracy =   0.626, test_acc =   0.506 time:   8.673\n",
      "Epoch: 618: loss =   1.043 , accuracy =   0.621, test_acc =   0.500 time:   8.661\n",
      "Epoch: 619: loss =   1.037 , accuracy =   0.629, test_acc =   0.506 time:   8.683\n",
      "Epoch: 620: loss =   1.037 , accuracy =   0.628, test_acc =   0.502 time:   8.595\n",
      "Epoch: 621: loss =   1.031 , accuracy =   0.632, test_acc =   0.501 time:   8.695\n",
      "Epoch: 622: loss =   1.025 , accuracy =   0.633, test_acc =   0.514 time:   8.735\n",
      "Epoch: 623: loss =   1.011 , accuracy =   0.628, test_acc =   0.499 time:   8.611\n",
      "Epoch: 624: loss =   1.020 , accuracy =   0.626, test_acc =   0.503 time:   8.692\n",
      "Epoch: 625: loss =   1.020 , accuracy =   0.638, test_acc =   0.497 time:   8.614\n",
      "Epoch: 626: loss =   1.019 , accuracy =   0.637, test_acc =   0.502 time:   8.661\n",
      "Epoch: 627: loss =   1.008 , accuracy =   0.639, test_acc =   0.508 time:   8.678\n",
      "Epoch: 628: loss =   1.002 , accuracy =   0.635, test_acc =   0.504 time:   8.762\n",
      "Epoch: 629: loss =   1.013 , accuracy =   0.640, test_acc =   0.508 time:   8.636\n",
      "Epoch: 630: loss =   1.000 , accuracy =   0.642, test_acc =   0.508 time:   8.675\n",
      "Epoch: 631: loss =   1.007 , accuracy =   0.637, test_acc =   0.503 time:   8.632\n",
      "Epoch: 632: loss =   1.005 , accuracy =   0.643, test_acc =   0.505 time:   8.654\n",
      "Epoch: 633: loss =   0.988 , accuracy =   0.641, test_acc =   0.501 time:   8.688\n",
      "Epoch: 634: loss =   0.998 , accuracy =   0.636, test_acc =   0.501 time:   8.709\n",
      "Epoch: 635: loss =   0.996 , accuracy =   0.635, test_acc =   0.502 time:   8.709\n",
      "Epoch: 636: loss =   0.991 , accuracy =   0.640, test_acc =   0.502 time:   8.757\n",
      "Epoch: 637: loss =   1.002 , accuracy =   0.641, test_acc =   0.506 time:   9.028\n",
      "Epoch: 638: loss =   0.998 , accuracy =   0.646, test_acc =   0.500 time:   8.639\n",
      "Epoch: 639: loss =   0.979 , accuracy =   0.643, test_acc =   0.500 time:   8.771\n",
      "Epoch: 640: loss =   0.982 , accuracy =   0.642, test_acc =   0.502 time:   8.647\n",
      "Epoch: 641: loss =   1.002 , accuracy =   0.649, test_acc =   0.503 time:   8.730\n",
      "Epoch: 642: loss =   0.976 , accuracy =   0.641, test_acc =   0.507 time:   8.661\n",
      "Epoch: 643: loss =   0.981 , accuracy =   0.646, test_acc =   0.499 time:   8.677\n",
      "Epoch: 644: loss =   0.985 , accuracy =   0.641, test_acc =   0.498 time:   8.679\n",
      "Epoch: 645: loss =   0.985 , accuracy =   0.651, test_acc =   0.506 time:   8.619\n",
      "Epoch: 646: loss =   0.978 , accuracy =   0.637, test_acc =   0.492 time:   8.724\n",
      "Epoch: 647: loss =   0.996 , accuracy =   0.624, test_acc =   0.495 time:   8.723\n",
      "Epoch: 648: loss =   1.042 , accuracy =   0.614, test_acc =   0.476 time:   8.706\n",
      "Epoch: 649: loss =   1.072 , accuracy =   0.627, test_acc =   0.494 time:   8.750\n",
      "Epoch: 650: loss =   1.053 , accuracy =   0.630, test_acc =   0.499 time:   8.659\n",
      "Epoch: 651: loss =   1.019 , accuracy =   0.634, test_acc =   0.492 time:   8.688\n",
      "Epoch: 652: loss =   1.018 , accuracy =   0.636, test_acc =   0.498 time:   8.626\n",
      "Epoch: 653: loss =   1.012 , accuracy =   0.631, test_acc =   0.494 time:   8.606\n",
      "Epoch: 654: loss =   1.030 , accuracy =   0.628, test_acc =   0.489 time:   8.644\n",
      "Epoch: 655: loss =   1.013 , accuracy =   0.640, test_acc =   0.498 time:   8.662\n",
      "Epoch: 656: loss =   0.998 , accuracy =   0.634, test_acc =   0.496 time:   8.602\n",
      "Epoch: 657: loss =   1.008 , accuracy =   0.651, test_acc =   0.496 time:   8.669\n",
      "Epoch: 658: loss =   0.972 , accuracy =   0.642, test_acc =   0.490 time:   8.649\n",
      "Epoch: 659: loss =   0.989 , accuracy =   0.648, test_acc =   0.492 time:   8.686\n",
      "Epoch: 660: loss =   0.992 , accuracy =   0.647, test_acc =   0.500 time:   8.706\n",
      "Epoch: 661: loss =   0.989 , accuracy =   0.648, test_acc =   0.492 time:   8.695\n",
      "Epoch: 662: loss =   0.988 , accuracy =   0.654, test_acc =   0.497 time:   8.687\n",
      "Epoch: 663: loss =   0.968 , accuracy =   0.650, test_acc =   0.494 time:   8.685\n",
      "Epoch: 664: loss =   0.969 , accuracy =   0.649, test_acc =   0.500 time:   8.640\n",
      "Epoch: 665: loss =   0.978 , accuracy =   0.657, test_acc =   0.499 time:   8.784\n",
      "Epoch: 666: loss =   0.958 , accuracy =   0.651, test_acc =   0.499 time:   8.717\n",
      "Epoch: 667: loss =   0.976 , accuracy =   0.663, test_acc =   0.499 time:   8.718\n",
      "Epoch: 668: loss =   0.935 , accuracy =   0.652, test_acc =   0.502 time:   8.713\n",
      "Epoch: 669: loss =   0.958 , accuracy =   0.658, test_acc =   0.499 time:   8.712\n",
      "Epoch: 670: loss =   0.947 , accuracy =   0.660, test_acc =   0.499 time:   8.692\n",
      "Epoch: 671: loss =   0.955 , accuracy =   0.656, test_acc =   0.504 time:   8.713\n",
      "Epoch: 672: loss =   0.941 , accuracy =   0.663, test_acc =   0.498 time:   8.706\n",
      "Epoch: 673: loss =   0.931 , accuracy =   0.656, test_acc =   0.499 time:   8.657\n",
      "Epoch: 674: loss =   0.934 , accuracy =   0.659, test_acc =   0.493 time:   8.753\n",
      "Epoch: 675: loss =   0.933 , accuracy =   0.662, test_acc =   0.499 time:   8.790\n",
      "Epoch: 676: loss =   0.936 , accuracy =   0.663, test_acc =   0.497 time:   8.690\n",
      "Epoch: 677: loss =   0.927 , accuracy =   0.651, test_acc =   0.496 time:   8.709\n",
      "Epoch: 678: loss =   0.934 , accuracy =   0.651, test_acc =   0.492 time:   8.709\n",
      "Epoch: 679: loss =   0.935 , accuracy =   0.657, test_acc =   0.490 time:   8.731\n",
      "Epoch: 680: loss =   0.948 , accuracy =   0.665, test_acc =   0.499 time:   8.691\n",
      "Epoch: 681: loss =   0.940 , accuracy =   0.666, test_acc =   0.491 time:   8.692\n",
      "Epoch: 682: loss =   0.943 , accuracy =   0.659, test_acc =   0.491 time:   8.671\n",
      "Epoch: 683: loss =   0.939 , accuracy =   0.656, test_acc =   0.495 time:   8.645\n",
      "Epoch: 684: loss =   0.946 , accuracy =   0.662, test_acc =   0.494 time:   8.769\n",
      "Epoch: 685: loss =   0.936 , accuracy =   0.669, test_acc =   0.501 time:   8.746\n",
      "Epoch: 686: loss =   0.917 , accuracy =   0.670, test_acc =   0.498 time:   8.697\n",
      "Epoch: 687: loss =   0.911 , accuracy =   0.671, test_acc =   0.489 time:   8.987\n",
      "Epoch: 688: loss =   0.916 , accuracy =   0.673, test_acc =   0.497 time:   8.702\n",
      "Epoch: 689: loss =   0.913 , accuracy =   0.667, test_acc =   0.494 time:   8.741\n",
      "Epoch: 690: loss =   0.909 , accuracy =   0.667, test_acc =   0.493 time:   8.667\n",
      "Epoch: 691: loss =   0.918 , accuracy =   0.672, test_acc =   0.493 time:   8.718\n",
      "Epoch: 692: loss =   0.925 , accuracy =   0.663, test_acc =   0.493 time:   8.741\n",
      "Epoch: 693: loss =   0.916 , accuracy =   0.672, test_acc =   0.489 time:   8.682\n",
      "Epoch: 694: loss =   0.931 , accuracy =   0.673, test_acc =   0.497 time:   8.698\n",
      "Epoch: 695: loss =   0.894 , accuracy =   0.675, test_acc =   0.494 time:   8.757\n",
      "Epoch: 696: loss =   0.915 , accuracy =   0.669, test_acc =   0.492 time:   8.700\n",
      "Epoch: 697: loss =   0.915 , accuracy =   0.674, test_acc =   0.491 time:   8.796\n",
      "Epoch: 698: loss =   0.902 , accuracy =   0.672, test_acc =   0.496 time:   8.726\n",
      "Epoch: 699: loss =   0.911 , accuracy =   0.672, test_acc =   0.492 time:   8.639\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "in our train step we can see that it lasts more tha na normal CNN to converge\n",
    "on the other side, we can have the confidence interval for our predictions, which are \n",
    "wonderful in terms of taking sensitive predictions\n",
    "\"\"\"\n",
    "times = []\n",
    "for i in range(700):\n",
    "    tic = time.time()\n",
    "    loss = train_step(X_train, y_train)\n",
    "    preds = bcnn(X_train)\n",
    "    acc = accuracy(preds, y_train)\n",
    "    preds_test = bcnn(X_test)\n",
    "    test_acc = accuracy(preds_test, y_test)\n",
    "    tac = time.time()\n",
    "    train_time = tac-tic\n",
    "    times.append(train_time)\n",
    "    print(\"Epoch: {}: loss = {:7.3f} , accuracy = {:7.3f}, test_acc = {:7.3f} time: {:7.3f}\".format(i, loss, acc, test_acc, train_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bcnn.save_weights(\"bcnn_cifar10.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### About the performance:\n",
    "\n",
    "mean = np.mean(times)\n",
    "std = np.std(times)\n",
    "print(\"In TensorFlow 2.0.0b1 our train time mean was : {:7.3f}, with std : {:7.3f}\".format(mean, std))\n",
    "\n",
    "no_outlier = times[1:]\n",
    "no_mean = np.mean(no_outlier)\n",
    "no_std = np.std(no_outlier)\n",
    "print(\"\\nHowever, by removing the outlier 1st time, our train time mean was : {:7.3f}, with std : {:7.3f}\".format(no_mean, no_std))\n",
    "#print(\"\\nWe conclude TensorFlow 2 has a longer time to start its variables, but then does it faster than TF1.14 Intel Optimzied (see other notebook)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we will illustrate our predictions and confidence intervals\n",
    "\n",
    "Those illustrative functions were taken from https://github.com/zhulingchen/tfp-tutorial/ repo, which had the tutorial (in Keras) that did let me learn how to \n",
    "\n",
    "### Here we have some statistics on recognizable and unrecognizable images from MNIST:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_mc_run = 100\n",
    "med_prob_thres = 0.20\n",
    "\n",
    "y_pred_logits_list = [bcnn(X_test) for _ in range(n_mc_run)]  # a list of predicted logits\n",
    "y_pred_prob_all = np.concatenate([softmax(y, axis=-1)[:, :, np.newaxis] for y in y_pred_logits_list], axis=-1)\n",
    "y_pred = [[int(np.median(y) >= med_prob_thres) for y in y_pred_prob] for y_pred_prob in y_pred_prob_all]\n",
    "y_pred = np.array(y_pred)\n",
    "\n",
    "idx_valid = [any(y) for y in y_pred]\n",
    "print('Number of recognizable samples:', sum(idx_valid))\n",
    "\n",
    "idx_invalid = [not any(y) for y in y_pred]\n",
    "print('Unrecognizable samples:', np.where(idx_invalid)[0])\n",
    "\n",
    "print('Test accuracy on MNIST (recognizable samples):',\n",
    "      sum(np.equal(np.argmax(y_test[idx_valid], axis=-1), np.argmax(y_pred[idx_valid], axis=-1))) / len(y_test[idx_valid]))\n",
    "\n",
    "print('Test accuracy on MNIST (unrecognizable samples):',\n",
    "      sum(np.equal(np.argmax(y_test[idx_invalid], axis=-1), np.argmax(y_pred[idx_invalid], axis=-1))) / len(y_test[idx_invalid]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With this other snippet, we can plot the predict distribution of our network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pred_hist(y_pred, n_class, n_mc_run, n_bins=30, med_prob_thres=0.2, n_subplot_rows=2, figsize=(25, 10)):\n",
    "    bins = np.logspace(-n_bins, 0, n_bins+1)\n",
    "    fig, ax = plt.subplots(n_subplot_rows, n_class // n_subplot_rows + 1, figsize=figsize)\n",
    "    for i in range(n_subplot_rows):\n",
    "        for j in range(n_class // n_subplot_rows + 1):\n",
    "            idx = i * (n_class // n_subplot_rows + 1) + j\n",
    "            if idx < n_class:\n",
    "                ax[i, j].hist(y_pred[idx], bins)\n",
    "                ax[i, j].set_xscale('log')\n",
    "                ax[i, j].set_ylim([0, n_mc_run])\n",
    "                ax[i, j].title.set_text(\"{} (median prob: {:.2f}) ({})\".format(str(idx),\n",
    "                                                                               np.median(y_pred[idx]),\n",
    "                                                                               str(np.median(y_pred[idx]) >= med_prob_thres)))\n",
    "            else:\n",
    "                ax[i, j].axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This example is not recognizable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in np.where(idx_invalid)[0]:\n",
    "    plt.imshow(X_test[idx, :, :, :], cmap='gist_gray')\n",
    "    print(\"True label of the test sample {}: {}\".format(idx, np.argmax(y_test[idx], axis=-1)))\n",
    "\n",
    "    plot_pred_hist(y_pred_prob_all[idx], class_nmr, n_mc_run, med_prob_thres=med_prob_thres)\n",
    "\n",
    "    if any(y_pred[idx]):\n",
    "        print(\"Predicted label of the test sample {}: {}\".format(idx, np.argmax(y_pred[idx], axis=-1)))\n",
    "    else:\n",
    "        print(\"I don't know!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A recognizable one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X_test[0, :, :, :], cmap='gist_gray')\n",
    "print(\"True label of the test sample {}: {}\".format(0, np.argmax(y_test[0], axis=-1)))\n",
    "\n",
    "plot_pred_hist(y_pred_prob_all[0], class_nmr, n_mc_run, med_prob_thres=med_prob_thres)\n",
    "\n",
    "if any(y_pred[0]):\n",
    "    print(\"Predicted label of the test sample {}: {}\".format(idx, np.argmax(y_pred[0], axis=-1)))\n",
    "else:\n",
    "    print(\"I don't know!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_prob",
   "language": "python",
   "name": "tf_prob"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
