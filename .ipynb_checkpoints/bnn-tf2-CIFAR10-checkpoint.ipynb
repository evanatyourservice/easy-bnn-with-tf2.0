{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Neural Networks in Keras and TensorFlow 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! . activate base\n",
    "#!pip install tensorflow==2.0.0b1 --user\n",
    "#!pip install tfp_nightly --user\n",
    "#!python -m ipykernel install --name tf_prob --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/u30073/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/u30073/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/u30073/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/u30073/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/u30073/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/u30073/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/glob/intel-python/python3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/home/u30073/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/u30073/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/u30073/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/u30073/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/u30073/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/u30073/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "#TODO: PUT RANDOM SEED FOR RESULT SECURING IN DEMOS\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import cv2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version (expected = 2.0.0-beta1): 2.0.0-beta1\n",
      "TensorFlow Probability version (expected = 0.9.0-dev20190912): 0.9.0-dev20190913\n"
     ]
    }
   ],
   "source": [
    "print('TensorFlow version (expected = 2.0.0-beta1):', tf.__version__)\n",
    "print('TensorFlow Probability version (expected = 0.9.0-dev20190912):', tfp.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X, theta = 1.0, axis = None):\n",
    "    \"\"\"\n",
    "    Compute the softmax of each element along an axis of X.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X: ND-Array. Probably should be floats.\n",
    "    theta (optional): float parameter, used as a multiplier\n",
    "        prior to exponentiation. Default = 1.0\n",
    "    axis (optional): axis to compute values along. Default is the\n",
    "        first non-singleton axis.\n",
    "\n",
    "    Returns an array the same size as X. The result will sum to 1\n",
    "    along the specified axis.\n",
    "    \"\"\"\n",
    "\n",
    "    # make X at least 2d\n",
    "    y = np.atleast_2d(X)\n",
    "\n",
    "    # find axis\n",
    "    if axis is None:\n",
    "        axis = next(j[0] for j in enumerate(y.shape) if j[1] > 1)\n",
    "\n",
    "    # multiply y against the theta parameter,\n",
    "    y = y * float(theta)\n",
    "\n",
    "    # subtract the max for numerical stability\n",
    "    y = y - np.expand_dims(np.max(y, axis = axis), axis)\n",
    "\n",
    "    # exponentiate y\n",
    "    y = np.exp(y)\n",
    "\n",
    "    # take the sum along the specified axis\n",
    "    ax_sum = np.expand_dims(np.sum(y, axis = axis), axis)\n",
    "\n",
    "    # finally: divide elementwise\n",
    "    p = y / ax_sum\n",
    "\n",
    "    # flatten if X was 1D\n",
    "    if len(X.shape) == 1: p = p.flatten()\n",
    "\n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We are all set up, lets go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are using CIFAR10 dataset!\n",
      "X_train.shape = (50000, 32, 32, 3)\n",
      "y_train.shape = (50000, 10)\n",
      "X_test.shape = (10000, 32, 32, 3)\n",
      "y_test.shape = (10000, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7ff3eac254e0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4xLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvAOZPmwAAH5FJREFUeJztnWmQXNd13/+nt9kX7Bgs4gAguEDc\nQCIUJSqyLDsuWqUKqYrsSB9UrEQWFJWVsqqcDyylKlKq8kFORVLpQ0ouKGSZThRJtCkVKZu2uYgk\nSMahAEEgCIok1gFmMIOZAWZfunu6++TDNB1weP93GhhMD6j3/1Wh0HNP3/fuu++d97rvv8855u4Q\nQiSP1EoPQAixMsj5hUgocn4hEoqcX4iEIucXIqHI+YVIKHJ+IRKKnF+IhCLnFyKhZJbS2czuA/Bd\nAGkA/8Pdvxl7f+fqNb5py9awMfJLQ2oxq2WYl0X0947MGBl7vjBDbTPTU5Fd8W3OFQq8X6USbCfN\ni2IWOS8e22j43MwVS5HtRbaW4uc6Zss1NIX7WJr2mSvx+W3INlJbucKPLZPmrtbSEN7m5HSe9pma\nCdvKczOolIs1OcYVO7/Nz95/B/AvAPQBOGBmT7r7r1mfTVu24n/97JmgrVIu032VK+GropKJDN+u\n7ENNJXYTqoTntFLiJ+nEsdeo7cAvXub7cj4f53tOUltxOnyzmZrhjhpzhHSaO8Jcmd/YzMPnpr/v\nIu1TKvExZhtz3NbcQG3brr812J5KtdA+QxfOUNt1W26ktsnJQWpbs2ojtd29/aZg+4v/9yjt8/Kh\nt4PtI338mlrIUj723w3ghLufcvcigB8BuH8J2xNC1JGlOP9mAL2X/N1XbRNCvA9YivOHPgO/5zOz\nme01s4NmdnB0hH/kE0LUl6U4fx+AS1fvtgDoX/gmd9/n7nvcfc+q1WuWsDshxNVkKc5/AMBOM9tm\nZjkAnwXw5NUZlhBiubni1X53L5nZVwD8A+alvkfc/Y1F+mCuzKQovsp+7PixYPt0RCpbu24ttbW3\nt1Nbf/8AtU3ni8H2/OwE7XP40H5qGx15zwelfyLjXFIqTkbkw4mxYPvE9BztY+Ar6R3tfCU9k+L9\nJsZng+2VIl/Rz89y1aQ4x9WP5shlPDI8HGzPZPi1MzM5Tm3jIxeobXJilNpacp3Udux4WL0Zm5yk\nfbKN4WO+HPV7STq/uz8F4KmlbEMIsTLoF35CJBQ5vxAJRc4vREKR8wuRUOT8QiSUJa32Xwnu4cin\ngQEue/3i1f8TbL8wzGW5tWu51JdK8XvehQthaQgAZoth+SpPIqwAYHKES0qrO/gYB0e4bJQqcT0n\nbeGAldGJ87RPPs9/ednQ0kVt05PT1HZhKCx/ZiKBU2YRGXAmPPcAUCrzbWYy2WA7C9ICgEwDlxWL\nM1x+Kxe4nDo1wefqyPGzwfbRYiQSkMjVqTQP0nrPe2t+pxDiNwo5vxAJRc4vREKR8wuRUOT8QiSU\nuq72T06M48Vnw6EAx46Fg3cAoK+vN9je0MCHP1jgq7Lj4zxwI53m90Mvh5WK4fNDtE9bIw8i2tW9\nidp2bG3l4yjw4JJVbeFAnJ5zfBV4ZoYrC1u2rqI2M65WTIyF5+rlV16nfYYvcGUkX+BKgOe5ElAm\nKdZyWR6w1NTMlYC2zTupbWqUqwSD/fwamR0JB2PNpNton5Y1q4Ptdhnp6/TkFyKhyPmFSChyfiES\nipxfiIQi5xciocj5hUgo9ZX6Jsfw4rN/G7RNjPM8eE6CQYpNPIdcqcRll6kpLgM2NzVTW1s2LNsV\nx7j0tuujPGPxv/x9LvW1NYSlnPn9cdmuMUvmqsJlo0Kk/FdzM38+tDZzOXJmOhxQM9gblm0B4FRP\nH7Vlc/y8ZHL8OiiQQJxMRM7zMp/fVKSMWntbB7VNTMUCmsL7m4tcw5OTpFxXJMhpIXryC5FQ5PxC\nJBQ5vxAJRc4vREKR8wuRUOT8QiSUJUl9ZtYDYBJAGUDJ3ffE3l8uVTA2Fo7cypNSWABQKoUjxCpk\nW1UrtczkI+WupnmE2CjJ4WdFLq/cfD2P6hvofYvaDg7wnG+xslCzM+GxjE/wMU4U+DzeeiO/RDor\n4Wg0ACgVwlLallVhCRAAHBFbLFrNudTnHr6uZub49VGc5vs6dY7nmmxf1U1tSIevYQDIdmwMtmci\n5bqKs+HIVK/w/bxn+zW/k/Pb7s6FbiHENYk+9guRUJbq/A7gaTP7pZntvRoDEkLUh6V+7L/X3fvN\nbD2AZ8zsLXd/V03q6k1hLwDkGnnZaSFEfVnSk9/d+6v/DwH4KYC7A+/Z5+573H1PNssXdIQQ9eWK\nnd/MWsys7Z3XAH4PwNGrNTAhxPKylI/9GwD81Mze2c7/dve/j3UolcsYGQnLF9XtBJkrhssgTY3x\nRJwNOX5oxQqPYpudjcmA4VJenZF9nTjJoxWfeOIctZ0ZbqK2TAuPOmsikXalIu8zazxirpzjEmxm\n6Di1FabC8mHX+g20T0MjjzxMZ/gct7fwyMm1G7YH25vaeNRkQ+TraWOkGlZPH0/SOXyRl/JKNYWT\npHZ18qhJdp6nB2r/dH3Fzu/upwDcfqX9hRAri6Q+IRKKnF+IhCLnFyKhyPmFSChyfiESSl0TeJbn\nyhgdvhi0pVL8PlQshqW5Qp5LdjPGo9iamyNRYOBRUfliWFqcNi7L9Q3zmnAjeS435cv82NrTPCKt\nvS0s9WQjt/npPB9jW0TG9GwLtY2SxJn5PE9K2dTEJbZcjs9xY8S2cePWYHtMVty+JRxlBwAdGS7Z\nteE0tT1zMXzdA8D0ZPi6aszzqElf0xVsr5T5+BaiJ78QCUXOL0RCkfMLkVDk/EIkFDm/EAmlrqv9\nlXIJU6PhFUxWkguYDwgK9knxYKBUZLU/A77inEnzyI3yTHi6SuDBL4UiDxRq4HEbWJ3iOfxu2LGN\ndySHlovkwJvJ8ixs69vDZaEAYHaCqw4jREBYt5YfdEOaB2oVZ/m+chH1pjEbVhCKs5FAmxLf17bN\n66gNBb7NV05y9WbuQjgv4IVhHjhVJKW85kpa7RdCLIKcX4iEIucXIqHI+YVIKHJ+IRKKnF+IhFJX\nqc/dUSZySCqSwy+FsGznGS7L5SI2j8ghpQKXAdc3dgTbN23kctgt3VziaVrFj7mlkUuVq9p58NHk\nRDjIpTzHT/XGrrXUtut6fmynDnGJ8M7d4Vx9o8P8edPUOEJtzRFdtI3kswP4tdPSzIOB8gUu3XZt\n7aa2gVFe6m3D2nCePgBYV+4Ntg/mueTY2BTWUmN+9J731vxOIcRvFHJ+IRKKnF+IhCLnFyKhyPmF\nSChyfiESyqJSn5k9AuBTAIbc/ZZq22oAPwbQDaAHwB+6++ii23IgTRSsmECRzYajtjKtvMzUqg6e\no21NC+83PsTlq927NgfbP3M/j/TadQPP07duI5eb0uBS5VBELjt6IlwyanKWS447NvGouO71/Mw0\n7+QS4fhMWIrqdV5OavdtndRWLvF8geZ8jI0NYbmsErngihHbobdPUdtTLx2itslRLh9+ZGP4XB+Y\n4nM1VgrLvU6kzRC1PPn/AsB9C9oeAvCcu+8E8Fz1byHE+4hFnd/d9wNY+OuL+wE8Wn39KIAHrvK4\nhBDLzJV+59/g7gMAUP1//dUbkhCiHiz7z3vNbC+AvQBg0W/2Qoh6cqVP/kEz6wKA6v+0MLm773P3\nPe6+R84vxLXDlTr/kwAerL5+EMATV2c4Qoh6UYvU90MAHwew1sz6AHwdwDcBPGZmXwBwFsAf1LKz\n1rY2fPjujwdtTU1c9mpvD8t2LZ1cssukuXzVmeM2khcRALC+JfwBJzX7Ju0z+BYvuTR4ikfnzeS5\nNHTbbp7A87fvDUceHnyNH9iRo2epbXNHODoPALJciUI2HR5/upmXBquUubw5OcMTmqLC+00VwnNs\nFT4fA8Nc7j19/G1qO39ukNrKFS7BDabDkZPlVPhcAoCBXcO1P88XdX53/xwx/U7NexFCXHPoF35C\nJBQ5vxAJRc4vREKR8wuRUOT8QiSUuibwbGpuxi233x60lcpc9qowW6QeX2mOyz8Xp7hsZM08eiw7\nEa67N9NzmvbpiEiOpXAZOQBAJmLbcT23begK/9J64gJPjvnWsT5q++e3cLlp3Ro+V00d4fk/NsTP\nc3GCzxXSvJ+nuGxX9vA4chFJt1CepLYSiaYDgNYsj5w80xdO0gkAh8bD87j77o/QPtnsmmD7SxeP\n0T4L0ZNfiIQi5xciocj5hUgocn4hEoqcX4iEIucXIqHUVeorlYoYuRCWlWKJB4vFsLxSMT78dCxD\nY5nX6stWJqito2k82L5hVTvts30Nl6+aO7mtoYVLlW1ZfmyFiXDk4Z238hC8u27/Z9TmQ/3Ulmrj\n4881hxNnrmriz5sPrOPHjDaeCNXTvKbdbDFss0iWztbcJmpLRa6rYePjPzd8ke+vPZy4tINEswLA\nzCyL+uRzsRA9+YVIKHJ+IRKKnF+IhCLnFyKhyPmFSCh1Xe1vbWnChz/0waCtXOarlGbhe1Rj5NZV\nOH+G2hpaebBKWztfsc2PzAbbM3ke4NK1iasYqzt59E4hkkywPM5XnMenwivws7ORklZZni9w8gwP\nculYfR215RrD27xhRzg4CgA+88AN1DY4tYrazg2GVRgAmJgMj6Oxgc993yy3DV/galA6x6/hpiZ+\n3CUiPg328z7FctgWCzxaiJ78QiQUOb8QCUXOL0RCkfMLkVDk/EIkFDm/EAmllnJdjwD4FIAhd7+l\n2vYNAF8EMFx929fc/anFtpXLZdD9gXDusXw+XLIIANLpsPzWNseDJUZ6eS6ztG+ltkKk5FJhNJyH\nrX0tv4c2EskLAEpFLudNj3OJsKef5+MbvRA+pW1rb6N90mkuKY318cCefAsv5TV1dirYfustXHLM\nZn9NbUcP80v13GAklyOZ/vZOXh7ujaHwNQoAh984SW2I5IYsFfk1Yh6+DgrT52ifXGv4wOZIEFyI\nWp78fwHgvkD7d9z9juq/RR1fCHFtsajzu/t+APxRI4R4X7KU7/xfMbMjZvaImfGfXwkhrkmu1Pm/\nB2AHgDsADAD4Fnujme01s4NmdnB8nP9UVAhRX67I+d190N3L7l4B8H0Ad0feu8/d97j7no4OnplE\nCFFfrsj5zazrkj8/DeDo1RmOEKJe1CL1/RDAxwGsNbM+AF8H8HEzuwOAA+gB8KWa9lYpo1wIR0Vl\nIjn84GF5qFgM56sDgNlRLsm0dXCJauA07zc18HawvfGGzbRPuZvnzitWeDRgfoZLYk3N3DaWIV+t\njMuKqz/QSm0bN3FZ9NhQWM4DgJ+9EC5h9uV/u532OdF7ltqGh7qoLR+R0QpTY8H2XAPPP3hhhG9v\ndJLLoukCn49IRTE0pMPX/vQ0316JPLcrldpz+C3q/O7+uUDzwzXvQQhxTaJf+AmRUOT8QiQUOb8Q\nCUXOL0RCkfMLkVDqmsDTUkBTM7nfGJevnNjaOsJljgCg804uKY1O8+irrd28LNR0Q/hXzA0tfOwl\ncElpYozLRvkyTyK5dedGattyc3gs5waHg+0AsOkmXp5qw+puaut5mod8nDlTCLYfOxYp17VzN7Xt\nuuN3qe10/wC1vfzCT4LtrZ18HB9sWkdteefnbHaER9Rd372D2vp7w7JoQ6Qs284bwslOf/5zLpcu\nRE9+IRKKnF+IhCLnFyKhyPmFSChyfiESipxfiIRSV6kP5qhYuDCZO4/qcxapZOHaeQCwaiOvuddy\n8RS1tW7jiR3zU+FowJlIfbTmNi71NUdm33I8GrBlNZ+r1vaGYHvbRh7tlWuOJFlJ8/HnsqTIHIAs\nmf6JWd5nNRk7AGztilwfXH3DSaLc5iK5JW66i0uO19+4jdqe/bunqe2P/s1nqe35558Jtp/r57Ld\nv/+TLwbbj77xAu2zED35hUgocn4hEoqcX4iEIucXIqHI+YVIKHVd7feKo1gIB3xYiq/OV+bCK8Rj\nI+dpn1Ke5+Lb0B7OIwgA69by1f7chvC9smx8RbwIrgSkPbKSnuJlvpDmK+bIhue3tZMHLMEied/K\nfJW90SKqQyactM7KPFilwfg1cOE0L79WHOaqTxbh8zk6wvvMprgadNc9H6O2/c8+S23Dw/xa/fCH\nw8mvv/fnB2mfuUL4+vBKJBfmAvTkFyKhyPmFSChyfiESipxfiIQi5xciocj5hUgotZTr2grgLwFs\nBFABsM/dv2tmqwH8GEA35kt2/aG7jy6yLeQy4eCNSFwPUtnwMDONvDL4xBS3jZ/ntZPGss3U1tYY\nlu0qxoNwUq1cYis7n/78cC+1zddHDTM2E57ImXye95nmsuLsLN9XQ66d2rq6w0FQb/XyIKJ1J7kE\ne76nh9pmJrj02TsaLteV6uBjP9pzgNquv/0uarvlg7uobf/+F6jtS//uj4Lta9bwHJXPPh0OBpqY\n4HO4kFqe/CUAf+ruNwO4B8Afm9kuAA8BeM7ddwJ4rvq3EOJ9wqLO7+4D7n6o+noSwJsANgO4H8Cj\n1bc9CuCB5RqkEOLqc1nf+c2sG8BuAK8C2ODuA8D8DQLA+qs9OCHE8lGz85tZK4DHAXzV3Wv+YmFm\ne83soJkdHB/n+fKFEPWlJuc3syzmHf8H7v5OFYRBM+uq2rsADIX6uvs+d9/j7ns6Ong9eiFEfVnU\n+c3MADwM4E13//YlpicBPFh9/SCAJ67+8IQQy0UtUX33Avg8gNfN7HC17WsAvgngMTP7AoCzAP5g\n8U0ZUpXw/SabjchlJOIvtWYL7TM2dwe1Pf7Yz6lt8GI/tW1dHx77B7bwclf3/NYeavMGHkH408dP\nUNvYyEVqY1W5xsd5xNz4DJc+LcMjDx/4V7dSW8umcMmrn/yMR6q9cmSQ2gpTXKpc3cbz8d155y3B\n9ht33UT7HPirl6ntVwf4+G+7LbwvAHjlpZeobXAwfM3dFckl+Mr+8Pamp6don4Us6vzu/jIAFof5\nOzXvSQhxTaFf+AmRUOT8QiQUOb8QCUXOL0RCkfMLkVDqmsDTYEiRXVZ4nkuUK2EpKpuN/GioqZua\nDp/mkWpH375AbRvbwnLZJ+7hJZyQDf72CQCw7rrrqG0wv53ajvXw+lSpcniuygWeOHOmzOcjlebR\nkUMj4WShAPDR3eFj275lnPa5OMnnfv2WtdS284ZuarvrQ7cH21et76B9Nm8Oy5QA8NKL+6ltkOms\nAMrOL/C/fepvgu2ZDD9nJ46/Hmwv5Hli0oXoyS9EQpHzC5FQ5PxCJBQ5vxAJRc4vREKR8wuRUOoq\n9cEdILXEKohk8LSw5DE1xaWmN97oobaZWS6VZdNcXimUwmPs6+ujfQb6zlJb63qe3HPSeRRelgcD\nojUVPqWTJS6xNaV5As+yN1Lb1Bh/dqxqDke4ffnBf037TBRHqC3dzK+PhmYeEdreHI74S+X4/N52\n683U1vcPh6jt8K+4rbGBj/HcuXCy1sHz52ifkZGwhFwiUm8IPfmFSChyfiESipxfiIQi5xciocj5\nhUgodV3tdxjK5fBqOkntBwBIZ8Irpcf7ec63Z14IBz4AwOAwz3NWjtQNmyHiwhs9PKAjE5nh1CBf\n3bYcD7aJVOvC+Xx4IotF3qlMciQCQCbN56rnDD+4Fw+EVY5t2/iJTvN0gWhrbOVGnt4PcxY+n5lK\nuGwcAOSn+DGf6XmL2rp38HJdcyVeUuzk8ZPB9oYGPr+VCjmfEdFsIXryC5FQ5PxCJBQ5vxAJRc4v\nREKR8wuRUOT8QiSURaU+M9sK4C8BbARQAbDP3b9rZt8A8EUA7+hcX3P3p+JbS8Ez4UCLfJEHJPSe\nPR9sf/XAMdrnbO8otZVjuqJxW6kSlhwnClwqQ+S4PJLXLR0JMIqokSiXwhJQJsulrfb2dmqbnhqj\ntnNDvFjzr0+Fz9nLB96mfWbzPFCrLVKSq6WZBx91toQv8ZZWfswTRT7BazesobZPfeqT1HbiBC+/\ndvZsWOpzHguEFC2iVbvWV4vOXwLwp+5+yMzaAPzSzJ6p2r7j7v+t5r0JIa4ZaqnVNwBgoPp60sze\nBLB5uQcmhFheLus7v5l1A9gN4NVq01fM7IiZPWJmPMezEOKao2bnN7NWAI8D+Kq7TwD4HoAdAO7A\n/CeDb5F+e83soJkdHJ+YvApDFkJcDWpyfjPLYt7xf+DuPwEAdx9097K7VwB8H8Ddob7uvs/d97j7\nno52vmgjhKgvizq/mRmAhwG86e7fvqS965K3fRrA0as/PCHEclHLav+9AD4P4HUzO1xt+xqAz5nZ\nHZjXFnoAfGmxDZXKjgvjYXnrlX88HGwHgOdf/Mdge/+5AdpnJpLfr1jmYWBl8Oi3tIXHXq7wiC2P\nbC8mylhEjsxEovBS2fBWsw1cciwU+NexWJmp5ohEODY9HWw/M8AjGSOBb8AwP2dc9gJy6fBxW+TK\n337jTm674Xpqq0Sug8lxLpl2doQjFmPlui4rfI9Qy2r/y0BwdhfR9IUQ1zL6hZ8QCUXOL0RCkfML\nkVDk/EIkFDm/EAml7gk8S+XwLhuaOmm/D95yZ7B96xYeuVfMz1Jb2bnN0ryUV1NjWHopl7nEE4vO\ni8k1mTSPVGts4pkucyR4r7mFR/V5ie+rEqn+lG3kYWdne8MJPFM53idWfm14mEtlFa5GokRC40pF\nfs7GJ3gCz1VreFTf4UO8XNdcgR/bju7rgu29vWdon1hkZ63oyS9EQpHzC5FQ5PxCJBQ5vxAJRc4v\nREKR8wuRUOoq9aVTKbQ2NQdtH/lQWM4DgOJd4ci4fEzOK3NpBRbRr8AloJyFI8vMY1F9XIeyFI/4\nSxmXCFOxW3YqfGwe0cMyzqU+B48gvDDOpdZGEkXY3MylvtOn+/m+vEhtManPIxF/jP4zfdQ2MniB\n76vM9beK82sulQr3KxR4JGOaXAQW02YX7rfmdwohfqOQ8wuRUOT8QiQUOb8QCUXOL0RCkfMLkVDq\nKvXBAZRJEkznskaGRMa1tvF7VzrdwscRkdFQ5rZ0kciHJS4rlspcogJJCAoASPFtpiwmKYXlw1gi\nUcQSmkako44WHl140/Xbgu2TE3w+ZiZPUxvm+KWadi5HljwcpVkh1yEAWETeLExzebnArg8ApUjk\nJ5Nu0xl+zJa6fAnzPftd8haEEO9L5PxCJBQ5vxAJRc4vREKR8wuRUBZd7TezRgD7ATRU3//X7v51\nM9sG4EcAVgM4BODz7pHoC8wv9pc8fL9JGV85NhKcEYmViOaei+KxVdRwUEosL51VIuW6IgdQAV+B\nn4sELTlIeSoSPAIAmYZYfr9IQFBkxfnk0XD5tf0vv0b7jE/wFfG5yIp+JZILsUIu8VhZtorFzllM\noeH9LKIwMRtTbubHQdppj/dSy5O/AOAT7n475stx32dm9wD4MwDfcfedAEYBfOEy9iuEWGEWdX6f\n5510ptnqPwfwCQB/XW1/FMADyzJCIcSyUNN3fjNLVyv0DgF4BsBJAGP+/z8D9QHYvDxDFEIsBzU5\nv7uX3f0OAFsA3A3g5tDbQn3NbK+ZHTSzgxMTE1c+UiHEVeWyVvvdfQzACwDuAdBp9k9VzrcACKZh\ncfd97r7H3fe0R+q5CyHqy6LOb2brzKyz+roJwO8CeBPA8wA+U33bgwCeWK5BCiGuPrUE9nQBeNTM\n0pi/WTzm7n9jZr8G8CMz+y8AfgXg4cU3ZUinmSwWCahJX8nPEa6sTJZH6iBViCkmNSEy9nSKS4Qp\ncOkzVbn8nIEWka/Kc5H8eMb3ZRFZtKN9Y7B927abaJ/e/iFqGxvl+QLnIuMvlYjUV4jIaJG8i1El\nOBI7FVHt4ttcRhZ1fnc/AmB3oP0U5r//CyHeh+gXfkIkFDm/EAlFzi9EQpHzC5FQ5PxCJBSLSVtX\nfWdmwwDOVP9cC4DXPqofGse70TjezfttHNe5+7paNlhX53/Xjs0OuvueFdm5xqFxaBz62C9EUpHz\nC5FQVtL5963gvi9F43g3Gse7+Y0dx4p95xdCrCz62C9EQlkR5zez+8zsbTM7YWYPrcQYquPoMbPX\nzeywmR2s434fMbMhMzt6SdtqM3vGzI5X/1+1QuP4hpmdq87JYTP7ZB3GsdXMnjezN83sDTP7k2p7\nXeckMo66zomZNZrZL8zsteo4/nO1fZuZvVqdjx+bRbLe1oK71/UfgDTm04BtB5AD8BqAXfUeR3Us\nPQDWrsB+PwbgTgBHL2n7rwAeqr5+CMCfrdA4vgHgP9R5ProA3Fl93QbgGIBd9Z6TyDjqOieYj0dv\nrb7OAngV8wl0HgPw2Wr7nwP48lL2sxJP/rsBnHD3Uz6f6vtHAO5fgXGsGO6+H8DIgub7MZ8IFahT\nQlQyjrrj7gPufqj6ehLzyWI2o85zEhlHXfF5lj1p7ko4/2YAvZf8vZLJPx3A02b2SzPbu0JjeIcN\n7j4AzF+EANav4Fi+YmZHql8Llv3rx6WYWTfm80e8ihWckwXjAOo8J/VImrsSzh/KW7JSksO97n4n\ngN8H8Mdm9rEVGse1xPcA7MB8jYYBAN+q147NrBXA4wC+6u4rlu01MI66z4kvIWlurayE8/cB2HrJ\n3zT553Lj7v3V/4cA/BQrm5lo0My6AKD6P89ptYy4+2D1wqsA+D7qNCdmlsW8w/3A3X9Sba77nITG\nsVJzUt33ZSfNrZWVcP4DAHZWVy5zAD4L4Ml6D8LMWsys7Z3XAH4PwNF4r2XlScwnQgVWMCHqO85W\n5dOow5zYfL2qhwG86e7fvsRU1zlh46j3nNQtaW69VjAXrGZ+EvMrqScB/McVGsN2zCsNrwF4o57j\nAPBDzH98nMP8J6EvAFgD4DkAx6v/r16hcfxPAK8DOIJ55+uqwzg+ivmPsEcAHK7++2S95yQyjrrO\nCYDbMJ8U9wjmbzT/6ZJr9hcATgD4KwANS9mPfuEnRELRL/yESChyfiESipxfiIQi5xciocj5hUgo\ncn4hEoqcX4iEIucXIqH8P1mpzvtETxLWAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff3f2172438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "first, we load the dataset. We are trying to do it first with CIFAR\n",
    "I've found this snippet somewhere in github\n",
    "\"\"\"\n",
    "\n",
    "#as we have 10 classes, I'm setting class number to 10\n",
    "class_nmr = 10\n",
    "\n",
    "print('We are using CIFAR10 dataset!')\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "#X_train = np.expand_dims(X_train, -1)\n",
    "n_train = X_train.shape[0]\n",
    "#X_test = np.expand_dims(X_test, -1)\n",
    "y_train = tf.keras.utils.to_categorical(y_train, class_nmr)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, class_nmr)\n",
    "\n",
    "# Normalize data\n",
    "X_train = X_train.astype('float32') / 255\n",
    "X_test = X_test.astype('float32') / 255\n",
    "\n",
    "print(\"X_train.shape =\", X_train.shape)\n",
    "print(\"y_train.shape =\", y_train.shape)\n",
    "print(\"X_test.shape =\", X_test.shape)\n",
    "print(\"y_test.shape =\", y_test.shape)\n",
    "\n",
    "plt.imshow(X_train[1026, :, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bayesian_bcnn_model(input_shape):\n",
    "    \n",
    "    \"\"\"\n",
    "    Here we use tf.keras.Model to use our graph as a Neural Network:\n",
    "    We select our input node as the net input, and the last node as our output (predict node).\n",
    "    Note that our model won't be compiled, as we are usign TF2.0 and will optimize it with\n",
    "    a custom @tf.function for loss and a @tf.function for train_step\n",
    "    Our input parameter is just the input shape, a tuple, for the input layer\n",
    "    \"\"\"\n",
    "    \n",
    "    model_in = tf.keras.layers.Input(shape=input_shape)\n",
    "    \n",
    "    conv_1 = tfp.python.layers.Convolution2DFlipout(32, kernel_size=(3, 3), padding=\"same\", strides=2)\n",
    "    x = conv_1(model_in)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "    \n",
    "    conv_2 = tfp.python.layers.Convolution2DFlipout(32, kernel_size=(3, 3), padding=\"same\", strides=2)\n",
    "    x = conv_2(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "    x = tf.keras.layers.MaxPool2D(pool_size=3, padding='SAME', data_format='channels_last')(x)\n",
    "    x = tf.keras.layers.Dropout(rate=0.2)(x)\n",
    "\n",
    "    \n",
    "    conv_3 = tfp.python.layers.Convolution2DFlipout(64, kernel_size=(3, 3), padding=\"same\", strides=2)\n",
    "    x = conv_3(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "    x = tf.keras.layers.MaxPool2D(pool_size=3, padding='SAME', data_format='channels_last')(x)\n",
    "    \n",
    "    conv_4 = tfp.python.layers.Convolution2DFlipout(64, kernel_size=(3, 3), padding=\"same\", strides=2)\n",
    "    x = conv_4(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "    x = tf.keras.layers.MaxPool2D(pool_size=3, padding='SAME', data_format='channels_last')(x)\n",
    "    x = tf.keras.layers.Dropout(rate=0.3)(x)\n",
    "\n",
    "    \n",
    "    conv_5 = tfp.python.layers.Convolution2DFlipout(128, kernel_size=(3, 3), padding=\"same\", strides=2)\n",
    "    x = conv_5(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "    x = tf.keras.layers.MaxPool2D(pool_size=3, padding='SAME', data_format='channels_last')(x)\n",
    "    \n",
    "    conv_6 = tfp.python.layers.Convolution2DFlipout(128, kernel_size=(3, 3), padding=\"same\", strides=2)\n",
    "    x = conv_6(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "    x = tf.keras.layers.MaxPool2D(pool_size=3, padding='SAME', data_format='channels_last')(x)\n",
    "    x = tf.keras.layers.Dropout(rate=0.4)(x)\n",
    "\n",
    "    \n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    x = tf.keras.layers.Dropout(rate=0.25)(x)\n",
    "    dense_1 = tfp.python.layers.DenseFlipout(256, activation='relu')\n",
    "    x = dense_1(x)\n",
    "    \n",
    "    x = tf.keras.layers.Dropout(rate=0.25)(x)\n",
    "    dense_2 = tfp.python.layers.DenseFlipout(10, activation=None)\n",
    "    model_out = dense_2(x)  # logits\n",
    "    model = tf.keras.Model(model_in, model_out)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "here we are just instancing our model and setting up an Optimizer\n",
    "\"\"\"\n",
    "bcnn = build_bayesian_bcnn_model(X_train.shape[1:])\n",
    "optimizer = tf.keras.optimizers.Adam(lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 32, 32, 3)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d_flipout (Conv2DFlipou (None, 16, 16, 32)        1760      \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 16, 16, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_flipout_1 (Conv2DFlip (None, 8, 8, 32)          18464     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 8, 8, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 3, 3, 32)          0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 3, 3, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_flipout_2 (Conv2DFlip (None, 2, 2, 64)          36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 2, 2, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 2, 2, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 1, 1, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_flipout_3 (Conv2DFlip (None, 1, 1, 64)          73792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 1, 1, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 1, 1, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 1, 1, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1, 1, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_flipout_4 (Conv2DFlip (None, 1, 1, 128)         147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 1, 1, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 1, 1, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 1, 1, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_flipout_5 (Conv2DFlip (None, 1, 1, 128)         295040    \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 1, 1, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 1, 1, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 1, 1, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1, 1, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_flipout (DenseFlipout) (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_flipout_1 (DenseFlipou (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 646,282\n",
      "Trainable params: 645,386\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "bcnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "this is our loss function: a sum of KL Divergence and Softmax crossentropy\n",
    "We use the @tf.function annotation becuase of TF2.0, and need no placeholders\n",
    "we get each loss and return its mean\n",
    "\"\"\"\n",
    "\n",
    "@tf.function\n",
    "def elbo_loss(labels, logits):\n",
    "    loss_en = tf.nn.softmax_cross_entropy_with_logits(labels, logits)\n",
    "    loss_kl = tf.keras.losses.KLD(labels, logits)\n",
    "    loss = tf.reduce_mean(tf.add(loss_en, loss_kl))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "this is our train step with tf2.0, very ellegant:\n",
    "We do our flow of the tensors over the model recording its gradientes\n",
    "Then, our gradient tape to give us a list of the gradients of each parameter in relation of the loss\n",
    "we dan ask our previously instanced optimizer to apply those gradients to the variable\n",
    "It is cool to see that it works even with TensorFlow probability- probabilistic layers parameters\n",
    "\"\"\"\n",
    "@tf.function\n",
    "def train_step(images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = bcnn(X_train)\n",
    "        loss = elbo_loss(labels, logits)\n",
    "    gradients = tape.gradient(loss, bcnn.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, bcnn.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "def accuracy(preds, labels):\n",
    "    return np.mean(np.argmax(preds, axis=1) == np.argmax(labels, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0: loss =  13.366 , accuracy =   0.100, test_acc =   0.102 time:  42.999\n",
      "Epoch: 1: loss =   3.528 , accuracy =   0.100, test_acc =   0.097 time:   7.330\n",
      "Epoch: 2: loss =   2.471 , accuracy =   0.101, test_acc =   0.101 time:   7.413\n",
      "Epoch: 3: loss =   2.514 , accuracy =   0.100, test_acc =   0.104 time:   7.332\n",
      "Epoch: 4: loss =   2.488 , accuracy =   0.100, test_acc =   0.103 time:   7.321\n",
      "Epoch: 5: loss =   2.412 , accuracy =   0.100, test_acc =   0.100 time:   7.330\n",
      "Epoch: 6: loss =   2.372 , accuracy =   0.098, test_acc =   0.101 time:   7.333\n",
      "Epoch: 7: loss =   2.348 , accuracy =   0.098, test_acc =   0.093 time:   7.418\n",
      "Epoch: 8: loss =   2.335 , accuracy =   0.102, test_acc =   0.107 time:   7.320\n",
      "Epoch: 9: loss =   2.330 , accuracy =   0.099, test_acc =   0.104 time:   7.382\n",
      "Epoch: 10: loss =   2.326 , accuracy =   0.101, test_acc =   0.098 time:   7.385\n",
      "Epoch: 11: loss =   2.327 , accuracy =   0.101, test_acc =   0.102 time:   7.344\n",
      "Epoch: 12: loss =   2.324 , accuracy =   0.101, test_acc =   0.103 time:   7.358\n",
      "Epoch: 13: loss =   2.320 , accuracy =   0.103, test_acc =   0.105 time:   7.325\n",
      "Epoch: 14: loss =   2.322 , accuracy =   0.101, test_acc =   0.099 time:   7.289\n",
      "Epoch: 15: loss =   2.320 , accuracy =   0.101, test_acc =   0.099 time:   7.281\n",
      "Epoch: 16: loss =   2.318 , accuracy =   0.103, test_acc =   0.097 time:   7.259\n",
      "Epoch: 17: loss =   2.315 , accuracy =   0.101, test_acc =   0.101 time:   7.280\n",
      "Epoch: 18: loss =   2.313 , accuracy =   0.098, test_acc =   0.097 time:   7.385\n",
      "Epoch: 19: loss =   2.311 , accuracy =   0.101, test_acc =   0.104 time:   7.304\n",
      "Epoch: 20: loss =   2.311 , accuracy =   0.101, test_acc =   0.097 time:   7.367\n",
      "Epoch: 21: loss =   2.309 , accuracy =   0.100, test_acc =   0.100 time:   7.382\n",
      "Epoch: 22: loss =   2.309 , accuracy =   0.101, test_acc =   0.099 time:   7.352\n",
      "Epoch: 23: loss =   2.307 , accuracy =   0.101, test_acc =   0.098 time:   7.289\n",
      "Epoch: 24: loss =   2.307 , accuracy =   0.102, test_acc =   0.102 time:   7.251\n",
      "Epoch: 25: loss =   2.307 , accuracy =   0.103, test_acc =   0.104 time:   7.349\n",
      "Epoch: 26: loss =   2.307 , accuracy =   0.105, test_acc =   0.108 time:   7.303\n",
      "Epoch: 27: loss =   2.306 , accuracy =   0.106, test_acc =   0.107 time:   7.417\n",
      "Epoch: 28: loss =   2.305 , accuracy =   0.108, test_acc =   0.109 time:   7.342\n",
      "Epoch: 29: loss =   2.304 , accuracy =   0.107, test_acc =   0.113 time:   7.276\n",
      "Epoch: 30: loss =   2.303 , accuracy =   0.113, test_acc =   0.111 time:   7.377\n",
      "Epoch: 31: loss =   2.302 , accuracy =   0.110, test_acc =   0.110 time:   7.401\n",
      "Epoch: 32: loss =   2.302 , accuracy =   0.117, test_acc =   0.119 time:   7.347\n",
      "Epoch: 33: loss =   2.298 , accuracy =   0.120, test_acc =   0.119 time:   7.259\n",
      "Epoch: 34: loss =   2.296 , accuracy =   0.125, test_acc =   0.127 time:   7.358\n",
      "Epoch: 35: loss =   2.291 , accuracy =   0.117, test_acc =   0.132 time:   7.528\n",
      "Epoch: 36: loss =   2.289 , accuracy =   0.134, test_acc =   0.135 time:   7.283\n",
      "Epoch: 37: loss =   2.283 , accuracy =   0.130, test_acc =   0.143 time:   7.476\n",
      "Epoch: 38: loss =   2.279 , accuracy =   0.137, test_acc =   0.135 time:   7.289\n",
      "Epoch: 39: loss =   2.265 , accuracy =   0.151, test_acc =   0.147 time:   7.327\n",
      "Epoch: 40: loss =   2.246 , accuracy =   0.149, test_acc =   0.149 time:   7.454\n",
      "Epoch: 41: loss =   2.253 , accuracy =   0.136, test_acc =   0.141 time:   7.408\n",
      "Epoch: 42: loss =   2.244 , accuracy =   0.137, test_acc =   0.140 time:   7.312\n",
      "Epoch: 43: loss =   2.252 , accuracy =   0.167, test_acc =   0.153 time:   7.341\n",
      "Epoch: 44: loss =   2.226 , accuracy =   0.168, test_acc =   0.165 time:   7.412\n",
      "Epoch: 45: loss =   2.216 , accuracy =   0.179, test_acc =   0.181 time:   7.384\n",
      "Epoch: 46: loss =   2.180 , accuracy =   0.168, test_acc =   0.180 time:   7.389\n",
      "Epoch: 47: loss =   2.176 , accuracy =   0.183, test_acc =   0.191 time:   7.419\n",
      "Epoch: 48: loss =   2.159 , accuracy =   0.186, test_acc =   0.175 time:   7.310\n",
      "Epoch: 49: loss =   2.141 , accuracy =   0.194, test_acc =   0.205 time:   7.321\n",
      "Epoch: 50: loss =   2.111 , accuracy =   0.191, test_acc =   0.204 time:   7.318\n",
      "Epoch: 51: loss =   2.138 , accuracy =   0.181, test_acc =   0.189 time:   7.379\n",
      "Epoch: 52: loss =   2.189 , accuracy =   0.196, test_acc =   0.199 time:   7.364\n",
      "Epoch: 53: loss =   2.102 , accuracy =   0.208, test_acc =   0.203 time:   7.341\n",
      "Epoch: 54: loss =   2.154 , accuracy =   0.204, test_acc =   0.213 time:   7.405\n",
      "Epoch: 55: loss =   2.070 , accuracy =   0.213, test_acc =   0.201 time:   7.357\n",
      "Epoch: 56: loss =   2.102 , accuracy =   0.221, test_acc =   0.204 time:   7.392\n",
      "Epoch: 57: loss =   2.071 , accuracy =   0.218, test_acc =   0.213 time:   7.317\n",
      "Epoch: 58: loss =   2.067 , accuracy =   0.225, test_acc =   0.218 time:   7.378\n",
      "Epoch: 59: loss =   2.080 , accuracy =   0.215, test_acc =   0.220 time:   7.301\n",
      "Epoch: 60: loss =   2.084 , accuracy =   0.217, test_acc =   0.230 time:   7.383\n",
      "Epoch: 61: loss =   2.077 , accuracy =   0.210, test_acc =   0.214 time:   7.348\n",
      "Epoch: 62: loss =   2.048 , accuracy =   0.222, test_acc =   0.229 time:   7.478\n",
      "Epoch: 63: loss =   2.036 , accuracy =   0.226, test_acc =   0.229 time:   7.362\n",
      "Epoch: 64: loss =   2.057 , accuracy =   0.216, test_acc =   0.224 time:   7.307\n",
      "Epoch: 65: loss =   2.014 , accuracy =   0.232, test_acc =   0.234 time:   7.318\n",
      "Epoch: 66: loss =   2.041 , accuracy =   0.225, test_acc =   0.234 time:   7.411\n",
      "Epoch: 67: loss =   2.002 , accuracy =   0.234, test_acc =   0.235 time:   7.325\n",
      "Epoch: 68: loss =   2.025 , accuracy =   0.231, test_acc =   0.241 time:   7.295\n",
      "Epoch: 69: loss =   2.017 , accuracy =   0.237, test_acc =   0.234 time:   7.334\n",
      "Epoch: 70: loss =   1.994 , accuracy =   0.242, test_acc =   0.242 time:   7.411\n",
      "Epoch: 71: loss =   2.025 , accuracy =   0.238, test_acc =   0.237 time:   7.413\n",
      "Epoch: 72: loss =   1.980 , accuracy =   0.239, test_acc =   0.238 time:   7.343\n",
      "Epoch: 73: loss =   1.993 , accuracy =   0.240, test_acc =   0.245 time:   7.413\n",
      "Epoch: 74: loss =   1.985 , accuracy =   0.244, test_acc =   0.247 time:   7.317\n",
      "Epoch: 75: loss =   1.988 , accuracy =   0.243, test_acc =   0.254 time:   7.323\n",
      "Epoch: 76: loss =   1.993 , accuracy =   0.250, test_acc =   0.249 time:   7.354\n",
      "Epoch: 77: loss =   1.960 , accuracy =   0.245, test_acc =   0.244 time:   7.362\n",
      "Epoch: 78: loss =   1.969 , accuracy =   0.248, test_acc =   0.255 time:   7.458\n",
      "Epoch: 79: loss =   1.955 , accuracy =   0.250, test_acc =   0.262 time:   7.304\n",
      "Epoch: 80: loss =   1.980 , accuracy =   0.248, test_acc =   0.255 time:   7.384\n",
      "Epoch: 81: loss =   1.976 , accuracy =   0.257, test_acc =   0.265 time:   7.402\n",
      "Epoch: 82: loss =   1.964 , accuracy =   0.262, test_acc =   0.252 time:   7.369\n",
      "Epoch: 83: loss =   1.948 , accuracy =   0.247, test_acc =   0.248 time:   7.339\n",
      "Epoch: 84: loss =   1.994 , accuracy =   0.255, test_acc =   0.265 time:   7.447\n",
      "Epoch: 85: loss =   1.937 , accuracy =   0.255, test_acc =   0.260 time:   7.340\n",
      "Epoch: 86: loss =   1.964 , accuracy =   0.261, test_acc =   0.268 time:   7.306\n",
      "Epoch: 87: loss =   1.971 , accuracy =   0.266, test_acc =   0.270 time:   7.406\n",
      "Epoch: 88: loss =   1.932 , accuracy =   0.256, test_acc =   0.240 time:   7.346\n",
      "Epoch: 89: loss =   1.961 , accuracy =   0.254, test_acc =   0.274 time:   7.314\n",
      "Epoch: 90: loss =   1.940 , accuracy =   0.273, test_acc =   0.285 time:   7.419\n",
      "Epoch: 91: loss =   1.906 , accuracy =   0.270, test_acc =   0.281 time:   7.318\n",
      "Epoch: 92: loss =   1.920 , accuracy =   0.275, test_acc =   0.280 time:   7.348\n",
      "Epoch: 93: loss =   1.901 , accuracy =   0.282, test_acc =   0.283 time:   7.348\n",
      "Epoch: 94: loss =   1.900 , accuracy =   0.275, test_acc =   0.261 time:   7.352\n",
      "Epoch: 95: loss =   1.907 , accuracy =   0.284, test_acc =   0.296 time:   7.310\n",
      "Epoch: 96: loss =   1.871 , accuracy =   0.285, test_acc =   0.298 time:   7.390\n",
      "Epoch: 97: loss =   1.886 , accuracy =   0.280, test_acc =   0.289 time:   7.320\n",
      "Epoch: 98: loss =   1.893 , accuracy =   0.281, test_acc =   0.285 time:   7.400\n",
      "Epoch: 99: loss =   1.878 , accuracy =   0.297, test_acc =   0.307 time:   7.406\n",
      "Epoch: 100: loss =   1.856 , accuracy =   0.308, test_acc =   0.306 time:   7.386\n",
      "Epoch: 101: loss =   1.856 , accuracy =   0.303, test_acc =   0.306 time:   7.441\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 102: loss =   1.862 , accuracy =   0.294, test_acc =   0.304 time:   7.295\n",
      "Epoch: 103: loss =   1.843 , accuracy =   0.299, test_acc =   0.299 time:   7.394\n",
      "Epoch: 104: loss =   1.849 , accuracy =   0.300, test_acc =   0.311 time:   7.476\n",
      "Epoch: 105: loss =   1.866 , accuracy =   0.305, test_acc =   0.316 time:   7.438\n",
      "Epoch: 106: loss =   1.832 , accuracy =   0.315, test_acc =   0.310 time:   7.349\n",
      "Epoch: 107: loss =   1.817 , accuracy =   0.314, test_acc =   0.314 time:   7.430\n",
      "Epoch: 108: loss =   1.809 , accuracy =   0.320, test_acc =   0.317 time:   7.427\n",
      "Epoch: 109: loss =   1.805 , accuracy =   0.312, test_acc =   0.318 time:   7.473\n",
      "Epoch: 110: loss =   1.829 , accuracy =   0.316, test_acc =   0.306 time:   7.336\n",
      "Epoch: 111: loss =   1.813 , accuracy =   0.324, test_acc =   0.325 time:   7.326\n",
      "Epoch: 112: loss =   1.823 , accuracy =   0.329, test_acc =   0.314 time:   7.321\n",
      "Epoch: 113: loss =   1.798 , accuracy =   0.325, test_acc =   0.323 time:   7.442\n",
      "Epoch: 114: loss =   1.821 , accuracy =   0.331, test_acc =   0.325 time:   7.521\n",
      "Epoch: 115: loss =   1.784 , accuracy =   0.328, test_acc =   0.333 time:   7.413\n",
      "Epoch: 116: loss =   1.830 , accuracy =   0.342, test_acc =   0.341 time:   7.319\n",
      "Epoch: 117: loss =   1.756 , accuracy =   0.337, test_acc =   0.337 time:   7.493\n",
      "Epoch: 118: loss =   1.784 , accuracy =   0.336, test_acc =   0.333 time:   7.290\n",
      "Epoch: 119: loss =   1.766 , accuracy =   0.350, test_acc =   0.339 time:   7.333\n",
      "Epoch: 120: loss =   1.740 , accuracy =   0.340, test_acc =   0.333 time:   7.342\n",
      "Epoch: 121: loss =   1.754 , accuracy =   0.338, test_acc =   0.342 time:   7.360\n",
      "Epoch: 122: loss =   1.763 , accuracy =   0.352, test_acc =   0.338 time:   7.412\n",
      "Epoch: 123: loss =   1.751 , accuracy =   0.339, test_acc =   0.351 time:   7.399\n",
      "Epoch: 124: loss =   1.727 , accuracy =   0.343, test_acc =   0.351 time:   7.426\n",
      "Epoch: 125: loss =   1.745 , accuracy =   0.354, test_acc =   0.358 time:   7.309\n",
      "Epoch: 126: loss =   1.727 , accuracy =   0.353, test_acc =   0.361 time:   7.414\n",
      "Epoch: 127: loss =   1.746 , accuracy =   0.360, test_acc =   0.342 time:   7.398\n",
      "Epoch: 128: loss =   1.737 , accuracy =   0.360, test_acc =   0.367 time:   7.394\n",
      "Epoch: 129: loss =   1.738 , accuracy =   0.358, test_acc =   0.358 time:   7.466\n",
      "Epoch: 130: loss =   1.722 , accuracy =   0.359, test_acc =   0.367 time:   7.388\n",
      "Epoch: 131: loss =   1.700 , accuracy =   0.368, test_acc =   0.368 time:   7.303\n",
      "Epoch: 132: loss =   1.728 , accuracy =   0.363, test_acc =   0.361 time:   7.398\n",
      "Epoch: 133: loss =   1.719 , accuracy =   0.366, test_acc =   0.363 time:   7.470\n",
      "Epoch: 134: loss =   1.711 , accuracy =   0.366, test_acc =   0.367 time:   7.423\n",
      "Epoch: 135: loss =   1.718 , accuracy =   0.370, test_acc =   0.356 time:   7.338\n",
      "Epoch: 136: loss =   1.698 , accuracy =   0.367, test_acc =   0.372 time:   7.374\n",
      "Epoch: 137: loss =   1.695 , accuracy =   0.357, test_acc =   0.380 time:   7.347\n",
      "Epoch: 138: loss =   1.690 , accuracy =   0.371, test_acc =   0.370 time:   7.391\n",
      "Epoch: 139: loss =   1.664 , accuracy =   0.378, test_acc =   0.374 time:   7.348\n",
      "Epoch: 140: loss =   1.683 , accuracy =   0.389, test_acc =   0.390 time:   7.337\n",
      "Epoch: 141: loss =   1.675 , accuracy =   0.388, test_acc =   0.379 time:   7.353\n",
      "Epoch: 142: loss =   1.656 , accuracy =   0.385, test_acc =   0.382 time:   7.360\n",
      "Epoch: 143: loss =   1.685 , accuracy =   0.390, test_acc =   0.366 time:   7.344\n",
      "Epoch: 144: loss =   1.689 , accuracy =   0.386, test_acc =   0.376 time:   7.371\n",
      "Epoch: 145: loss =   1.672 , accuracy =   0.386, test_acc =   0.378 time:   7.386\n",
      "Epoch: 146: loss =   1.689 , accuracy =   0.390, test_acc =   0.383 time:   7.340\n",
      "Epoch: 147: loss =   1.671 , accuracy =   0.391, test_acc =   0.373 time:   7.379\n",
      "Epoch: 148: loss =   1.674 , accuracy =   0.393, test_acc =   0.386 time:   7.383\n",
      "Epoch: 149: loss =   1.640 , accuracy =   0.391, test_acc =   0.376 time:   7.322\n",
      "Epoch: 150: loss =   1.635 , accuracy =   0.398, test_acc =   0.393 time:   7.444\n",
      "Epoch: 151: loss =   1.655 , accuracy =   0.394, test_acc =   0.392 time:   7.394\n",
      "Epoch: 152: loss =   1.664 , accuracy =   0.397, test_acc =   0.399 time:   7.341\n",
      "Epoch: 153: loss =   1.655 , accuracy =   0.397, test_acc =   0.401 time:   7.403\n",
      "Epoch: 154: loss =   1.638 , accuracy =   0.401, test_acc =   0.392 time:   7.418\n",
      "Epoch: 155: loss =   1.659 , accuracy =   0.402, test_acc =   0.401 time:   7.385\n",
      "Epoch: 156: loss =   1.625 , accuracy =   0.395, test_acc =   0.390 time:   7.385\n",
      "Epoch: 157: loss =   1.640 , accuracy =   0.394, test_acc =   0.402 time:   7.359\n",
      "Epoch: 158: loss =   1.612 , accuracy =   0.403, test_acc =   0.394 time:   7.465\n",
      "Epoch: 159: loss =   1.618 , accuracy =   0.412, test_acc =   0.409 time:   7.427\n",
      "Epoch: 160: loss =   1.623 , accuracy =   0.412, test_acc =   0.410 time:   7.436\n",
      "Epoch: 161: loss =   1.603 , accuracy =   0.411, test_acc =   0.410 time:   7.370\n",
      "Epoch: 162: loss =   1.587 , accuracy =   0.414, test_acc =   0.408 time:   7.435\n",
      "Epoch: 163: loss =   1.607 , accuracy =   0.421, test_acc =   0.412 time:   7.385\n",
      "Epoch: 164: loss =   1.615 , accuracy =   0.416, test_acc =   0.402 time:   7.393\n",
      "Epoch: 165: loss =   1.605 , accuracy =   0.418, test_acc =   0.418 time:   7.357\n",
      "Epoch: 166: loss =   1.586 , accuracy =   0.416, test_acc =   0.409 time:   7.396\n",
      "Epoch: 167: loss =   1.576 , accuracy =   0.420, test_acc =   0.414 time:   7.418\n",
      "Epoch: 168: loss =   1.600 , accuracy =   0.423, test_acc =   0.419 time:   7.386\n",
      "Epoch: 169: loss =   1.582 , accuracy =   0.425, test_acc =   0.409 time:   7.354\n",
      "Epoch: 170: loss =   1.599 , accuracy =   0.422, test_acc =   0.424 time:   7.406\n",
      "Epoch: 171: loss =   1.576 , accuracy =   0.418, test_acc =   0.420 time:   7.468\n",
      "Epoch: 172: loss =   1.583 , accuracy =   0.426, test_acc =   0.418 time:   7.434\n",
      "Epoch: 173: loss =   1.570 , accuracy =   0.426, test_acc =   0.422 time:   7.470\n",
      "Epoch: 174: loss =   1.569 , accuracy =   0.428, test_acc =   0.415 time:   7.407\n",
      "Epoch: 175: loss =   1.568 , accuracy =   0.428, test_acc =   0.426 time:   7.476\n",
      "Epoch: 176: loss =   1.561 , accuracy =   0.425, test_acc =   0.414 time:   7.360\n",
      "Epoch: 177: loss =   1.568 , accuracy =   0.428, test_acc =   0.418 time:   7.488\n",
      "Epoch: 178: loss =   1.557 , accuracy =   0.432, test_acc =   0.426 time:   7.275\n",
      "Epoch: 179: loss =   1.551 , accuracy =   0.431, test_acc =   0.424 time:   7.394\n",
      "Epoch: 180: loss =   1.566 , accuracy =   0.423, test_acc =   0.423 time:   7.437\n",
      "Epoch: 181: loss =   1.547 , accuracy =   0.436, test_acc =   0.424 time:   7.348\n",
      "Epoch: 182: loss =   1.548 , accuracy =   0.441, test_acc =   0.421 time:   7.443\n",
      "Epoch: 183: loss =   1.545 , accuracy =   0.432, test_acc =   0.432 time:   7.320\n",
      "Epoch: 184: loss =   1.547 , accuracy =   0.431, test_acc =   0.432 time:   7.424\n",
      "Epoch: 185: loss =   1.537 , accuracy =   0.431, test_acc =   0.436 time:   7.392\n",
      "Epoch: 186: loss =   1.562 , accuracy =   0.438, test_acc =   0.440 time:   7.533\n",
      "Epoch: 187: loss =   1.532 , accuracy =   0.447, test_acc =   0.426 time:   7.414\n",
      "Epoch: 188: loss =   1.523 , accuracy =   0.444, test_acc =   0.439 time:   7.444\n",
      "Epoch: 189: loss =   1.545 , accuracy =   0.447, test_acc =   0.440 time:   7.421\n",
      "Epoch: 190: loss =   1.531 , accuracy =   0.445, test_acc =   0.444 time:   7.376\n",
      "Epoch: 191: loss =   1.520 , accuracy =   0.445, test_acc =   0.441 time:   7.347\n",
      "Epoch: 192: loss =   1.513 , accuracy =   0.448, test_acc =   0.437 time:   7.394\n",
      "Epoch: 193: loss =   1.524 , accuracy =   0.443, test_acc =   0.438 time:   7.412\n",
      "Epoch: 194: loss =   1.503 , accuracy =   0.452, test_acc =   0.437 time:   7.385\n",
      "Epoch: 195: loss =   1.517 , accuracy =   0.450, test_acc =   0.441 time:   7.440\n",
      "Epoch: 196: loss =   1.524 , accuracy =   0.446, test_acc =   0.441 time:   7.466\n",
      "Epoch: 197: loss =   1.514 , accuracy =   0.453, test_acc =   0.447 time:   7.379\n",
      "Epoch: 198: loss =   1.524 , accuracy =   0.452, test_acc =   0.443 time:   7.348\n",
      "Epoch: 199: loss =   1.502 , accuracy =   0.456, test_acc =   0.438 time:   7.417\n",
      "Epoch: 200: loss =   1.508 , accuracy =   0.457, test_acc =   0.448 time:   7.472\n",
      "Epoch: 201: loss =   1.505 , accuracy =   0.450, test_acc =   0.451 time:   7.316\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 202: loss =   1.479 , accuracy =   0.457, test_acc =   0.452 time:   7.334\n",
      "Epoch: 203: loss =   1.496 , accuracy =   0.461, test_acc =   0.452 time:   7.391\n",
      "Epoch: 204: loss =   1.499 , accuracy =   0.462, test_acc =   0.448 time:   7.340\n",
      "Epoch: 205: loss =   1.484 , accuracy =   0.465, test_acc =   0.455 time:   7.373\n",
      "Epoch: 206: loss =   1.489 , accuracy =   0.463, test_acc =   0.454 time:   7.443\n",
      "Epoch: 207: loss =   1.467 , accuracy =   0.458, test_acc =   0.460 time:   7.343\n",
      "Epoch: 208: loss =   1.477 , accuracy =   0.459, test_acc =   0.453 time:   7.519\n",
      "Epoch: 209: loss =   1.496 , accuracy =   0.464, test_acc =   0.455 time:   7.402\n",
      "Epoch: 210: loss =   1.467 , accuracy =   0.467, test_acc =   0.452 time:   7.457\n",
      "Epoch: 211: loss =   1.460 , accuracy =   0.463, test_acc =   0.446 time:   7.498\n",
      "Epoch: 212: loss =   1.490 , accuracy =   0.470, test_acc =   0.454 time:   7.465\n",
      "Epoch: 213: loss =   1.543 , accuracy =   0.469, test_acc =   0.456 time:   7.378\n",
      "Epoch: 214: loss =   1.497 , accuracy =   0.465, test_acc =   0.460 time:   7.403\n",
      "Epoch: 215: loss =   1.482 , accuracy =   0.471, test_acc =   0.466 time:   7.343\n",
      "Epoch: 216: loss =   1.489 , accuracy =   0.463, test_acc =   0.456 time:   7.380\n",
      "Epoch: 217: loss =   1.477 , accuracy =   0.466, test_acc =   0.454 time:   7.420\n",
      "Epoch: 218: loss =   1.479 , accuracy =   0.463, test_acc =   0.463 time:   7.374\n",
      "Epoch: 219: loss =   1.487 , accuracy =   0.467, test_acc =   0.451 time:   7.421\n",
      "Epoch: 220: loss =   1.471 , accuracy =   0.474, test_acc =   0.468 time:   7.383\n",
      "Epoch: 221: loss =   1.481 , accuracy =   0.467, test_acc =   0.462 time:   7.335\n",
      "Epoch: 222: loss =   1.467 , accuracy =   0.475, test_acc =   0.467 time:   7.383\n",
      "Epoch: 223: loss =   1.466 , accuracy =   0.467, test_acc =   0.465 time:   7.409\n",
      "Epoch: 224: loss =   1.471 , accuracy =   0.475, test_acc =   0.457 time:   7.299\n",
      "Epoch: 225: loss =   1.452 , accuracy =   0.478, test_acc =   0.467 time:   7.420\n",
      "Epoch: 226: loss =   1.459 , accuracy =   0.474, test_acc =   0.461 time:   7.405\n",
      "Epoch: 227: loss =   1.440 , accuracy =   0.479, test_acc =   0.455 time:   7.400\n",
      "Epoch: 228: loss =   1.443 , accuracy =   0.480, test_acc =   0.467 time:   7.392\n",
      "Epoch: 229: loss =   1.424 , accuracy =   0.483, test_acc =   0.469 time:   7.392\n",
      "Epoch: 230: loss =   1.432 , accuracy =   0.475, test_acc =   0.470 time:   7.405\n",
      "Epoch: 231: loss =   1.448 , accuracy =   0.482, test_acc =   0.473 time:   7.381\n",
      "Epoch: 232: loss =   1.428 , accuracy =   0.482, test_acc =   0.469 time:   7.482\n",
      "Epoch: 233: loss =   1.429 , accuracy =   0.478, test_acc =   0.475 time:   7.427\n",
      "Epoch: 234: loss =   1.443 , accuracy =   0.486, test_acc =   0.465 time:   7.347\n",
      "Epoch: 235: loss =   1.439 , accuracy =   0.479, test_acc =   0.469 time:   7.396\n",
      "Epoch: 236: loss =   1.431 , accuracy =   0.481, test_acc =   0.472 time:   7.390\n",
      "Epoch: 237: loss =   1.445 , accuracy =   0.476, test_acc =   0.469 time:   7.373\n",
      "Epoch: 238: loss =   1.452 , accuracy =   0.486, test_acc =   0.475 time:   7.387\n",
      "Epoch: 239: loss =   1.412 , accuracy =   0.487, test_acc =   0.479 time:   7.386\n",
      "Epoch: 240: loss =   1.410 , accuracy =   0.486, test_acc =   0.477 time:   7.470\n",
      "Epoch: 241: loss =   1.411 , accuracy =   0.487, test_acc =   0.479 time:   7.456\n",
      "Epoch: 242: loss =   1.407 , accuracy =   0.491, test_acc =   0.469 time:   7.392\n",
      "Epoch: 243: loss =   1.407 , accuracy =   0.489, test_acc =   0.479 time:   7.480\n",
      "Epoch: 244: loss =   1.402 , accuracy =   0.489, test_acc =   0.473 time:   7.410\n",
      "Epoch: 245: loss =   1.396 , accuracy =   0.500, test_acc =   0.483 time:   7.419\n",
      "Epoch: 246: loss =   1.397 , accuracy =   0.490, test_acc =   0.484 time:   7.495\n",
      "Epoch: 247: loss =   1.404 , accuracy =   0.490, test_acc =   0.481 time:   7.445\n",
      "Epoch: 248: loss =   1.393 , accuracy =   0.496, test_acc =   0.485 time:   7.451\n",
      "Epoch: 249: loss =   1.386 , accuracy =   0.494, test_acc =   0.486 time:   7.471\n",
      "Epoch: 250: loss =   1.404 , accuracy =   0.494, test_acc =   0.474 time:   7.434\n",
      "Epoch: 251: loss =   1.393 , accuracy =   0.501, test_acc =   0.486 time:   7.483\n",
      "Epoch: 252: loss =   1.396 , accuracy =   0.493, test_acc =   0.493 time:   7.382\n",
      "Epoch: 253: loss =   1.409 , accuracy =   0.498, test_acc =   0.479 time:   7.434\n",
      "Epoch: 254: loss =   1.391 , accuracy =   0.490, test_acc =   0.477 time:   7.464\n",
      "Epoch: 255: loss =   1.409 , accuracy =   0.489, test_acc =   0.478 time:   7.331\n",
      "Epoch: 256: loss =   1.415 , accuracy =   0.503, test_acc =   0.483 time:   7.438\n",
      "Epoch: 257: loss =   1.391 , accuracy =   0.495, test_acc =   0.482 time:   7.396\n",
      "Epoch: 258: loss =   1.403 , accuracy =   0.493, test_acc =   0.471 time:   7.436\n",
      "Epoch: 259: loss =   1.386 , accuracy =   0.493, test_acc =   0.486 time:   7.432\n",
      "Epoch: 260: loss =   1.404 , accuracy =   0.498, test_acc =   0.484 time:   7.429\n",
      "Epoch: 261: loss =   1.400 , accuracy =   0.498, test_acc =   0.484 time:   7.490\n",
      "Epoch: 262: loss =   1.389 , accuracy =   0.501, test_acc =   0.487 time:   7.437\n",
      "Epoch: 263: loss =   1.371 , accuracy =   0.494, test_acc =   0.485 time:   7.420\n",
      "Epoch: 264: loss =   1.389 , accuracy =   0.502, test_acc =   0.490 time:   7.538\n",
      "Epoch: 265: loss =   1.360 , accuracy =   0.499, test_acc =   0.483 time:   7.471\n",
      "Epoch: 266: loss =   1.374 , accuracy =   0.505, test_acc =   0.492 time:   7.525\n",
      "Epoch: 267: loss =   1.363 , accuracy =   0.509, test_acc =   0.481 time:   7.391\n",
      "Epoch: 268: loss =   1.364 , accuracy =   0.508, test_acc =   0.493 time:   7.514\n",
      "Epoch: 269: loss =   1.375 , accuracy =   0.512, test_acc =   0.489 time:   7.452\n",
      "Epoch: 270: loss =   1.365 , accuracy =   0.510, test_acc =   0.491 time:   7.401\n",
      "Epoch: 271: loss =   1.357 , accuracy =   0.514, test_acc =   0.494 time:   7.359\n",
      "Epoch: 272: loss =   1.385 , accuracy =   0.514, test_acc =   0.497 time:   7.378\n",
      "Epoch: 273: loss =   1.348 , accuracy =   0.511, test_acc =   0.497 time:   7.342\n",
      "Epoch: 274: loss =   1.350 , accuracy =   0.512, test_acc =   0.495 time:   7.337\n",
      "Epoch: 275: loss =   1.357 , accuracy =   0.517, test_acc =   0.502 time:   7.403\n",
      "Epoch: 276: loss =   1.350 , accuracy =   0.513, test_acc =   0.501 time:   7.319\n",
      "Epoch: 277: loss =   1.348 , accuracy =   0.517, test_acc =   0.497 time:   7.414\n",
      "Epoch: 278: loss =   1.349 , accuracy =   0.512, test_acc =   0.494 time:   7.377\n",
      "Epoch: 279: loss =   1.344 , accuracy =   0.518, test_acc =   0.496 time:   7.435\n",
      "Epoch: 280: loss =   1.331 , accuracy =   0.517, test_acc =   0.499 time:   7.452\n",
      "Epoch: 281: loss =   1.336 , accuracy =   0.522, test_acc =   0.501 time:   7.419\n",
      "Epoch: 282: loss =   1.345 , accuracy =   0.517, test_acc =   0.496 time:   7.462\n",
      "Epoch: 283: loss =   1.352 , accuracy =   0.514, test_acc =   0.495 time:   7.489\n",
      "Epoch: 284: loss =   1.341 , accuracy =   0.517, test_acc =   0.501 time:   7.393\n",
      "Epoch: 285: loss =   1.317 , accuracy =   0.516, test_acc =   0.493 time:   7.427\n",
      "Epoch: 286: loss =   1.322 , accuracy =   0.521, test_acc =   0.496 time:   7.420\n",
      "Epoch: 287: loss =   1.315 , accuracy =   0.510, test_acc =   0.484 time:   7.374\n",
      "Epoch: 288: loss =   1.334 , accuracy =   0.518, test_acc =   0.500 time:   7.459\n",
      "Epoch: 289: loss =   1.331 , accuracy =   0.515, test_acc =   0.500 time:   7.443\n",
      "Epoch: 290: loss =   1.320 , accuracy =   0.526, test_acc =   0.510 time:   7.426\n",
      "Epoch: 291: loss =   1.328 , accuracy =   0.524, test_acc =   0.507 time:   7.380\n",
      "Epoch: 292: loss =   1.341 , accuracy =   0.524, test_acc =   0.508 time:   7.463\n",
      "Epoch: 293: loss =   1.323 , accuracy =   0.525, test_acc =   0.505 time:   7.419\n",
      "Epoch: 294: loss =   1.333 , accuracy =   0.525, test_acc =   0.507 time:   7.482\n",
      "Epoch: 295: loss =   1.310 , accuracy =   0.522, test_acc =   0.498 time:   7.435\n",
      "Epoch: 296: loss =   1.318 , accuracy =   0.524, test_acc =   0.512 time:   7.406\n",
      "Epoch: 297: loss =   1.318 , accuracy =   0.526, test_acc =   0.499 time:   7.401\n",
      "Epoch: 298: loss =   1.318 , accuracy =   0.525, test_acc =   0.504 time:   7.460\n",
      "Epoch: 299: loss =   1.318 , accuracy =   0.531, test_acc =   0.505 time:   7.408\n",
      "Epoch: 300: loss =   1.295 , accuracy =   0.529, test_acc =   0.496 time:   7.467\n",
      "Epoch: 301: loss =   1.306 , accuracy =   0.529, test_acc =   0.508 time:   7.474\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 302: loss =   1.289 , accuracy =   0.526, test_acc =   0.506 time:   7.383\n",
      "Epoch: 303: loss =   1.311 , accuracy =   0.521, test_acc =   0.505 time:   7.384\n",
      "Epoch: 304: loss =   1.319 , accuracy =   0.525, test_acc =   0.506 time:   7.434\n",
      "Epoch: 305: loss =   1.311 , accuracy =   0.528, test_acc =   0.508 time:   7.368\n",
      "Epoch: 306: loss =   1.305 , accuracy =   0.527, test_acc =   0.503 time:   7.550\n",
      "Epoch: 307: loss =   1.301 , accuracy =   0.529, test_acc =   0.512 time:   7.408\n",
      "Epoch: 308: loss =   1.300 , accuracy =   0.529, test_acc =   0.504 time:   7.461\n",
      "Epoch: 309: loss =   1.291 , accuracy =   0.526, test_acc =   0.505 time:   7.424\n",
      "Epoch: 310: loss =   1.297 , accuracy =   0.529, test_acc =   0.511 time:   7.421\n",
      "Epoch: 311: loss =   1.302 , accuracy =   0.537, test_acc =   0.514 time:   7.337\n",
      "Epoch: 312: loss =   1.289 , accuracy =   0.536, test_acc =   0.508 time:   7.397\n",
      "Epoch: 313: loss =   1.285 , accuracy =   0.532, test_acc =   0.509 time:   7.499\n",
      "Epoch: 314: loss =   1.286 , accuracy =   0.528, test_acc =   0.506 time:   7.473\n",
      "Epoch: 315: loss =   1.297 , accuracy =   0.534, test_acc =   0.509 time:   7.473\n",
      "Epoch: 316: loss =   1.285 , accuracy =   0.539, test_acc =   0.517 time:   7.402\n",
      "Epoch: 317: loss =   1.290 , accuracy =   0.532, test_acc =   0.513 time:   7.477\n",
      "Epoch: 318: loss =   1.283 , accuracy =   0.541, test_acc =   0.516 time:   7.473\n",
      "Epoch: 319: loss =   1.273 , accuracy =   0.535, test_acc =   0.517 time:   7.453\n",
      "Epoch: 320: loss =   1.293 , accuracy =   0.541, test_acc =   0.514 time:   7.492\n",
      "Epoch: 321: loss =   1.275 , accuracy =   0.538, test_acc =   0.517 time:   7.474\n",
      "Epoch: 322: loss =   1.268 , accuracy =   0.541, test_acc =   0.515 time:   7.468\n",
      "Epoch: 323: loss =   1.280 , accuracy =   0.532, test_acc =   0.511 time:   7.441\n",
      "Epoch: 324: loss =   1.293 , accuracy =   0.524, test_acc =   0.501 time:   7.367\n",
      "Epoch: 325: loss =   1.311 , accuracy =   0.517, test_acc =   0.499 time:   7.366\n",
      "Epoch: 326: loss =   1.341 , accuracy =   0.521, test_acc =   0.503 time:   7.403\n",
      "Epoch: 327: loss =   1.301 , accuracy =   0.535, test_acc =   0.512 time:   7.435\n",
      "Epoch: 328: loss =   1.281 , accuracy =   0.522, test_acc =   0.510 time:   7.387\n",
      "Epoch: 329: loss =   1.311 , accuracy =   0.526, test_acc =   0.504 time:   7.443\n",
      "Epoch: 330: loss =   1.315 , accuracy =   0.536, test_acc =   0.511 time:   7.402\n",
      "Epoch: 331: loss =   1.280 , accuracy =   0.530, test_acc =   0.510 time:   7.419\n",
      "Epoch: 332: loss =   1.308 , accuracy =   0.530, test_acc =   0.510 time:   7.397\n",
      "Epoch: 333: loss =   1.280 , accuracy =   0.526, test_acc =   0.507 time:   7.423\n",
      "Epoch: 334: loss =   1.312 , accuracy =   0.531, test_acc =   0.506 time:   7.442\n",
      "Epoch: 335: loss =   1.303 , accuracy =   0.534, test_acc =   0.506 time:   7.407\n",
      "Epoch: 336: loss =   1.310 , accuracy =   0.537, test_acc =   0.518 time:   7.459\n",
      "Epoch: 337: loss =   1.279 , accuracy =   0.535, test_acc =   0.508 time:   7.446\n",
      "Epoch: 338: loss =   1.290 , accuracy =   0.536, test_acc =   0.514 time:   7.862\n",
      "Epoch: 339: loss =   1.279 , accuracy =   0.525, test_acc =   0.506 time:   7.459\n",
      "Epoch: 340: loss =   1.278 , accuracy =   0.540, test_acc =   0.516 time:   7.462\n",
      "Epoch: 341: loss =   1.270 , accuracy =   0.544, test_acc =   0.521 time:   7.497\n",
      "Epoch: 342: loss =   1.265 , accuracy =   0.538, test_acc =   0.513 time:   7.448\n",
      "Epoch: 343: loss =   1.266 , accuracy =   0.543, test_acc =   0.516 time:   7.568\n",
      "Epoch: 344: loss =   1.261 , accuracy =   0.547, test_acc =   0.521 time:   7.431\n",
      "Epoch: 345: loss =   1.251 , accuracy =   0.545, test_acc =   0.522 time:   7.455\n",
      "Epoch: 346: loss =   1.245 , accuracy =   0.539, test_acc =   0.526 time:   7.465\n",
      "Epoch: 347: loss =   1.258 , accuracy =   0.548, test_acc =   0.520 time:   7.433\n",
      "Epoch: 348: loss =   1.250 , accuracy =   0.550, test_acc =   0.523 time:   7.474\n",
      "Epoch: 349: loss =   1.244 , accuracy =   0.548, test_acc =   0.524 time:   7.394\n",
      "Epoch: 350: loss =   1.251 , accuracy =   0.551, test_acc =   0.524 time:   7.383\n",
      "Epoch: 351: loss =   1.245 , accuracy =   0.553, test_acc =   0.515 time:   7.487\n",
      "Epoch: 352: loss =   1.226 , accuracy =   0.550, test_acc =   0.524 time:   7.530\n",
      "Epoch: 353: loss =   1.246 , accuracy =   0.555, test_acc =   0.524 time:   7.353\n",
      "Epoch: 354: loss =   1.237 , accuracy =   0.553, test_acc =   0.524 time:   7.406\n",
      "Epoch: 355: loss =   1.238 , accuracy =   0.552, test_acc =   0.522 time:   7.464\n",
      "Epoch: 356: loss =   1.235 , accuracy =   0.557, test_acc =   0.524 time:   7.449\n",
      "Epoch: 357: loss =   1.238 , accuracy =   0.553, test_acc =   0.522 time:   7.495\n",
      "Epoch: 358: loss =   1.226 , accuracy =   0.557, test_acc =   0.522 time:   7.436\n",
      "Epoch: 359: loss =   1.214 , accuracy =   0.556, test_acc =   0.528 time:   7.556\n",
      "Epoch: 360: loss =   1.213 , accuracy =   0.558, test_acc =   0.529 time:   7.515\n",
      "Epoch: 361: loss =   1.235 , accuracy =   0.559, test_acc =   0.527 time:   7.491\n",
      "Epoch: 362: loss =   1.229 , accuracy =   0.556, test_acc =   0.527 time:   7.393\n",
      "Epoch: 363: loss =   1.217 , accuracy =   0.555, test_acc =   0.523 time:   7.490\n",
      "Epoch: 364: loss =   1.219 , accuracy =   0.557, test_acc =   0.527 time:   7.502\n",
      "Epoch: 365: loss =   1.217 , accuracy =   0.554, test_acc =   0.523 time:   7.477\n",
      "Epoch: 366: loss =   1.220 , accuracy =   0.557, test_acc =   0.522 time:   7.483\n",
      "Epoch: 367: loss =   1.226 , accuracy =   0.558, test_acc =   0.533 time:   7.448\n",
      "Epoch: 368: loss =   1.231 , accuracy =   0.558, test_acc =   0.525 time:   7.426\n",
      "Epoch: 369: loss =   1.214 , accuracy =   0.562, test_acc =   0.533 time:   7.475\n",
      "Epoch: 370: loss =   1.206 , accuracy =   0.565, test_acc =   0.529 time:   7.412\n",
      "Epoch: 371: loss =   1.208 , accuracy =   0.553, test_acc =   0.525 time:   7.429\n",
      "Epoch: 372: loss =   1.210 , accuracy =   0.563, test_acc =   0.531 time:   7.388\n",
      "Epoch: 373: loss =   1.209 , accuracy =   0.565, test_acc =   0.533 time:   7.523\n",
      "Epoch: 374: loss =   1.207 , accuracy =   0.561, test_acc =   0.522 time:   7.362\n",
      "Epoch: 375: loss =   1.210 , accuracy =   0.559, test_acc =   0.531 time:   7.548\n",
      "Epoch: 376: loss =   1.227 , accuracy =   0.565, test_acc =   0.533 time:   7.508\n",
      "Epoch: 377: loss =   1.194 , accuracy =   0.568, test_acc =   0.533 time:   7.415\n",
      "Epoch: 378: loss =   1.214 , accuracy =   0.562, test_acc =   0.530 time:   7.558\n",
      "Epoch: 379: loss =   1.202 , accuracy =   0.564, test_acc =   0.533 time:   7.533\n",
      "Epoch: 380: loss =   1.210 , accuracy =   0.558, test_acc =   0.529 time:   7.421\n",
      "Epoch: 381: loss =   1.205 , accuracy =   0.568, test_acc =   0.532 time:   7.442\n",
      "Epoch: 382: loss =   1.203 , accuracy =   0.563, test_acc =   0.533 time:   7.412\n",
      "Epoch: 383: loss =   1.218 , accuracy =   0.560, test_acc =   0.529 time:   7.408\n",
      "Epoch: 384: loss =   1.217 , accuracy =   0.568, test_acc =   0.528 time:   7.459\n",
      "Epoch: 385: loss =   1.197 , accuracy =   0.568, test_acc =   0.529 time:   7.461\n",
      "Epoch: 386: loss =   1.188 , accuracy =   0.558, test_acc =   0.530 time:   7.447\n",
      "Epoch: 387: loss =   1.206 , accuracy =   0.567, test_acc =   0.528 time:   7.424\n",
      "Epoch: 388: loss =   1.194 , accuracy =   0.565, test_acc =   0.538 time:   7.392\n",
      "Epoch: 389: loss =   1.196 , accuracy =   0.568, test_acc =   0.538 time:   7.394\n",
      "Epoch: 390: loss =   1.207 , accuracy =   0.565, test_acc =   0.533 time:   7.456\n",
      "Epoch: 391: loss =   1.197 , accuracy =   0.567, test_acc =   0.535 time:   7.542\n",
      "Epoch: 392: loss =   1.206 , accuracy =   0.571, test_acc =   0.532 time:   7.497\n",
      "Epoch: 393: loss =   1.210 , accuracy =   0.568, test_acc =   0.537 time:   7.533\n",
      "Epoch: 394: loss =   1.190 , accuracy =   0.575, test_acc =   0.536 time:   7.422\n",
      "Epoch: 395: loss =   1.190 , accuracy =   0.567, test_acc =   0.535 time:   7.428\n",
      "Epoch: 396: loss =   1.177 , accuracy =   0.569, test_acc =   0.538 time:   7.534\n",
      "Epoch: 397: loss =   1.194 , accuracy =   0.571, test_acc =   0.534 time:   7.487\n",
      "Epoch: 398: loss =   1.185 , accuracy =   0.574, test_acc =   0.534 time:   7.345\n",
      "Epoch: 399: loss =   1.180 , accuracy =   0.577, test_acc =   0.540 time:   7.446\n",
      "Epoch: 400: loss =   1.172 , accuracy =   0.570, test_acc =   0.540 time:   7.448\n",
      "Epoch: 401: loss =   1.176 , accuracy =   0.573, test_acc =   0.536 time:   7.424\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 402: loss =   1.185 , accuracy =   0.565, test_acc =   0.537 time:   7.526\n",
      "Epoch: 403: loss =   1.185 , accuracy =   0.570, test_acc =   0.539 time:   7.448\n",
      "Epoch: 404: loss =   1.194 , accuracy =   0.576, test_acc =   0.541 time:   7.461\n",
      "Epoch: 405: loss =   1.171 , accuracy =   0.574, test_acc =   0.541 time:   7.517\n",
      "Epoch: 406: loss =   1.165 , accuracy =   0.575, test_acc =   0.539 time:   7.481\n",
      "Epoch: 407: loss =   1.169 , accuracy =   0.575, test_acc =   0.541 time:   7.436\n",
      "Epoch: 408: loss =   1.175 , accuracy =   0.574, test_acc =   0.535 time:   7.489\n",
      "Epoch: 409: loss =   1.173 , accuracy =   0.576, test_acc =   0.532 time:   7.493\n",
      "Epoch: 410: loss =   1.161 , accuracy =   0.578, test_acc =   0.542 time:   7.469\n",
      "Epoch: 411: loss =   1.157 , accuracy =   0.579, test_acc =   0.537 time:   7.445\n",
      "Epoch: 412: loss =   1.161 , accuracy =   0.580, test_acc =   0.541 time:   7.441\n",
      "Epoch: 413: loss =   1.169 , accuracy =   0.574, test_acc =   0.535 time:   7.539\n",
      "Epoch: 414: loss =   1.164 , accuracy =   0.577, test_acc =   0.534 time:   7.477\n",
      "Epoch: 415: loss =   1.165 , accuracy =   0.580, test_acc =   0.537 time:   7.422\n",
      "Epoch: 416: loss =   1.164 , accuracy =   0.575, test_acc =   0.534 time:   7.488\n",
      "Epoch: 417: loss =   1.164 , accuracy =   0.573, test_acc =   0.531 time:   7.419\n",
      "Epoch: 418: loss =   1.176 , accuracy =   0.569, test_acc =   0.533 time:   7.440\n",
      "Epoch: 419: loss =   1.173 , accuracy =   0.574, test_acc =   0.538 time:   7.438\n",
      "Epoch: 420: loss =   1.173 , accuracy =   0.578, test_acc =   0.538 time:   7.384\n",
      "Epoch: 421: loss =   1.172 , accuracy =   0.576, test_acc =   0.528 time:   7.475\n",
      "Epoch: 422: loss =   1.177 , accuracy =   0.576, test_acc =   0.535 time:   7.393\n",
      "Epoch: 423: loss =   1.180 , accuracy =   0.577, test_acc =   0.532 time:   7.437\n",
      "Epoch: 424: loss =   1.169 , accuracy =   0.578, test_acc =   0.542 time:   7.490\n",
      "Epoch: 425: loss =   1.159 , accuracy =   0.578, test_acc =   0.538 time:   7.467\n",
      "Epoch: 426: loss =   1.165 , accuracy =   0.580, test_acc =   0.540 time:   7.530\n",
      "Epoch: 427: loss =   1.158 , accuracy =   0.577, test_acc =   0.540 time:   7.484\n",
      "Epoch: 428: loss =   1.165 , accuracy =   0.576, test_acc =   0.532 time:   7.436\n",
      "Epoch: 429: loss =   1.166 , accuracy =   0.580, test_acc =   0.538 time:   7.469\n",
      "Epoch: 430: loss =   1.158 , accuracy =   0.582, test_acc =   0.539 time:   7.498\n",
      "Epoch: 431: loss =   1.163 , accuracy =   0.580, test_acc =   0.540 time:   7.415\n",
      "Epoch: 432: loss =   1.164 , accuracy =   0.581, test_acc =   0.536 time:   7.500\n",
      "Epoch: 433: loss =   1.166 , accuracy =   0.583, test_acc =   0.540 time:   7.447\n",
      "Epoch: 434: loss =   1.162 , accuracy =   0.584, test_acc =   0.541 time:   7.383\n",
      "Epoch: 435: loss =   1.157 , accuracy =   0.585, test_acc =   0.541 time:   7.466\n",
      "Epoch: 436: loss =   1.154 , accuracy =   0.583, test_acc =   0.543 time:   7.570\n",
      "Epoch: 437: loss =   1.139 , accuracy =   0.587, test_acc =   0.545 time:   7.385\n",
      "Epoch: 438: loss =   1.139 , accuracy =   0.585, test_acc =   0.542 time:   7.479\n",
      "Epoch: 439: loss =   1.141 , accuracy =   0.589, test_acc =   0.541 time:   7.469\n",
      "Epoch: 440: loss =   1.135 , accuracy =   0.589, test_acc =   0.543 time:   7.428\n",
      "Epoch: 441: loss =   1.143 , accuracy =   0.581, test_acc =   0.541 time:   7.507\n",
      "Epoch: 442: loss =   1.136 , accuracy =   0.590, test_acc =   0.545 time:   7.448\n",
      "Epoch: 443: loss =   1.137 , accuracy =   0.586, test_acc =   0.545 time:   7.480\n",
      "Epoch: 444: loss =   1.133 , accuracy =   0.586, test_acc =   0.538 time:   7.503\n",
      "Epoch: 445: loss =   1.144 , accuracy =   0.585, test_acc =   0.535 time:   7.436\n",
      "Epoch: 446: loss =   1.149 , accuracy =   0.587, test_acc =   0.542 time:   7.570\n",
      "Epoch: 447: loss =   1.124 , accuracy =   0.590, test_acc =   0.545 time:   7.402\n",
      "Epoch: 448: loss =   1.134 , accuracy =   0.591, test_acc =   0.544 time:   7.512\n",
      "Epoch: 449: loss =   1.127 , accuracy =   0.593, test_acc =   0.547 time:   7.398\n",
      "Epoch: 450: loss =   1.136 , accuracy =   0.593, test_acc =   0.540 time:   7.482\n",
      "Epoch: 451: loss =   1.137 , accuracy =   0.589, test_acc =   0.547 time:   7.466\n",
      "Epoch: 452: loss =   1.128 , accuracy =   0.592, test_acc =   0.538 time:   7.409\n",
      "Epoch: 453: loss =   1.120 , accuracy =   0.597, test_acc =   0.544 time:   7.530\n",
      "Epoch: 454: loss =   1.116 , accuracy =   0.594, test_acc =   0.546 time:   7.510\n",
      "Epoch: 455: loss =   1.118 , accuracy =   0.594, test_acc =   0.541 time:   7.479\n",
      "Epoch: 456: loss =   1.119 , accuracy =   0.590, test_acc =   0.547 time:   7.450\n",
      "Epoch: 457: loss =   1.127 , accuracy =   0.597, test_acc =   0.549 time:   7.428\n",
      "Epoch: 458: loss =   1.125 , accuracy =   0.588, test_acc =   0.542 time:   7.504\n",
      "Epoch: 459: loss =   1.126 , accuracy =   0.593, test_acc =   0.547 time:   7.453\n",
      "Epoch: 460: loss =   1.132 , accuracy =   0.598, test_acc =   0.543 time:   7.635\n",
      "Epoch: 461: loss =   1.113 , accuracy =   0.592, test_acc =   0.545 time:   7.427\n",
      "Epoch: 462: loss =   1.117 , accuracy =   0.591, test_acc =   0.547 time:   7.420\n",
      "Epoch: 463: loss =   1.124 , accuracy =   0.591, test_acc =   0.543 time:   7.451\n",
      "Epoch: 464: loss =   1.116 , accuracy =   0.596, test_acc =   0.542 time:   7.472\n",
      "Epoch: 465: loss =   1.122 , accuracy =   0.591, test_acc =   0.538 time:   7.481\n",
      "Epoch: 466: loss =   1.140 , accuracy =   0.591, test_acc =   0.539 time:   7.434\n",
      "Epoch: 467: loss =   1.128 , accuracy =   0.587, test_acc =   0.539 time:   7.803\n",
      "Epoch: 468: loss =   1.113 , accuracy =   0.595, test_acc =   0.540 time:   7.362\n",
      "Epoch: 469: loss =   1.119 , accuracy =   0.600, test_acc =   0.546 time:   7.452\n",
      "Epoch: 470: loss =   1.112 , accuracy =   0.600, test_acc =   0.541 time:   7.529\n",
      "Epoch: 471: loss =   1.117 , accuracy =   0.595, test_acc =   0.538 time:   7.459\n",
      "Epoch: 472: loss =   1.116 , accuracy =   0.599, test_acc =   0.543 time:   7.482\n",
      "Epoch: 473: loss =   1.094 , accuracy =   0.600, test_acc =   0.544 time:   7.462\n",
      "Epoch: 474: loss =   1.104 , accuracy =   0.601, test_acc =   0.544 time:   7.487\n",
      "Epoch: 475: loss =   1.106 , accuracy =   0.606, test_acc =   0.544 time:   7.524\n",
      "Epoch: 476: loss =   1.107 , accuracy =   0.597, test_acc =   0.544 time:   7.443\n",
      "Epoch: 477: loss =   1.101 , accuracy =   0.605, test_acc =   0.547 time:   7.556\n",
      "Epoch: 478: loss =   1.100 , accuracy =   0.604, test_acc =   0.548 time:   7.436\n",
      "Epoch: 479: loss =   1.097 , accuracy =   0.602, test_acc =   0.542 time:   7.461\n",
      "Epoch: 480: loss =   1.093 , accuracy =   0.597, test_acc =   0.546 time:   7.480\n",
      "Epoch: 481: loss =   1.096 , accuracy =   0.603, test_acc =   0.543 time:   7.409\n",
      "Epoch: 482: loss =   1.083 , accuracy =   0.603, test_acc =   0.545 time:   7.429\n",
      "Epoch: 483: loss =   1.101 , accuracy =   0.606, test_acc =   0.546 time:   7.465\n",
      "Epoch: 484: loss =   1.080 , accuracy =   0.602, test_acc =   0.547 time:   7.439\n",
      "Epoch: 485: loss =   1.083 , accuracy =   0.599, test_acc =   0.543 time:   7.533\n",
      "Epoch: 486: loss =   1.092 , accuracy =   0.603, test_acc =   0.542 time:   7.407\n",
      "Epoch: 487: loss =   1.107 , accuracy =   0.600, test_acc =   0.544 time:   7.429\n",
      "Epoch: 488: loss =   1.115 , accuracy =   0.597, test_acc =   0.542 time:   7.498\n",
      "Epoch: 489: loss =   1.115 , accuracy =   0.600, test_acc =   0.546 time:   7.479\n",
      "Epoch: 490: loss =   1.111 , accuracy =   0.594, test_acc =   0.538 time:   7.397\n",
      "Epoch: 491: loss =   1.126 , accuracy =   0.596, test_acc =   0.542 time:   7.456\n",
      "Epoch: 492: loss =   1.114 , accuracy =   0.595, test_acc =   0.537 time:   7.560\n",
      "Epoch: 493: loss =   1.131 , accuracy =   0.601, test_acc =   0.540 time:   7.369\n",
      "Epoch: 494: loss =   1.099 , accuracy =   0.607, test_acc =   0.543 time:   7.465\n",
      "Epoch: 495: loss =   1.099 , accuracy =   0.586, test_acc =   0.536 time:   7.498\n",
      "Epoch: 496: loss =   1.132 , accuracy =   0.591, test_acc =   0.533 time:   7.488\n",
      "Epoch: 497: loss =   1.131 , accuracy =   0.603, test_acc =   0.540 time:   7.513\n",
      "Epoch: 498: loss =   1.105 , accuracy =   0.595, test_acc =   0.539 time:   7.479\n",
      "Epoch: 499: loss =   1.120 , accuracy =   0.602, test_acc =   0.543 time:   7.473\n",
      "Epoch: 500: loss =   1.100 , accuracy =   0.603, test_acc =   0.542 time:   7.438\n",
      "Epoch: 501: loss =   1.099 , accuracy =   0.601, test_acc =   0.542 time:   7.500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 502: loss =   1.112 , accuracy =   0.607, test_acc =   0.540 time:   7.474\n",
      "Epoch: 503: loss =   1.092 , accuracy =   0.605, test_acc =   0.546 time:   7.516\n",
      "Epoch: 504: loss =   1.091 , accuracy =   0.602, test_acc =   0.542 time:   7.469\n",
      "Epoch: 505: loss =   1.101 , accuracy =   0.608, test_acc =   0.545 time:   7.440\n",
      "Epoch: 506: loss =   1.096 , accuracy =   0.601, test_acc =   0.541 time:   7.450\n",
      "Epoch: 507: loss =   1.081 , accuracy =   0.605, test_acc =   0.545 time:   7.430\n",
      "Epoch: 508: loss =   1.088 , accuracy =   0.606, test_acc =   0.541 time:   7.504\n",
      "Epoch: 509: loss =   1.069 , accuracy =   0.607, test_acc =   0.544 time:   7.526\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "in our train step we can see that it lasts more tha na normal CNN to converge\n",
    "on the other side, we can have the confidence interval for our predictions, which are \n",
    "wonderful in terms of taking sensitive predictions\n",
    "\"\"\"\n",
    "times = []\n",
    "for i in range(700):\n",
    "    tic = time.time()\n",
    "    loss = train_step(X_train, y_train)\n",
    "    preds = bcnn(X_train)\n",
    "    acc = accuracy(preds, y_train)\n",
    "    preds_test = bcnn(X_test)\n",
    "    test_acc = accuracy(preds_test, y_test)\n",
    "    tac = time.time()\n",
    "    train_time = tac-tic\n",
    "    times.append(train_time)\n",
    "    print(\"Epoch: {}: loss = {:7.3f} , accuracy = {:7.3f}, test_acc = {:7.3f} time: {:7.3f}\".format(i, loss, acc, test_acc, train_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bcnn.save_weights(\"bcnn_cifar10.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### About the performance:\n",
    "\n",
    "mean = np.mean(times)\n",
    "std = np.std(times)\n",
    "print(\"In TensorFlow 2.0.0b1 our train time mean was : {:7.3f}, with std : {:7.3f}\".format(mean, std))\n",
    "\n",
    "no_outlier = times[1:]\n",
    "no_mean = np.mean(no_outlier)\n",
    "no_std = np.std(no_outlier)\n",
    "print(\"\\nHowever, by removing the outlier 1st time, our train time mean was : {:7.3f}, with std : {:7.3f}\".format(no_mean, no_std))\n",
    "#print(\"\\nWe conclude TensorFlow 2 has a longer time to start its variables, but then does it faster than TF1.14 Intel Optimzied (see other notebook)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we will illustrate our predictions and confidence intervals\n",
    "\n",
    "Those illustrative functions were taken from https://github.com/zhulingchen/tfp-tutorial/ repo, which had the tutorial (in Keras) that did let me learn how to \n",
    "\n",
    "### Here we have some statistics on recognizable and unrecognizable images from MNIST:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_mc_run = 100\n",
    "med_prob_thres = 0.20\n",
    "\n",
    "y_pred_logits_list = [bcnn(X_test) for _ in range(n_mc_run)]  # a list of predicted logits\n",
    "y_pred_prob_all = np.concatenate([softmax(y, axis=-1)[:, :, np.newaxis] for y in y_pred_logits_list], axis=-1)\n",
    "y_pred = [[int(np.median(y) >= med_prob_thres) for y in y_pred_prob] for y_pred_prob in y_pred_prob_all]\n",
    "y_pred = np.array(y_pred)\n",
    "\n",
    "idx_valid = [any(y) for y in y_pred]\n",
    "print('Number of recognizable samples:', sum(idx_valid))\n",
    "\n",
    "idx_invalid = [not any(y) for y in y_pred]\n",
    "print('Unrecognizable samples:', np.where(idx_invalid)[0])\n",
    "\n",
    "print('Test accuracy on MNIST (recognizable samples):',\n",
    "      sum(np.equal(np.argmax(y_test[idx_valid], axis=-1), np.argmax(y_pred[idx_valid], axis=-1))) / len(y_test[idx_valid]))\n",
    "\n",
    "print('Test accuracy on MNIST (unrecognizable samples):',\n",
    "      sum(np.equal(np.argmax(y_test[idx_invalid], axis=-1), np.argmax(y_pred[idx_invalid], axis=-1))) / len(y_test[idx_invalid]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With this other snippet, we can plot the predict distribution of our network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pred_hist(y_pred, n_class, n_mc_run, n_bins=30, med_prob_thres=0.2, n_subplot_rows=2, figsize=(25, 10)):\n",
    "    bins = np.logspace(-n_bins, 0, n_bins+1)\n",
    "    fig, ax = plt.subplots(n_subplot_rows, n_class // n_subplot_rows + 1, figsize=figsize)\n",
    "    for i in range(n_subplot_rows):\n",
    "        for j in range(n_class // n_subplot_rows + 1):\n",
    "            idx = i * (n_class // n_subplot_rows + 1) + j\n",
    "            if idx < n_class:\n",
    "                ax[i, j].hist(y_pred[idx], bins)\n",
    "                ax[i, j].set_xscale('log')\n",
    "                ax[i, j].set_ylim([0, n_mc_run])\n",
    "                ax[i, j].title.set_text(\"{} (median prob: {:.2f}) ({})\".format(str(idx),\n",
    "                                                                               np.median(y_pred[idx]),\n",
    "                                                                               str(np.median(y_pred[idx]) >= med_prob_thres)))\n",
    "            else:\n",
    "                ax[i, j].axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This example is not recognizable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in np.where(idx_invalid)[0]:\n",
    "    plt.imshow(X_test[idx, :, :, :], cmap='gist_gray')\n",
    "    print(\"True label of the test sample {}: {}\".format(idx, np.argmax(y_test[idx], axis=-1)))\n",
    "\n",
    "    plot_pred_hist(y_pred_prob_all[idx], class_nmr, n_mc_run, med_prob_thres=med_prob_thres)\n",
    "\n",
    "    if any(y_pred[idx]):\n",
    "        print(\"Predicted label of the test sample {}: {}\".format(idx, np.argmax(y_pred[idx], axis=-1)))\n",
    "    else:\n",
    "        print(\"I don't know!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A recognizable one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X_test[0, :, :, :], cmap='gist_gray')\n",
    "print(\"True label of the test sample {}: {}\".format(0, np.argmax(y_test[0], axis=-1)))\n",
    "\n",
    "plot_pred_hist(y_pred_prob_all[0], class_nmr, n_mc_run, med_prob_thres=med_prob_thres)\n",
    "\n",
    "if any(y_pred[0]):\n",
    "    print(\"Predicted label of the test sample {}: {}\".format(idx, np.argmax(y_pred[0], axis=-1)))\n",
    "else:\n",
    "    print(\"I don't know!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_prob",
   "language": "python",
   "name": "tf_prob"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
