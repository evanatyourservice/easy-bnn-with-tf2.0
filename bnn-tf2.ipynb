{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Neural Networks in Keras and TensorFlow 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/u30073/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/u30073/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/u30073/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/u30073/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/u30073/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/u30073/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/glob/intel-python/python3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/home/u30073/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/u30073/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/u30073/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/u30073/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/u30073/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/u30073/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "#TODO: PUT RANDOM SEED FOR RESULT SECURING IN DEMOS\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version (expected = 2.0.0-beta1): 2.0.0-beta1\n",
      "TensorFlow Probability version (expected = 0.9.0-dev20190912): 0.9.0-dev20190912\n"
     ]
    }
   ],
   "source": [
    "print('TensorFlow version (expected = 2.0.0-beta1):', tf.__version__)\n",
    "print('TensorFlow Probability version (expected = 0.9.0-dev20190912):', tfp.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X, theta = 1.0, axis = None):\n",
    "    \"\"\"\n",
    "    Compute the softmax of each element along an axis of X.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X: ND-Array. Probably should be floats.\n",
    "    theta (optional): float parameter, used as a multiplier\n",
    "        prior to exponentiation. Default = 1.0\n",
    "    axis (optional): axis to compute values along. Default is the\n",
    "        first non-singleton axis.\n",
    "\n",
    "    Returns an array the same size as X. The result will sum to 1\n",
    "    along the specified axis.\n",
    "    \"\"\"\n",
    "\n",
    "    # make X at least 2d\n",
    "    y = np.atleast_2d(X)\n",
    "\n",
    "    # find axis\n",
    "    if axis is None:\n",
    "        axis = next(j[0] for j in enumerate(y.shape) if j[1] > 1)\n",
    "\n",
    "    # multiply y against the theta parameter,\n",
    "    y = y * float(theta)\n",
    "\n",
    "    # subtract the max for numerical stability\n",
    "    y = y - np.expand_dims(np.max(y, axis = axis), axis)\n",
    "\n",
    "    # exponentiate y\n",
    "    y = np.exp(y)\n",
    "\n",
    "    # take the sum along the specified axis\n",
    "    ax_sum = np.expand_dims(np.sum(y, axis = axis), axis)\n",
    "\n",
    "    # finally: divide elementwise\n",
    "    p = y / ax_sum\n",
    "\n",
    "    # flatten if X was 1D\n",
    "    if len(X.shape) == 1: p = p.flatten()\n",
    "\n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We are all set up, lets go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are using MNIST dataset!\n",
      "X_train.shape = (60000, 28, 28, 1)\n",
      "y_train.shape = (60000, 10)\n",
      "X_test.shape = (10000, 28, 28, 1)\n",
      "y_test.shape = (10000, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f0a06e60cc0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhB8IivADQf0bIHV3m9z7zF4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0a08c617f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "first, we load the dataset. We are trying to do it first with MNIST\n",
    "I've found this snippet somewhere in github\n",
    "\"\"\"\n",
    "\n",
    "#as we have 10 classes, I'm setting class number to 10\n",
    "class_nmr = 10\n",
    "\n",
    "print('We are using MNIST dataset!')\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "X_train = np.expand_dims(X_train, -1)\n",
    "n_train = X_train.shape[0]\n",
    "X_test = np.expand_dims(X_test, -1)\n",
    "y_train = tf.keras.utils.to_categorical(y_train, class_nmr)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, class_nmr)\n",
    "\n",
    "# Normalize data\n",
    "X_train = X_train.astype('float32') / 255\n",
    "X_test = X_test.astype('float32') / 255\n",
    "\n",
    "print(\"X_train.shape =\", X_train.shape)\n",
    "print(\"y_train.shape =\", y_train.shape)\n",
    "print(\"X_test.shape =\", X_test.shape)\n",
    "print(\"y_test.shape =\", y_test.shape)\n",
    "\n",
    "plt.imshow(X_train[1026, :, :, 0], cmap='gist_gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bayesian_bcnn_model(input_shape):\n",
    "    model_in = tf.keras.layers.Input(shape=input_shape)\n",
    "    conv_1 = tfp.python.layers.Convolution2DFlipout(32, kernel_size=(3, 3), padding=\"same\", strides=2)\n",
    "    x = conv_1(model_in)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "    conv_2 = tfp.python.layers.Convolution2DFlipout(64, kernel_size=(3, 3), padding=\"same\", strides=2)\n",
    "    x = conv_2(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    dense_1 = tfp.python.layers.DenseFlipout(512, activation='relu')\n",
    "    x = dense_1(x)\n",
    "    dense_2 = tfp.python.layers.DenseFlipout(10, activation=None)\n",
    "    model_out = dense_2(x)  # logits\n",
    "    model = tf.keras.Model(model_in, model_out)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bcnn = build_bayesian_bcnn_model(X_train.shape[1:])\n",
    "optimizer = tf.keras.optimizers.Adam(lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def elbo_loss(labels, logits):\n",
    "    loss_en = tf.nn.softmax_cross_entropy_with_logits(labels, logits)\n",
    "    loss_kl = tf.keras.losses.KLD(labels, logits)\n",
    "    loss = tf.reduce_mean(tf.add(loss_en, loss_kl))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = bcnn(X_train)\n",
    "        loss = elbo_loss(labels, logits)\n",
    "    gradients = tape.gradient(loss, bcnn.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, bcnn.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "def accuracy(preds, labels):\n",
    "    return np.mean(np.argmax(preds, axis=1) == np.argmax(y_train, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0912 17:22:46.940508 139683306188544 deprecation.py:323] From /home/u30073/.local/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0: loss = 11.073323249816895 , accuracy = 0.1221\n",
      "Epoch: 1: loss = 6.9439697265625 , accuracy = 0.22618333333333332\n",
      "Epoch: 2: loss = 6.832186222076416 , accuracy = 0.23513333333333333\n",
      "Epoch: 3: loss = 6.935305118560791 , accuracy = 0.2617833333333333\n",
      "Epoch: 4: loss = 4.872640609741211 , accuracy = 0.3858333333333333\n",
      "Epoch: 5: loss = 3.878277063369751 , accuracy = 0.39986666666666665\n",
      "Epoch: 6: loss = 2.8462696075439453 , accuracy = 0.43205\n",
      "Epoch: 7: loss = 1.9528741836547852 , accuracy = 0.5061\n",
      "Epoch: 8: loss = 1.7277404069900513 , accuracy = 0.5768\n",
      "Epoch: 9: loss = 1.5229679346084595 , accuracy = 0.6357833333333334\n",
      "Epoch: 10: loss = 1.2816596031188965 , accuracy = 0.7013166666666667\n",
      "Epoch: 11: loss = 0.9405604600906372 , accuracy = 0.7491666666666666\n",
      "Epoch: 12: loss = 0.7817077040672302 , accuracy = 0.7639666666666667\n",
      "Epoch: 13: loss = 0.7388714551925659 , accuracy = 0.7898333333333334\n",
      "Epoch: 14: loss = 0.7215876579284668 , accuracy = 0.8225\n",
      "Epoch: 15: loss = 0.6683122515678406 , accuracy = 0.8385166666666667\n",
      "Epoch: 16: loss = 0.6098821759223938 , accuracy = 0.8460666666666666\n",
      "Epoch: 17: loss = 0.5835403800010681 , accuracy = 0.866\n",
      "Epoch: 18: loss = 0.5117616653442383 , accuracy = 0.8639\n",
      "Epoch: 19: loss = 0.48298534750938416 , accuracy = 0.8638833333333333\n",
      "Epoch: 20: loss = 0.4586915373802185 , accuracy = 0.8701833333333333\n",
      "Epoch: 21: loss = 0.43474870920181274 , accuracy = 0.88025\n",
      "Epoch: 22: loss = 0.42533743381500244 , accuracy = 0.8846166666666667\n",
      "Epoch: 23: loss = 0.418544739484787 , accuracy = 0.8858333333333334\n",
      "Epoch: 24: loss = 0.3836084008216858 , accuracy = 0.8956\n",
      "Epoch: 25: loss = 0.35764768719673157 , accuracy = 0.9013166666666667\n",
      "Epoch: 26: loss = 0.3276669979095459 , accuracy = 0.9048\n",
      "Epoch: 27: loss = 0.3274906277656555 , accuracy = 0.9104\n",
      "Epoch: 28: loss = 0.3127041757106781 , accuracy = 0.9169166666666667\n",
      "Epoch: 29: loss = 0.2984698712825775 , accuracy = 0.9210166666666667\n",
      "Epoch: 30: loss = 0.2811444103717804 , accuracy = 0.9240166666666667\n",
      "Epoch: 31: loss = 0.27423229813575745 , accuracy = 0.9271166666666667\n",
      "Epoch: 32: loss = 0.2555074989795685 , accuracy = 0.9295166666666667\n",
      "Epoch: 33: loss = 0.24462252855300903 , accuracy = 0.93055\n",
      "Epoch: 34: loss = 0.230399027466774 , accuracy = 0.9354666666666667\n"
     ]
    }
   ],
   "source": [
    "for i in range(35):\n",
    "    loss = train_step(X_train, y_train)\n",
    "    preds = bcnn(X_train)\n",
    "    acc = accuracy(preds, y_train)\n",
    "    print(\"Epoch: {}: loss = {} , accuracy = {}\".format(i, loss, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of recognizable samples: 9999\n",
      "Unrecognizable samples: [6572]\n",
      "Test accuracy on MNIST (recognizable samples): 0.9427942794279428\n",
      "Test accuracy on MNIST (unrecognizable samples): 0.0\n"
     ]
    }
   ],
   "source": [
    "n_mc_run = 50\n",
    "med_prob_thres = 0.20\n",
    "\n",
    "y_pred_logits_list = [bcnn(X_test) for _ in range(n_mc_run)]  # a list of predicted logits\n",
    "y_pred_prob_all = np.concatenate([softmax(y, axis=-1)[:, :, np.newaxis] for y in y_pred_logits_list], axis=-1)\n",
    "y_pred = [[int(np.median(y) >= med_prob_thres) for y in y_pred_prob] for y_pred_prob in y_pred_prob_all]\n",
    "y_pred = np.array(y_pred)\n",
    "\n",
    "idx_valid = [any(y) for y in y_pred]\n",
    "print('Number of recognizable samples:', sum(idx_valid))\n",
    "\n",
    "idx_invalid = [not any(y) for y in y_pred]\n",
    "print('Unrecognizable samples:', np.where(idx_invalid)[0])\n",
    "\n",
    "print('Test accuracy on MNIST (recognizable samples):',\n",
    "      sum(np.equal(np.argmax(y_test[idx_valid], axis=-1), np.argmax(y_pred[idx_valid], axis=-1))) / len(y_test[idx_valid]))\n",
    "\n",
    "print('Test accuracy on MNIST (unrecognizable samples):',\n",
    "      sum(np.equal(np.argmax(y_test[idx_invalid], axis=-1), np.argmax(y_pred[idx_invalid], axis=-1))) / len(y_test[idx_invalid]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f0998289470>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f09d810f470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test = X_train[6572].reshape(-1, 28, 28, 1)\n",
    "plt.imshow(X_train[6572, :, :, 0], cmap='gist_gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfp_nightly",
   "language": "python",
   "name": "tfp_nightly"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
